/bin/bash /home/chrisjihee/proj/DeepKNLP/task2-nerG-trainer2.sh
(DeepKNLP) chrisjihee@ptlm2:~/proj/DeepKNLP$ /bin/bash /home/chrisjihee/proj/DeepKNLP/task2-nerG-trainer2.sh
+ /home/chrisjihee/miniforge3/envs/DeepKNLP/bin/python -m deepspeed.launcher.runner --include=localhost:0,1,2,3 --master_port 30000 task2-nerG-trainer2.py train
[01.17 02:11:11] ┇ WARNING ┇                                        DeepSpeed ┇ Unable to find hostfile, will proceed with training with local resources only.
[01.17 02:11:25] ┇ WARNING ┇                                         __main__ ┇ Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: True
[01.17 02:11:25] ┇ WARNING ┇                                         __main__ ┇ Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True
[01.17 02:11:25] ┇ INFO    ┇                                         __main__ ┇ Training/evaluation parameters Seq2SeqTrainingArgumentsForGNER(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed_configs/deepspeed_zero1_llama.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_data_path=data/gner/zero-shot-test-min.jsonl,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=1280,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER/EAGLE-1B-debug/runs/Jan17_02-11-23_ptlm2,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_source_length=640,
max_steps=-1,
max_target_length=640,
metric_for_best_model=None,
model_name_or_path=etri-lirs/egpt-1.3b-preview,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=0.5,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER/EAGLE-1B-debug,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
pred_data_path=data/gner/zero-shot-test-min.jsonl,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER/EAGLE-1B-debug,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=7,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_data_path=data/gner/zero-shot-train.jsonl,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.04,
warmup_steps=0,
weight_decay=0.0,
)
[01.17 02:11:25] ┇ WARNING ┇                                         __main__ ┇ Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: True
[01.17 02:11:25] ┇ WARNING ┇                                         __main__ ┇ Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: True
[01.17 02:11:25] ┇ INFO    ┇                                         __main__ ┇ training_args.should_log=True
[01.17 02:11:26] ┇ WARNING ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48
[01.17 02:11:26] ┇ WARNING ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48
[01.17 02:11:26] ┇ WARNING ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48
[01.17 02:11:26] ┇ WARNING ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48
[01.17 02:11:28] ┇ INFO    ┇                                         __main__ ┇ type(model)=<class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>
[01.17 02:11:28] ┇ INFO    ┇                                         __main__ ┇ model.generation_config.pad_token_id=0
[01.17 02:11:29] ┇ INFO    ┇                                         __main__ ┇ Use data/gner/zero-shot-train.jsonl as train_dataset(#=18135)
Running tokenizer on train_dataset (num_proc=4):   0%|                                                                                | 0/18135 [00:00<?, ? examples/s][rank1]:[W117 02:11:30.679758337 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W117 02:11:30.679758317 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W117 02:11:30.682731825 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Running tokenizer on train_dataset (num_proc=4): 100%|██████████████████████████████████████████████████████████████████| 18135/18135 [00:05<00:00, 3262.85 examples/s]
[rank0]:[W117 02:11:36.497062994 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Running tokenizer on train_dataset (num_proc=4):   0%|                                                                                | 0/18135 [00:00<?, ? examples/s][01.17 02:11:38] ┇ INFO    ┇                                         __main__ ┇ Use data/gner/zero-shot-test-min.jsonl as eval_dataset(#=65)
Running tokenizer on eval_dataset (num_proc=4): 100%|██████████████████████████████████████████████████████████████████████████| 65/65 [00:00<00:00, 111.73 examples/s]
Running tokenizer on train_dataset (num_proc=4): 100%|██████████████████████████████████████████████████████████████████| 18135/18135 [00:06<00:00, 2722.59 examples/s]
Running tokenizer on train_dataset (num_proc=4): 100%|██████████████████████████████████████████████████████████████████| 18135/18135 [00:06<00:00, 2711.83 examples/s]
Running tokenizer on train_dataset (num_proc=4): 100%|██████████████████████████████████████████████████████████████████| 18135/18135 [00:06<00:00, 2699.99 examples/s]
[01.17 02:11:45] ┇ INFO    ┇                             transformers.trainer ┇ Using auto half precision backend
[01.17 02:11:45] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] DeepSpeed info: version=0.16.3+05eaf3d1, git-hash=05eaf3d1, git-branch=master
[01.17 02:11:45] ┇ INFO    ┇                                        DeepSpeed ┇ Config mesh_device None world_size = 4
Running tokenizer on eval_dataset (num_proc=4): 100%|██████████████████████████████████████████████████████████████████████████| 65/65 [00:00<00:00, 101.19 examples/s]
Running tokenizer on eval_dataset (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████| 65/65 [00:00<00:00, 95.75 examples/s]
Running tokenizer on eval_dataset (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████| 65/65 [00:00<00:00, 96.67 examples/s]
[01.17 02:11:50] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] DeepSpeed Flops Profiler Enabled: False
[01.17 02:11:50] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[01.17 02:11:50] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[01.17 02:11:50] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[01.17 02:11:50] ┇ INFO    ┇                                        DeepSpeed ┇ Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[01.17 02:11:50] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[01.17 02:11:50] ┇ INFO    ┇                                        DeepSpeed ┇ Reduce bucket size 200000000
[01.17 02:11:50] ┇ INFO    ┇                                        DeepSpeed ┇ Allgather bucket size 200000000
[01.17 02:11:50] ┇ INFO    ┇                                        DeepSpeed ┇ CPU Offload: False
[01.17 02:11:50] ┇ INFO    ┇                                        DeepSpeed ┇ Round robin gradient partitioning: False
[01.17 02:11:54] ┇ WARNING ┇                                        DeepSpeed ┇ Attempting to get learning rate from scheduler before it has started
[01.17 02:11:54] ┇ WARNING ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[01.17 02:11:55] ┇ WARNING ┇                                        DeepSpeed ┇ Attempting to get learning rate from scheduler before it has started
[01.17 02:11:55] ┇ WARNING ┇                                        DeepSpeed ┇ Attempting to get learning rate from scheduler before it has started
[01.17 02:11:55] ┇ WARNING ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[01.17 02:11:55] ┇ WARNING ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ Before initializing optimizer states
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ MA 3.84 GB         Max_MA 4.47 GB         CA 4.65 GB         Max_CA 5 GB
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ CPU Virtual Memory:  used = 17.4 GB, percent = 6.9%
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ After initializing optimizer states
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ MA 3.84 GB         Max_MA 5.09 GB         CA 5.9 GB         Max_CA 6 GB
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ CPU Virtual Memory:  used = 17.4 GB, percent = 6.9%
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ optimizer state initialized
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ After initializing ZeRO optimizer
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ MA 3.84 GB         Max_MA 3.84 GB         CA 5.9 GB         Max_CA 6 GB
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ CPU Virtual Memory:  used = 17.41 GB, percent = 6.9%
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[01.17 02:11:55] ┇ WARNING ┇                                        DeepSpeed ┇ Attempting to get learning rate from scheduler before it has started
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7f5ae33726c0>
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇ DeepSpeedEngine configuration:
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   amp_enabled .................. False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   amp_params ................... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   bfloat16_enabled ............. True
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   bfloat16_immediate_grad_update  False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   checkpoint_parallel_write_pipeline  False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   checkpoint_tag_validation_enabled  True
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   checkpoint_tag_validation_fail  False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5ae31ad430>
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   communication_data_type ...... None
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   curriculum_enabled_legacy .... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   curriculum_params_legacy ..... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   data_efficiency_enabled ...... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   dataloader_drop_last ......... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   disable_allgather ............ False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   dump_state ................... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   dynamic_loss_scale_args ...... None
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   eigenvalue_enabled ........... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   eigenvalue_gas_boundary_resolution  1
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   eigenvalue_layer_name ........ bert.encoder.layer
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   eigenvalue_layer_num ......... 0
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   eigenvalue_max_iter .......... 100
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   eigenvalue_stability ......... 1e-06
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   eigenvalue_tol ............... 0.01
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   eigenvalue_verbose ........... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   elasticity_enabled ........... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   fp16_auto_cast ............... None
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   fp16_enabled ................. False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   fp16_master_weights_and_gradients  False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   global_rank .................. 0
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   grad_accum_dtype ............. None
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   gradient_accumulation_steps .. 4
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   gradient_clipping ............ 1.0
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   gradient_predivide_factor .... 1.0
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   graph_harvesting ............. False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   initial_dynamic_scale ........ 1
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   load_universal_checkpoint .... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   loss_scale ................... 1.0
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   memory_breakdown ............. False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   mics_hierarchial_params_gather  False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   mics_shard_size .............. -1
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   optimizer_legacy_fusion ...... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   optimizer_name ............... adamw
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   pld_enabled .................. False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   pld_params ................... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   prescale_gradients ........... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   scheduler_name ............... WarmupDecayLR
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 3, 'total_num_steps': 71}
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   seq_parallel_communication_data_type  torch.float32
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   sparse_attention ............. None
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   sparse_gradients_enabled ..... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   steps_per_print .............. inf
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   timers_config ................ enabled=True synchronized=True
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   train_batch_size ............. 128
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   train_micro_batch_size_per_gpu  8
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   use_data_before_expert_parallel_  False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   use_node_local_storage ....... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   wall_clock_breakdown ......... False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   weight_quantization_config ... None
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   world_size ................... 4
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   zero_allow_untested_optimizer  False
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   zero_enabled ................. True
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   zero_force_ds_cpu_optimizer .. True
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   zero_optimization_stage ...... 1
[01.17 02:11:55] ┇ INFO    ┇                                        DeepSpeed ┇   json = {
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 3,
            "total_num_steps": 71
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 128,
    "train_micro_batch_size_per_gpu": 8,
    "steps_per_print": inf,
    "fp16": {
        "enabled": false
    }
}
[01.17 02:11:55] ┇ INFO    ┇                             transformers.trainer ┇ ***** Running training *****
[01.17 02:11:55] ┇ INFO    ┇                             transformers.trainer ┇   Num examples = 18,135
[01.17 02:11:55] ┇ INFO    ┇                             transformers.trainer ┇   Num Epochs = 1
[01.17 02:11:55] ┇ INFO    ┇                             transformers.trainer ┇   Instantaneous batch size per device = 8
[01.17 02:11:55] ┇ INFO    ┇                             transformers.trainer ┇   Total train batch size (w. parallel, distributed & accumulation) = 128
[01.17 02:11:55] ┇ INFO    ┇                             transformers.trainer ┇   Gradient Accumulation steps = 4
[01.17 02:11:55] ┇ INFO    ┇                             transformers.trainer ┇   Total optimization steps = 71
[01.17 02:11:55] ┇ INFO    ┇                             transformers.trainer ┇   Number of trainable parameters = 1,341,247,488
  0%|                                                                                                                                           | 0/71 [00:00<?, ?it/s][01.17 02:11:55] ┇ WARNING ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
{'loss': 0.9358, 'grad_norm': 0.7520268559455872, 'learning_rate': 1.823529411764706e-05, 'epoch': 0.07}
{'loss': 0.1278, 'grad_norm': 0.31632018089294434, 'learning_rate': 1.5294117647058822e-05, 'epoch': 0.14}
{'loss': 0.0705, 'grad_norm': 0.19092705845832825, 'learning_rate': 1.235294117647059e-05, 'epoch': 0.21}
{'loss': 0.0532, 'grad_norm': 0.195684015750885, 'learning_rate': 9.411764705882354e-06, 'epoch': 0.28}
{'loss': 0.0477, 'grad_norm': 0.153150275349617, 'learning_rate': 6.470588235294119e-06, 'epoch': 0.35}
{'loss': 0.039, 'grad_norm': 0.10214919596910477, 'learning_rate': 3.529411764705883e-06, 'epoch': 0.42}
{'loss': 0.0407, 'grad_norm': 0.11457229405641556, 'learning_rate': 5.882352941176471e-07, 'epoch': 0.49}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71/71 [02:28<00:00,  1.98s/it][01.17 02:14:25] ┇ INFO    ┇                             transformers.trainer ┇ Saving model checkpoint to output/GNER/EAGLE-1B-debug/checkpoint-71
[01.17 02:14:25] ┇ INFO    ┇                 transformers.configuration_utils ┇ Configuration saved in output/GNER/EAGLE-1B-debug/checkpoint-71/config.json
[01.17 02:14:25] ┇ INFO    ┇      transformers.generation.configuration_utils ┇ Configuration saved in output/GNER/EAGLE-1B-debug/checkpoint-71/generation_config.json
[01.17 02:14:51] ┇ INFO    ┇                      transformers.modeling_utils ┇ Model weights saved in output/GNER/EAGLE-1B-debug/checkpoint-71/model.safetensors
[01.17 02:14:51] ┇ INFO    ┇             transformers.tokenization_utils_base ┇ tokenizer config file saved in output/GNER/EAGLE-1B-debug/checkpoint-71/tokenizer_config.json
[01.17 02:14:51] ┇ INFO    ┇             transformers.tokenization_utils_base ┇ Special tokens file saved in output/GNER/EAGLE-1B-debug/checkpoint-71/special_tokens_map.json
[01.17 02:14:51] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] [Torch] Checkpoint global_step71 is about to be saved!
[01.17 02:14:51] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] Saving model checkpoint: output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/mp_rank_00_model_states.pt
[01.17 02:14:51] ┇ INFO    ┇                                        DeepSpeed ┇ [Torch] Saving output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/mp_rank_00_model_states.pt...
[01.17 02:15:19] ┇ INFO    ┇                                        DeepSpeed ┇ [Torch] Saved output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/mp_rank_00_model_states.pt.
[01.17 02:15:19] ┇ INFO    ┇                                        DeepSpeed ┇ [Torch] Saving output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[01.17 02:17:55] ┇ INFO    ┇                                        DeepSpeed ┇ [Torch] Saved output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[01.17 02:17:55] ┇ INFO    ┇                                        DeepSpeed ┇ zero checkpoint saved output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[01.17 02:17:55] ┇ INFO    ┇                                        DeepSpeed ┇ [Torch] Checkpoint global_step71 is ready now!
[01.17 02:17:55] ┇ INFO    ┇                             transformers.trainer ┇ ***** Running Evaluation *****
[01.17 02:17:55] ┇ INFO    ┇                             transformers.trainer ┇   Num examples = 65
[01.17 02:17:55] ┇ INFO    ┇                             transformers.trainer ┇   Batch size = 8
{'eval_crossner_ai_prec': 0.35532994923839833, 'eval_crossner_ai_rec': 0.31390134529133906, 'eval_crossner_ai_f1': 0.33333333328336623, 'eval_AVERAGE_f1': 0.33333333328336623, 'eval_runtime': 13.2351, 'eval_samples_per_second': 4.911, 'eval_steps_per_second': 0.227, 'epoch': 0.5}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71/71 [06:12<00:00,  1.98s/it]
[01.17 02:18:10] ┇ INFO    ┇                             transformers.trainer ┇ Saving model checkpoint to output/GNER/EAGLE-1B-debug/checkpoint-71
[01.17 02:18:10] ┇ INFO    ┇                 transformers.configuration_utils ┇ Configuration saved in output/GNER/EAGLE-1B-debug/checkpoint-71/config.json
[01.17 02:18:10] ┇ INFO    ┇      transformers.generation.configuration_utils ┇ Configuration saved in output/GNER/EAGLE-1B-debug/checkpoint-71/generation_config.json
[01.17 02:18:37] ┇ INFO    ┇                      transformers.modeling_utils ┇ Model weights saved in output/GNER/EAGLE-1B-debug/checkpoint-71/model.safetensors
[01.17 02:18:37] ┇ INFO    ┇             transformers.tokenization_utils_base ┇ tokenizer config file saved in output/GNER/EAGLE-1B-debug/checkpoint-71/tokenizer_config.json
[01.17 02:18:37] ┇ INFO    ┇             transformers.tokenization_utils_base ┇ Special tokens file saved in output/GNER/EAGLE-1B-debug/checkpoint-71/special_tokens_map.json
[01.17 02:18:37] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] [Torch] Checkpoint global_step71 is about to be saved!
[01.17 02:18:37] ┇ INFO    ┇                                        DeepSpeed ┇ [Rank 0] Saving model checkpoint: output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/mp_rank_00_model_states.pt
[01.17 02:18:37] ┇ INFO    ┇                                        DeepSpeed ┇ [Torch] Saving output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/mp_rank_00_model_states.pt...
[01.17 02:19:03] ┇ INFO    ┇                                        DeepSpeed ┇ [Torch] Saved output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/mp_rank_00_model_states.pt.
[01.17 02:19:03] ┇ INFO    ┇                                        DeepSpeed ┇ [Torch] Saving output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[01.17 02:22:04] ┇ INFO    ┇                                        DeepSpeed ┇ [Torch] Saved output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[01.17 02:22:04] ┇ INFO    ┇                                        DeepSpeed ┇ zero checkpoint saved output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[01.17 02:22:04] ┇ INFO    ┇                                        DeepSpeed ┇ [Torch] Checkpoint global_step71 is ready now!
[01.17 02:22:07] ┇ INFO    ┇                             transformers.trainer ┇

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 611.3726, 'train_samples_per_second': 14.831, 'train_steps_per_second': 0.116, 'train_loss': 0.1858822997187225, 'epoch': 0.5}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71/71 [10:11<00:00,  8.61s/it]
[01.17 02:22:07] ┇ INFO    ┇                                         __main__ ┇ train_result=TrainOutput(global_step=71, training_loss=0.1858822997187225, metrics={'train_runtime': 611.3726, 'train_samples_per_second': 14.831, 'train_steps_per_second': 0.116, 'total_flos': 2.2242260498251776e+16, 'train_loss': 0.1858822997187225, 'epoch': 0.5008818342151675})
[rank0]:[W117 02:22:09.384054174 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
(DeepKNLP) chrisjihee@ptlm2:~/proj/DeepKNLP$