(DeepKNLP) chrisjihee@ptlm2:~/proj/DeepKNLP$ pip list | grep -E "torch|lightn|trans|accel|speed|numpy|piece|chris|prog|pydantic"; ds_report
accelerate               1.2.1
chrisbase                0.5.7           /home/chrisjihee/proj/DeepKNLP/chrisbase
chrisdata                0.5.0           /home/chrisjihee/proj/DeepKNLP/chrisdata
deepspeed                0.16.3+05eaf3d1 /home/chrisjihee/proj/DeepKNLP/deepspeed
elastic-transport        8.17.0
lightning                2.5.0.post0
lightning-utilities      0.11.9
numpy                    2.2.1
progiter                 2.0.0.post1     /home/chrisjihee/proj/DeepKNLP/progiter
pydantic                 2.10.5
pydantic_core            2.27.2
pytorch-lightning        2.5.0.post0
sentencepiece            0.2.0
torch                    2.5.1+cu118
torchmetrics             1.6.1
transformers             4.49.0.dev0     /home/chrisjihee/proj/DeepKNLP/transformers
/home/chrisjihee/miniforge3/envs/DeepKNLP/bin/ds_report:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  __import__('pkg_resources').require('deepspeed==0.16.3+05eaf3d1')
[2025-01-17 01:13:46,760] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 01:13:46,826] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [OKAY]
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [NO] ....... [NO]
fused_adam ............. [YES] ...... [OKAY]
cpu_adam ............... [NO] ....... [OKAY]
cpu_adagrad ............ [NO] ....... [OKAY]
cpu_lion ............... [NO] ....... [OKAY]
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
evoformer_attn ......... [NO] ....... [NO]
 [WARNING]  FP Quantizer is using an untested triton version (3.1.0), only 2.3.(0, 1) and 3.0.0 are known to be compatible with these kernels
fp_quantizer ........... [NO] ....... [NO]
fused_lamb ............. [NO] ....... [OKAY]
fused_lion ............. [NO] ....... [OKAY]
/home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat/ld: cannot find -lcufile: No such file or directory
collect2: error: ld returned 1 exit status
gds .................... [NO] ....... [NO]
transformer_inference .. [NO] ....... [OKAY]
inference_core_ops ..... [NO] ....... [OKAY]
cutlass_ops ............ [NO] ....... [OKAY]
quantizer .............. [NO] ....... [OKAY]
ragged_device_ops ...... [NO] ....... [OKAY]
ragged_ops ............. [NO] ....... [OKAY]
random_ltd ............. [NO] ....... [OKAY]
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.5
 [WARNING]  using untested triton version (3.1.0), only 1.0.0 is known to be compatible
sparse_attn ............ [NO] ....... [NO]
spatial_inference ...... [NO] ....... [OKAY]
transformer ............ [NO] ....... [OKAY]
stochastic_transformer . [NO] ....... [OKAY]
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/chrisjihee/miniforge3/envs/DeepKNLP/lib/python3.12/site-packages/torch']
torch version .................... 2.5.1+cu118
deepspeed install path ........... ['/home/chrisjihee/proj/DeepKNLP/deepspeed/deepspeed']
deepspeed info ................... 0.16.3+05eaf3d1, 05eaf3d1, master
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.5, cuda 11.8
shared memory (/dev/shm) size .... 125.78 GB
(DeepKNLP) chrisjihee@ptlm2:~/proj/DeepKNLP$



/bin/bash /home/chrisjihee/proj/DeepKNLP/task2-nerG-trainer2.sh
(DeepKNLP) chrisjihee@ptlm2:~/proj/DeepKNLP$ /bin/bash /home/chrisjihee/proj/DeepKNLP/task2-nerG-trainer2.sh
+ /home/chrisjihee/miniforge3/envs/DeepKNLP/bin/python -m deepspeed.launcher.runner --include=localhost:0,1,2,3 --master_port 30000 task2-nerG-trainer2.py train
[2025-01-17 01:02:54,912] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 01:02:54,978] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 01:02:57,642] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-01-17 01:02:57,643] [INFO] [runner.py:607:main] cmd = /home/chrisjihee/miniforge3/envs/DeepKNLP/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=30000 --enable_each_rank_log=None task2-nerG-trainer2.py train
[2025-01-17 01:02:59,784] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 01:02:59,850] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 01:03:03,663] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-01-17 01:03:03,663] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-01-17 01:03:03,664] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-01-17 01:03:03,664] [INFO] [launch.py:164:main] dist_world_size=4
[2025-01-17 01:03:03,664] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-01-17 01:03:03,664] [INFO] [launch.py:256:main] process 107438 spawned with command: ['/home/chrisjihee/miniforge3/envs/DeepKNLP/bin/python', '-u', 'task2-nerG-trainer2.py', '--local_rank=0', 'train']
[2025-01-17 01:03:03,665] [INFO] [launch.py:256:main] process 107439 spawned with command: ['/home/chrisjihee/miniforge3/envs/DeepKNLP/bin/python', '-u', 'task2-nerG-trainer2.py', '--local_rank=1', 'train']
[2025-01-17 01:03:03,666] [INFO] [launch.py:256:main] process 107440 spawned with command: ['/home/chrisjihee/miniforge3/envs/DeepKNLP/bin/python', '-u', 'task2-nerG-trainer2.py', '--local_rank=2', 'train']
[2025-01-17 01:03:03,666] [INFO] [launch.py:256:main] process 107441 spawned with command: ['/home/chrisjihee/miniforge3/envs/DeepKNLP/bin/python', '-u', 'task2-nerG-trainer2.py', '--local_rank=3', 'train']
[01.17 01:03:09] ┇ INFO     ┇                                         __main__ ┇ env=hostname='ptlm2' hostaddr='129.254.121.73' global_rank=-1 local_rank=1 node_rank=-1 world_size=-1 time_stamp='0117.010308' python_path=PosixPath('/home/chrisjihee/miniforge3/envs/DeepKNLP/bin/python') current_dir=PosixPath('/home/chrisjihee/proj/DeepKNLP') current_file=PosixPath('/home/chrisjihee/proj/DeepKNLP/chrisbase/src/chrisbase/data.py') command_args=['--local_rank=1', 'train'] output_home='output' output_name='GNER' run_version='EAGLE-1B-debug' logging_file='train-messages.out' logging_level='info' logging_format='%(asctime)s ┇ %(levelname)-8s ┇ %(name)48s ┇ %(message)s' datetime_format='[%m.%d %H:%M:%S]' argument_file='train-arguments.json' random_seed=7 max_workers=4 debugging=False output_dir=PosixPath('output/GNER/EAGLE-1B-debug')
[01.17 01:03:09] ┇ INFO     ┇                                         __main__ ┇ env.output_dir=output/GNER/EAGLE-1B-debug
[01.17 01:03:09] ┇ INFO     ┇                                         __main__ ┇ env.time_stamp=0117.010308
[01.17 01:03:09] ┇ INFO     ┇                                         __main__ ┇ env=hostname='ptlm2' hostaddr='129.254.121.73' global_rank=-1 local_rank=3 node_rank=-1 world_size=-1 time_stamp='0117.010309' python_path=PosixPath('/home/chrisjihee/miniforge3/envs/DeepKNLP/bin/python') current_dir=PosixPath('/home/chrisjihee/proj/DeepKNLP') current_file=PosixPath('/home/chrisjihee/proj/DeepKNLP/chrisbase/src/chrisbase/data.py') command_args=['--local_rank=3', 'train'] output_home='output' output_name='GNER' run_version='EAGLE-1B-debug' logging_file='train-messages.out' logging_level='info' logging_format='%(asctime)s ┇ %(levelname)-8s ┇ %(name)48s ┇ %(message)s' datetime_format='[%m.%d %H:%M:%S]' argument_file='train-arguments.json' random_seed=7 max_workers=4 debugging=False output_dir=PosixPath('output/GNER/EAGLE-1B-debug')
[01.17 01:03:09] ┇ INFO     ┇                                         __main__ ┇ env.output_dir=output/GNER/EAGLE-1B-debug
[01.17 01:03:09] ┇ INFO     ┇                                         __main__ ┇ env.time_stamp=0117.010309
[01.17 01:03:09] ┇ INFO     ┇                                         __main__ ┇ env=hostname='ptlm2' hostaddr='129.254.121.73' global_rank=-1 local_rank=2 node_rank=-1 world_size=-1 time_stamp='0117.010309' python_path=PosixPath('/home/chrisjihee/miniforge3/envs/DeepKNLP/bin/python') current_dir=PosixPath('/home/chrisjihee/proj/DeepKNLP') current_file=PosixPath('/home/chrisjihee/proj/DeepKNLP/chrisbase/src/chrisbase/data.py') command_args=['--local_rank=2', 'train'] output_home='output' output_name='GNER' run_version='EAGLE-1B-debug' logging_file='train-messages.out' logging_level='info' logging_format='%(asctime)s ┇ %(levelname)-8s ┇ %(name)48s ┇ %(message)s' datetime_format='[%m.%d %H:%M:%S]' argument_file='train-arguments.json' random_seed=7 max_workers=4 debugging=False output_dir=PosixPath('output/GNER/EAGLE-1B-debug')
[01.17 01:03:09] ┇ INFO     ┇                                         __main__ ┇ env.output_dir=output/GNER/EAGLE-1B-debug
[01.17 01:03:09] ┇ INFO     ┇                                         __main__ ┇ env.time_stamp=0117.010309
[01.17 01:03:09] ┇ INFO     ┇                                         __main__ ┇ env=hostname='ptlm2' hostaddr='129.254.121.73' global_rank=-1 local_rank=0 node_rank=-1 world_size=-1 time_stamp='0117.010309' python_path=PosixPath('/home/chrisjihee/miniforge3/envs/DeepKNLP/bin/python') current_dir=PosixPath('/home/chrisjihee/proj/DeepKNLP') current_file=PosixPath('/home/chrisjihee/proj/DeepKNLP/chrisbase/src/chrisbase/data.py') command_args=['--local_rank=0', 'train'] output_home='output' output_name='GNER' run_version='EAGLE-1B-debug' logging_file='train-messages.out' logging_level='info' logging_format='%(asctime)s ┇ %(levelname)-8s ┇ %(name)48s ┇ %(message)s' datetime_format='[%m.%d %H:%M:%S]' argument_file='train-arguments.json' random_seed=7 max_workers=4 debugging=False output_dir=PosixPath('output/GNER/EAGLE-1B-debug')
[01.17 01:03:09] ┇ INFO     ┇                                         __main__ ┇ env.output_dir=output/GNER/EAGLE-1B-debug
[01.17 01:03:09] ┇ INFO     ┇                                         __main__ ┇ env.time_stamp=0117.010309
[2025-01-17 01:03:09,734] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 01:03:09,736] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 01:03:09,740] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 01:03:09,743] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 01:03:09,800] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 01:03:09,800] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 01:03:09,804] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-17 01:03:09,806] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[01.17 01:03:09] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -c /tmp/tmppae8psjw/test.c -o /tmp/tmppae8psjw/test.o
[01.17 01:03:09] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -c /tmp/tmp4_vua6hg/test.c -o /tmp/tmp4_vua6hg/test.o
[01.17 01:03:09] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -c /tmp/tmp405x0ee0/test.c -o /tmp/tmp405x0ee0/test.o
[01.17 01:03:09] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -c /tmp/tmpcolfu7gx/test.c -o /tmp/tmpcolfu7gx/test.o
[01.17 01:03:09] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat /tmp/tmppae8psjw/test.o -laio -o /tmp/tmppae8psjw/a.out
[01.17 01:03:09] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat /tmp/tmp405x0ee0/test.o -laio -o /tmp/tmp405x0ee0/a.out
[01.17 01:03:09] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat /tmp/tmp4_vua6hg/test.o -laio -o /tmp/tmp4_vua6hg/a.out
[01.17 01:03:09] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat /tmp/tmpcolfu7gx/test.o -laio -o /tmp/tmpcolfu7gx/a.out
[01.17 01:03:10] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -c /tmp/tmp7wljdzmz/test.c -o /tmp/tmp7wljdzmz/test.o
[01.17 01:03:10] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -c /tmp/tmplekn0myo/test.c -o /tmp/tmplekn0myo/test.o
[01.17 01:03:10] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat /tmp/tmp7wljdzmz/test.o -L/home/chrisjihee/miniforge3/envs/DeepKNLP -L/home/chrisjihee/miniforge3/envs/DeepKNLP/lib64 -lcufile -o /tmp/tmp7wljdzmz/a.out
[01.17 01:03:10] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -c /tmp/tmpapc0zoi8/test.c -o /tmp/tmpapc0zoi8/test.o
[01.17 01:03:10] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat /tmp/tmplekn0myo/test.o -L/home/chrisjihee/miniforge3/envs/DeepKNLP -L/home/chrisjihee/miniforge3/envs/DeepKNLP/lib64 -lcufile -o /tmp/tmplekn0myo/a.out
[01.17 01:03:10] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -O2 -isystem /home/chrisjihee/miniforge3/envs/DeepKNLP/include -fPIC -c /tmp/tmpof1sdubo/test.c -o /tmp/tmpof1sdubo/test.o
[01.17 01:03:10] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat /tmp/tmpapc0zoi8/test.o -L/home/chrisjihee/miniforge3/envs/DeepKNLP -L/home/chrisjihee/miniforge3/envs/DeepKNLP/lib64 -lcufile -o /tmp/tmpapc0zoi8/a.out
[01.17 01:03:10] ┇ INFO     ┇                                             root ┇ gcc -pthread -B /home/chrisjihee/miniforge3/envs/DeepKNLP/compiler_compat /tmp/tmpof1sdubo/test.o -L/home/chrisjihee/miniforge3/envs/DeepKNLP -L/home/chrisjihee/miniforge3/envs/DeepKNLP/lib64 -lcufile -o /tmp/tmpof1sdubo/a.out
[2025-01-17 01:03:10,416] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-17 01:03:10,417] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-01-17 01:03:10,417] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-01-17 01:03:10,425] [INFO] [comm.py:652:init_distributed] cdb=None
[01.17 01:03:10] ┇ DEBUG    ┇                         c10d-NullHandler-default ┇ {'func_name': 'init_process_group', 'args': "('nccl',), {'timeout': datetime.timedelta(seconds=1800), 'init_method': None, 'rank': -1, 'world_size': -1}", 'pg_name': 'None', 'backend': 'nccl', 'world_size': '4', 'group_size': '4', 'global_rank': '3', 'local_rank': '3', 'nccl_version': '2.21.5', 'time_spent': '1005546ns'}
[2025-01-17 01:03:10,439] [INFO] [comm.py:652:init_distributed] cdb=None
[01.17 01:03:10] ┇ DEBUG    ┇                         c10d-NullHandler-default ┇ {'func_name': 'init_process_group', 'args': "('nccl',), {'timeout': datetime.timedelta(seconds=1800), 'init_method': None, 'rank': -1, 'world_size': -1}", 'pg_name': 'None', 'backend': 'nccl', 'world_size': '4', 'group_size': '4', 'global_rank': '1', 'local_rank': '1', 'nccl_version': '2.21.5', 'time_spent': '970901ns'}
[01.17 01:03:11] ┇ WARNING  ┇                                         __main__ ┇ Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: True
[01.17 01:03:11] ┇ WARNING  ┇                                         __main__ ┇ Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: True
[01.17 01:03:11] ┇ DEBUG    ┇                         c10d-NullHandler-default ┇ {'func_name': 'init_process_group', 'args': "('nccl',), {'timeout': datetime.timedelta(seconds=1800), 'init_method': None, 'rank': -1, 'world_size': -1}", 'pg_name': 'None', 'backend': 'nccl', 'world_size': '4', 'group_size': '4', 'global_rank': '2', 'local_rank': '2', 'nccl_version': '2.21.5', 'time_spent': '736884736ns'}
[01.17 01:03:11] ┇ DEBUG    ┇                         c10d-NullHandler-default ┇ {'func_name': 'init_process_group', 'args': "('nccl',), {'timeout': datetime.timedelta(seconds=1800), 'init_method': None, 'rank': -1, 'world_size': -1}", 'pg_name': 'None', 'backend': 'nccl', 'world_size': '4', 'group_size': '4', 'global_rank': '0', 'local_rank': '0', 'nccl_version': '2.21.5', 'time_spent': '737810522ns'}
[01.17 01:03:11] ┇ WARNING  ┇                                         __main__ ┇ Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True
[01.17 01:03:11] ┇ INFO     ┇                                         __main__ ┇ Training/evaluation parameters Seq2SeqTrainingArgumentsForGNER(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=True,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed_configs/deepspeed_zero1_llama.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_data_path=data/gner/zero-shot-test-min.jsonl,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=1280,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
ignore_pad_token_for_loss=True,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER/EAGLE-1B-debug/runs/Jan17_01-03-09_ptlm2,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=1.0,
max_source_length=640,
max_steps=-1,
max_target_length=640,
metric_for_best_model=None,
model_name_or_path=etri-lirs/egpt-1.3b-preview,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=0.5,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER/EAGLE-1B-debug,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
pred_data_path=data/gner/zero-shot-test-min.jsonl,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER/EAGLE-1B-debug,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=7,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_data_path=data/gner/zero-shot-train.jsonl,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.04,
warmup_steps=0,
weight_decay=0.0,
)
[01.17 01:03:11] ┇ INFO     ┇                                         __main__ ┇ training_args.should_log=True
[01.17 01:03:11] ┇ WARNING  ┇                                         __main__ ┇ Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: True
[01.17 01:03:12] ┇ WARNING  ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48
[01.17 01:03:12] ┇ WARNING  ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48
[01.17 01:03:12] ┇ WARNING  ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48
[01.17 01:03:12] ┇ WARNING  ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48
[01.17 01:03:15] ┇ INFO     ┇                                         __main__ ┇ type(model)=<class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>
[01.17 01:03:15] ┇ INFO     ┇                                         __main__ ┇ model.generation_config.pad_token_id=0
[rank3]:[W117 01:03:16.237651266 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[01.17 01:03:16] ┇ INFO     ┇                                         __main__ ┇ Use data/gner/zero-shot-train.jsonl as train_dataset(#=18135)
[rank2]:[W117 01:03:16.349888954 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W117 01:03:16.469831101 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Running tokenizer on train_dataset (num_proc=4): 100%|██████████████████████████████████████████████████████████████████| 18135/18135 [00:05<00:00, 3321.78 examples/s]
[rank0]:[W117 01:03:22.438022633 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Running tokenizer on train_dataset (num_proc=4):   0%|▏                                                                     | 40/18135 [00:00<02:08, 141.10 examples/s][01.17 01:03:24] ┇ INFO     ┇                                         __main__ ┇ Use data/gner/zero-shot-test-min.jsonl as eval_dataset(#=65)
Running tokenizer on eval_dataset (num_proc=4): 100%|██████████████████████████████████████████████████████████████████████████| 65/65 [00:00<00:00, 113.90 examples/s]
Running tokenizer on train_dataset (num_proc=4): 100%|██████████████████████████████████████████████████████████████████| 18135/18135 [00:05<00:00, 3485.54 examples/s]
Running tokenizer on train_dataset (num_proc=4): 100%|██████████████████████████████████████████████████████████████████| 18135/18135 [00:05<00:00, 3396.88 examples/s]
Running tokenizer on train_dataset (num_proc=4): 100%|██████████████████████████████████████████████████████████████████| 18135/18135 [00:06<00:00, 2980.73 examples/s]
[01.17 01:03:30] ┇ INFO     ┇                             transformers.trainer ┇ Using auto half precision backend
[01.17 01:03:31] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] DeepSpeed info: version=0.16.3+05eaf3d1, git-hash=05eaf3d1, git-branch=master
[01.17 01:03:31] ┇ INFO     ┇                                        DeepSpeed ┇ Config mesh_device None world_size = 4
Running tokenizer on eval_dataset (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████| 65/65 [00:00<00:00, 97.95 examples/s]
Running tokenizer on eval_dataset (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████| 65/65 [00:00<00:00, 90.70 examples/s]
Running tokenizer on eval_dataset (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████| 65/65 [00:00<00:00, 88.38 examples/s]
[01.17 01:03:32] ┇ DEBUG    ┇                         c10d-NullHandler-default ┇ {'func_name': 'new_group', 'args': '(range(0, 4),), {}', 'pg_name': 'None', 'backend': 'nccl', 'world_size': '4', 'group_size': '4', 'global_rank': '0', 'local_rank': '0', 'nccl_version': '2.21.5', 'time_spent': '408474ns'}
[01.17 01:03:34] ┇ DEBUG    ┇                         c10d-NullHandler-default ┇ {'func_name': 'new_group', 'args': '(range(0, 4),), {}', 'pg_name': 'None', 'backend': 'nccl', 'world_size': '4', 'group_size': '4', 'global_rank': '2', 'local_rank': '2', 'nccl_version': '2.21.5', 'time_spent': '394528ns'}
[01.17 01:03:35] ┇ DEBUG    ┇                         c10d-NullHandler-default ┇ {'func_name': 'new_group', 'args': '(range(0, 4),), {}', 'pg_name': 'None', 'backend': 'nccl', 'world_size': '4', 'group_size': '4', 'global_rank': '1', 'local_rank': '1', 'nccl_version': '2.21.5', 'time_spent': '400339ns'}
[01.17 01:03:35] ┇ DEBUG    ┇                         c10d-NullHandler-default ┇ {'func_name': 'new_group', 'args': '(range(0, 4),), {}', 'pg_name': 'None', 'backend': 'nccl', 'world_size': '4', 'group_size': '4', 'global_rank': '3', 'local_rank': '3', 'nccl_version': '2.21.5', 'time_spent': '386954ns'}
[01.17 01:03:35] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] DeepSpeed Flops Profiler Enabled: False
[01.17 01:03:35] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[01.17 01:03:35] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[01.17 01:03:35] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[01.17 01:03:35] ┇ INFO     ┇                                        DeepSpeed ┇ Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[01.17 01:03:35] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[01.17 01:03:35] ┇ INFO     ┇                                        DeepSpeed ┇ Reduce bucket size 200000000
[01.17 01:03:35] ┇ INFO     ┇                                        DeepSpeed ┇ Allgather bucket size 200000000
[01.17 01:03:35] ┇ INFO     ┇                                        DeepSpeed ┇ CPU Offload: False
[01.17 01:03:35] ┇ INFO     ┇                                        DeepSpeed ┇ Round robin gradient partitioning: False
[01.17 01:03:40] ┇ WARNING  ┇                                        DeepSpeed ┇ Attempting to get learning rate from scheduler before it has started
[01.17 01:03:40] ┇ WARNING  ┇                                        DeepSpeed ┇ Attempting to get learning rate from scheduler before it has started
[01.17 01:03:40] ┇ WARNING  ┇                                        DeepSpeed ┇ Attempting to get learning rate from scheduler before it has started
[01.17 01:03:40] ┇ WARNING  ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[01.17 01:03:40] ┇ WARNING  ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[01.17 01:03:40] ┇ INFO     ┇                                        DeepSpeed ┇ Before initializing optimizer states
[01.17 01:03:40] ┇ INFO     ┇                                        DeepSpeed ┇ MA 3.84 GB         Max_MA 4.47 GB         CA 4.65 GB         Max_CA 5 GB
[01.17 01:03:40] ┇ INFO     ┇                                        DeepSpeed ┇ CPU Virtual Memory:  used = 16.66 GB, percent = 6.6%
[01.17 01:03:40] ┇ WARNING  ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
[01.17 01:03:40] ┇ INFO     ┇                                        DeepSpeed ┇ After initializing optimizer states
[01.17 01:03:40] ┇ INFO     ┇                                        DeepSpeed ┇ MA 3.84 GB         Max_MA 5.09 GB         CA 5.9 GB         Max_CA 6 GB
[01.17 01:03:40] ┇ INFO     ┇                                        DeepSpeed ┇ CPU Virtual Memory:  used = 16.67 GB, percent = 6.6%
[01.17 01:03:40] ┇ INFO     ┇                                        DeepSpeed ┇ optimizer state initialized
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇ After initializing ZeRO optimizer
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇ MA 3.84 GB         Max_MA 3.84 GB         CA 5.9 GB         Max_CA 6 GB
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇ CPU Virtual Memory:  used = 16.67 GB, percent = 6.6%
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[01.17 01:03:41] ┇ WARNING  ┇                                        DeepSpeed ┇ Attempting to get learning rate from scheduler before it has started
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fb09391a840>
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇ DeepSpeedEngine configuration:
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   amp_enabled .................. False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   amp_params ................... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   bfloat16_enabled ............. True
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   bfloat16_immediate_grad_update  False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   checkpoint_parallel_write_pipeline  False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   checkpoint_tag_validation_enabled  True
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   checkpoint_tag_validation_fail  False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb09391a300>
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   communication_data_type ...... None
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   curriculum_enabled_legacy .... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   curriculum_params_legacy ..... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   data_efficiency_enabled ...... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   dataloader_drop_last ......... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   disable_allgather ............ False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   dump_state ................... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   dynamic_loss_scale_args ...... None
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   eigenvalue_enabled ........... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   eigenvalue_gas_boundary_resolution  1
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   eigenvalue_layer_name ........ bert.encoder.layer
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   eigenvalue_layer_num ......... 0
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   eigenvalue_max_iter .......... 100
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   eigenvalue_stability ......... 1e-06
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   eigenvalue_tol ............... 0.01
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   eigenvalue_verbose ........... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   elasticity_enabled ........... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   fp16_auto_cast ............... None
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   fp16_enabled ................. False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   fp16_master_weights_and_gradients  False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   global_rank .................. 0
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   grad_accum_dtype ............. None
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   gradient_accumulation_steps .. 4
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   gradient_clipping ............ 1.0
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   gradient_predivide_factor .... 1.0
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   graph_harvesting ............. False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   initial_dynamic_scale ........ 1
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   load_universal_checkpoint .... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   loss_scale ................... 1.0
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   memory_breakdown ............. False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   mics_hierarchial_params_gather  False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   mics_shard_size .............. -1
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   optimizer_legacy_fusion ...... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   optimizer_name ............... adamw
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   pld_enabled .................. False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   pld_params ................... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   prescale_gradients ........... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   scheduler_name ............... WarmupDecayLR
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 3, 'total_num_steps': 71}
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   seq_parallel_communication_data_type  torch.float32
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   sparse_attention ............. None
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   sparse_gradients_enabled ..... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   steps_per_print .............. inf
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   timers_config ................ enabled=True synchronized=True
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   train_batch_size ............. 128
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   train_micro_batch_size_per_gpu  8
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   use_data_before_expert_parallel_  False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   use_node_local_storage ....... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   wall_clock_breakdown ......... False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   weight_quantization_config ... None
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   world_size ................... 4
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   zero_allow_untested_optimizer  False
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   zero_enabled ................. True
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   zero_force_ds_cpu_optimizer .. True
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   zero_optimization_stage ...... 1
[01.17 01:03:41] ┇ INFO     ┇                                        DeepSpeed ┇   json = {
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 3,
            "total_num_steps": 71
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 128,
    "train_micro_batch_size_per_gpu": 8,
    "steps_per_print": inf,
    "fp16": {
        "enabled": false
    }
}
[01.17 01:03:41] ┇ INFO     ┇                             transformers.trainer ┇ ***** Running training *****
[01.17 01:03:41] ┇ INFO     ┇                             transformers.trainer ┇   Num examples = 18,135
[01.17 01:03:41] ┇ INFO     ┇                             transformers.trainer ┇   Num Epochs = 1
[01.17 01:03:41] ┇ INFO     ┇                             transformers.trainer ┇   Instantaneous batch size per device = 8
[01.17 01:03:41] ┇ INFO     ┇                             transformers.trainer ┇   Total train batch size (w. parallel, distributed & accumulation) = 128
[01.17 01:03:41] ┇ INFO     ┇                             transformers.trainer ┇   Gradient Accumulation steps = 4
[01.17 01:03:41] ┇ INFO     ┇                             transformers.trainer ┇   Total optimization steps = 71
[01.17 01:03:41] ┇ INFO     ┇                             transformers.trainer ┇   Number of trainable parameters = 1,341,247,488
  0%|                                                                                                                                           | 0/71 [00:00<?, ?it/s][01.17 01:03:41] ┇ WARNING  ┇   transformers.models.gpt_neox.modeling_gpt_neox ┇ `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
{'loss': 0.9358, 'grad_norm': 0.7520268559455872, 'learning_rate': 1.823529411764706e-05, 'epoch': 0.07}
{'loss': 0.1278, 'grad_norm': 0.31632018089294434, 'learning_rate': 1.5294117647058822e-05, 'epoch': 0.14}
{'loss': 0.0705, 'grad_norm': 0.19092705845832825, 'learning_rate': 1.235294117647059e-05, 'epoch': 0.21}
{'loss': 0.0532, 'grad_norm': 0.195684015750885, 'learning_rate': 9.411764705882354e-06, 'epoch': 0.28}
{'loss': 0.0477, 'grad_norm': 0.153150275349617, 'learning_rate': 6.470588235294119e-06, 'epoch': 0.35}
{'loss': 0.039, 'grad_norm': 0.10214919596910477, 'learning_rate': 3.529411764705883e-06, 'epoch': 0.42}
{'loss': 0.0407, 'grad_norm': 0.11457229405641556, 'learning_rate': 5.882352941176471e-07, 'epoch': 0.49}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71/71 [02:27<00:00,  1.98s/it][01.17 01:06:10] ┇ INFO     ┇                             transformers.trainer ┇ Saving model checkpoint to output/GNER/EAGLE-1B-debug/checkpoint-71
[01.17 01:06:10] ┇ INFO     ┇                 transformers.configuration_utils ┇ Configuration saved in output/GNER/EAGLE-1B-debug/checkpoint-71/config.json
[01.17 01:06:10] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Configuration saved in output/GNER/EAGLE-1B-debug/checkpoint-71/generation_config.json
[01.17 01:06:14] ┇ INFO     ┇                      transformers.modeling_utils ┇ Model weights saved in output/GNER/EAGLE-1B-debug/checkpoint-71/model.safetensors
[01.17 01:06:14] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ tokenizer config file saved in output/GNER/EAGLE-1B-debug/checkpoint-71/tokenizer_config.json
[01.17 01:06:14] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens file saved in output/GNER/EAGLE-1B-debug/checkpoint-71/special_tokens_map.json
[01.17 01:06:14] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] [Torch] Checkpoint global_step71 is about to be saved!
[01.17 01:06:15] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] Saving model checkpoint: output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/mp_rank_00_model_states.pt
[01.17 01:06:15] ┇ INFO     ┇                                        DeepSpeed ┇ [Torch] Saving output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/mp_rank_00_model_states.pt...
[01.17 01:06:21] ┇ INFO     ┇                                        DeepSpeed ┇ [Torch] Saved output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/mp_rank_00_model_states.pt.
[01.17 01:06:21] ┇ INFO     ┇                                        DeepSpeed ┇ [Torch] Saving output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[01.17 01:06:30] ┇ INFO     ┇                                        DeepSpeed ┇ [Torch] Saved output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[01.17 01:06:31] ┇ INFO     ┇                                        DeepSpeed ┇ zero checkpoint saved output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[01.17 01:06:31] ┇ INFO     ┇                                        DeepSpeed ┇ [Torch] Checkpoint global_step71 is ready now!
[01.17 01:06:31] ┇ INFO     ┇                             transformers.trainer ┇ ***** Running Evaluation *****
[01.17 01:06:31] ┇ INFO     ┇                             transformers.trainer ┇   Num examples = 65
[01.17 01:06:31] ┇ INFO     ┇                             transformers.trainer ┇   Batch size = 8
{'eval_crossner_ai_prec': 0.35532994923839833, 'eval_crossner_ai_rec': 0.31390134529133906, 'eval_crossner_ai_f1': 0.33333333328336623, 'eval_AVERAGE_f1': 0.33333333328336623, 'eval_runtime': 13.321, 'eval_samples_per_second': 4.88, 'eval_steps_per_second': 0.225, 'epoch': 0.5}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71/71 [03:03<00:00,  1.98s/it]
[01.17 01:06:45] ┇ INFO     ┇                             transformers.trainer ┇ Saving model checkpoint to output/GNER/EAGLE-1B-debug/checkpoint-71
[01.17 01:06:45] ┇ INFO     ┇                 transformers.configuration_utils ┇ Configuration saved in output/GNER/EAGLE-1B-debug/checkpoint-71/config.json
[01.17 01:06:45] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Configuration saved in output/GNER/EAGLE-1B-debug/checkpoint-71/generation_config.json
[01.17 01:07:30] ┇ INFO     ┇                      transformers.modeling_utils ┇ Model weights saved in output/GNER/EAGLE-1B-debug/checkpoint-71/model.safetensors
[01.17 01:07:30] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ tokenizer config file saved in output/GNER/EAGLE-1B-debug/checkpoint-71/tokenizer_config.json
[01.17 01:07:30] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens file saved in output/GNER/EAGLE-1B-debug/checkpoint-71/special_tokens_map.json
[01.17 01:07:30] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] [Torch] Checkpoint global_step71 is about to be saved!
[01.17 01:07:30] ┇ INFO     ┇                                        DeepSpeed ┇ [Rank 0] Saving model checkpoint: output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/mp_rank_00_model_states.pt
[01.17 01:07:30] ┇ INFO     ┇                                        DeepSpeed ┇ [Torch] Saving output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/mp_rank_00_model_states.pt...
[01.17 01:08:13] ┇ INFO     ┇                                        DeepSpeed ┇ [Torch] Saved output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/mp_rank_00_model_states.pt.
[01.17 01:08:13] ┇ INFO     ┇                                        DeepSpeed ┇ [Torch] Saving output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[01.17 01:10:44] ┇ INFO     ┇                                        DeepSpeed ┇ [Torch] Saved output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[01.17 01:10:44] ┇ INFO     ┇                                        DeepSpeed ┇ zero checkpoint saved output/GNER/EAGLE-1B-debug/checkpoint-71/global_step71/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[01.17 01:10:44] ┇ INFO     ┇                                        DeepSpeed ┇ [Torch] Checkpoint global_step71 is ready now!
[01.17 01:10:45] ┇ INFO     ┇                             transformers.trainer ┇

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 424.3038, 'train_samples_per_second': 21.37, 'train_steps_per_second': 0.167, 'train_loss': 0.1858822997187225, 'epoch': 0.5}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71/71 [07:04<00:00,  5.98s/it]
[01.17 01:10:45] ┇ INFO     ┇                                         __main__ ┇ train_result=TrainOutput(global_step=71, training_loss=0.1858822997187225, metrics={'train_runtime': 424.3038, 'train_samples_per_second': 21.37, 'train_steps_per_second': 0.167, 'total_flos': 2.2242260498251776e+16, 'train_loss': 0.1858822997187225, 'epoch': 0.5008818342151675})
[rank0]:[W117 01:10:46.076067656 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[2025-01-17 01:10:49,723] [INFO] [launch.py:351:main] Process 107439 exits successfully.
[2025-01-17 01:10:49,723] [INFO] [launch.py:351:main] Process 107440 exits successfully.
[2025-01-17 01:10:49,724] [INFO] [launch.py:351:main] Process 107438 exits successfully.
[2025-01-17 01:10:49,724] [INFO] [launch.py:351:main] Process 107441 exits successfully.
(DeepKNLP) chrisjihee@ptlm2:~/proj/DeepKNLP$
