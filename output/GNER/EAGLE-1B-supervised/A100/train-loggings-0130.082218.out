[01.30 08:22:24] ┇ INFO    ┇             DeepKNLP ┇ Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ =========================================================================================================================================
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ [INIT] python task2-nerG-trainer2.py --local_rank=0 --trainer_deepspeed configs/deepspeed/ds1_llama.json
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ =========================================================================================================================================
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ -----------------------------------------------------------------------------------------------------------------------------------------
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇   #	TrainingArgumentsForAccelerator              	value
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ -----------------------------------------------------------------------------------------------------------------------------------------
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇   1	env.hostname                                 	lirs-b1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇   2	env.hostaddr                                 	129.254.111.103
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇   3	env.global_rank                              	-1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇   4	env.local_rank                               	0
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇   5	env.node_rank                                	-1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇   6	env.world_size                               	4
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇   7	env.time_stamp                               	0130.082218
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇   8	env.python_path                              	/home/chrisjihee/miniforge3/envs/DeepKNLP/bin/python
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇   9	env.current_dir                              	/home/chrisjihee/proj/DeepKNLP
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  10	env.current_file                             	task2-nerG-trainer2.py
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  11	env.command_args                             	['--local_rank=0', '--trainer_deepspeed', 'configs/deepspeed/ds1_llama.json']
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  12	env.output_home                              	output
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  13	env.output_name                              	GNER
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  14	env.run_version                              	EAGLE-1B-debug
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  15	env.output_file                              	train-metrics-0130.082218.csv
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  16	env.logging_file                             	train-loggings-0130.082218.out
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  17	env.logging_level                            	30
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  18	env.logging_format                           	%(asctime)s ┇ %(levelname)-7s ┇ %(name)20s ┇ %(message)s
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  19	env.datetime_format                          	[%m.%d %H:%M:%S]
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  20	env.argument_file                            	train-arguments-0130.082218.json
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  21	env.random_seed                              	7
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  22	env.max_workers                              	4
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  23	env.debugging                                	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  24	env.output_dir                               	output/GNER/EAGLE-1B-debug
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  25	time.t1                                      	2025-01-30 08:22:24.658131
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  26	time.t2                                      	2025-01-30 08:22:18.184529
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  27	time.started                                 	[01.30 08:22:24]
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  28	time.settled
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  29	time.elapsed
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  30	data.pretrained                              	etri-lirs/egpt-1.3b-preview
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  31	data.train_file                              	data/gner/zero-shot-train.jsonl
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  32	data.study_file
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  33	data.eval_file                               	data/gner/zero-shot-dev-100.jsonl
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  34	data.pred_file                               	data/gner/zero-shot-test-100.jsonl
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  35	data.max_train_samples                       	-1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  36	data.max_study_samples                       	-1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  37	data.max_eval_samples                        	-1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  38	data.max_pred_samples                        	-1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  39	data.max_source_length                       	640
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  40	data.max_target_length                       	640
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  41	data.use_cache_data                          	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  42	data.ignore_pad_token_for_loss               	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  43	train.output_dir                             	output/GNER/EAGLE-1B-debug
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  44	train.overwrite_output_dir                   	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  45	train.do_train                               	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  46	train.do_eval                                	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  47	train.do_predict                             	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  48	train.eval_strategy                          	IntervalStrategy.NO
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  49	train.prediction_loss_only                   	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  50	train.per_device_train_batch_size            	8
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  51	train.per_device_eval_batch_size             	32
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  52	train.per_gpu_train_batch_size
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  53	train.per_gpu_eval_batch_size
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  54	train.gradient_accumulation_steps            	4
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  55	train.eval_accumulation_steps                	1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  56	train.eval_delay                             	0
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  57	train.torch_empty_cache_steps
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  58	train.learning_rate                          	2e-05
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  59	train.weight_decay                           	0.0
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  60	train.adam_beta1                             	0.9
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  61	train.adam_beta2                             	0.999
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  62	train.adam_epsilon                           	1e-08
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  63	train.max_grad_norm                          	1.0
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  64	train.num_train_epochs                       	6.0
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  65	train.max_steps                              	-1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  66	train.lr_scheduler_type                      	SchedulerType.COSINE
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  67	train.lr_scheduler_kwargs                    	{}
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  68	train.warmup_ratio                           	0.04
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  69	train.warmup_steps                           	0
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  70	train.log_level                              	warning
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  71	train.log_level_replica                      	error
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  72	train.log_on_each_node                       	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  73	train.logging_dir                            	output/GNER/EAGLE-1B-debug/runs/Jan30_08-22-23_lirs-b1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  74	train.logging_strategy                       	IntervalStrategy.NO
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  75	train.logging_first_step                     	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  76	train.logging_steps                          	0.1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  77	train.logging_nan_inf_filter                 	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  78	train.save_strategy                          	SaveStrategy.NO
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  79	train.save_steps                             	9223372036854775807
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  80	train.save_total_limit
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  81	train.save_safetensors                       	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  82	train.save_on_each_node                      	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  83	train.save_only_model                        	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  84	train.restore_callback_states_from_checkpoint	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  85	train.no_cuda                                	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  86	train.use_cpu                                	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  87	train.use_mps_device                         	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  88	train.seed                                   	7
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  89	train.data_seed
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  90	train.jit_mode_eval                          	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  91	train.use_ipex                               	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  92	train.bf16                                   	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  93	train.fp16                                   	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  94	train.fp16_opt_level                         	O1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  95	train.half_precision_backend                 	auto
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  96	train.bf16_full_eval                         	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  97	train.fp16_full_eval                         	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  98	train.tf32                                   	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇  99	train.local_rank                             	0
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 100	train.ddp_backend
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 101	train.tpu_num_cores
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 102	train.tpu_metrics_debug                      	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 103	train.debug                                  	[]
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 104	train.dataloader_drop_last                   	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 105	train.eval_steps                             	0.3333333333333333
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 106	train.dataloader_num_workers                 	0
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 107	train.dataloader_prefetch_factor
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 108	train.past_index                             	-1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 109	train.run_name                               	output/GNER/EAGLE-1B-debug
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 110	train.disable_tqdm                           	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 111	train.remove_unused_columns                  	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 112	train.label_names
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 113	train.load_best_model_at_end                 	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 114	train.metric_for_best_model
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 115	train.greater_is_better
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 116	train.ignore_data_skip                       	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 117	train.fsdp                                   	[]
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 118	train.fsdp_min_num_params                    	0
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 119	train.fsdp_config                            	{'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 120	train.fsdp_transformer_layer_cls_to_wrap
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 121	train.accelerator_config                     	{'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 122	train.deepspeed                              	configs/deepspeed/ds1_llama.json
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 123	train.label_smoothing_factor                 	0.0
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 124	train.optim                                  	OptimizerNames.ADAMW_TORCH
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 125	train.optim_args
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 126	train.adafactor                              	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 127	train.group_by_length                        	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 128	train.length_column_name                     	length
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 129	train.report_to                              	[]
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 130	train.ddp_find_unused_parameters
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 131	train.ddp_bucket_cap_mb
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 132	train.ddp_broadcast_buffers
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 133	train.dataloader_pin_memory                  	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 134	train.dataloader_persistent_workers          	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 135	train.skip_memory_metrics                    	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 136	train.use_legacy_prediction_loop             	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 137	train.push_to_hub                            	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 138	train.resume_from_checkpoint
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 139	train.hub_model_id
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 140	train.hub_strategy                           	HubStrategy.EVERY_SAVE
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 141	train.hub_token
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 142	train.hub_private_repo
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 143	train.hub_always_push                        	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 144	train.gradient_checkpointing                 	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 145	train.gradient_checkpointing_kwargs
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 146	train.include_inputs_for_metrics             	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 147	train.include_for_metrics                    	[]
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 148	train.eval_do_concat_batches                 	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 149	train.fp16_backend                           	auto
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 150	train.evaluation_strategy
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 151	train.push_to_hub_model_id
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 152	train.push_to_hub_organization
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 153	train.push_to_hub_token
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 154	train._n_gpu                                 	1
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 155	train.mp_parameters
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 156	train.auto_find_batch_size                   	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 157	train.full_determinism                       	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 158	train.torchdynamo
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 159	train.ray_scope                              	last
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 160	train.ddp_timeout                            	1800
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 161	train.torch_compile                          	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 162	train.torch_compile_backend
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 163	train.torch_compile_mode
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 164	train.dispatch_batches
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 165	train.split_batches
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 166	train.include_tokens_per_second              	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 167	train.include_num_input_tokens_seen          	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 168	train.neftune_noise_alpha
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 169	train.optim_target_modules
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 170	train.batch_eval_metrics                     	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 171	train.eval_on_start                          	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 172	train.use_liger_kernel                       	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 173	train.eval_use_gather_object                 	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 174	train.average_tokens_across_devices          	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 175	train.sortish_sampler                        	False
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 176	train.predict_with_generate                  	True
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 177	train.generation_max_length                  	640
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 178	train.generation_num_beams
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ 179	train.generation_config
[01.30 08:22:24] ┇ INFO    ┇       chrisbase.data ┇ -----------------------------------------------------------------------------------------------------------------------------------------
[01.30 08:22:33] ┇ INFO    ┇             DeepKNLP ┇ type(model)=<class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>
[01.30 08:22:33] ┇ INFO    ┇             DeepKNLP ┇ model.generation_config.pad_token_id=0
[01.30 08:22:34] ┇ INFO    ┇             DeepKNLP ┇ Loaded raw train_dataset (#=18135): data/gner/zero-shot-train.jsonl
[01.30 08:22:34] ┇ INFO    ┇             DeepKNLP ┇ Completed preprocessing for train_dataset at [01.28 02:46:40]
[01.30 08:22:34] ┇ INFO    ┇             DeepKNLP ┇ Loaded raw eval_dataset (#=700): data/gner/zero-shot-dev-100.jsonl
[01.30 08:22:34] ┇ INFO    ┇             DeepKNLP ┇ Completed preprocessing for eval_dataset at [01.30 08:18:52]
[01.30 08:22:34] ┇ INFO    ┇ transformers.trainer ┇ Using auto half precision backend
[01.30 08:22:34] ┇ INFO    ┇             DeepKNLP ┇ Using deepspeed configuration:
{
  "fp16": {
    "enabled": false
  },
  "bf16": {
    "enabled": true
  },
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 2e-05,
      "betas": [
        0.9,
        0.999
      ],
      "eps": 1e-08,
      "weight_decay": 0.0
    }
  },
  "scheduler": {
    "type": "WarmupDecayLR",
    "params": {
      "warmup_min_lr": 0,
      "warmup_max_lr": 2e-05,
      "warmup_num_steps": "auto",
      "total_num_steps": "auto"
    }
  },
  "zero_optimization": {
    "stage": 1,
    "allgather_partitions": true,
    "allgather_bucket_size": 200000000.0,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 200000000.0,
    "contiguous_gradients": true
  },
  "gradient_accumulation_steps": 4,
  "gradient_clipping": 1.0,
  "train_batch_size": 128,
  "train_micro_batch_size_per_gpu": 8,
  "steps_per_print": Infinity
}
[01.30 08:22:59] ┇ WARNING ┇            DeepSpeed ┇ Attempting to get learning rate from scheduler before it has started
[01.30 08:22:59] ┇ INFO    ┇ transformers.trainer ┇ ***** Running training *****
[01.30 08:22:59] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 18,135
[01.30 08:22:59] ┇ INFO    ┇ transformers.trainer ┇   Num Epochs = 6
[01.30 08:22:59] ┇ INFO    ┇ transformers.trainer ┇   Instantaneous batch size per device = 8
[01.30 08:22:59] ┇ INFO    ┇ transformers.trainer ┇   Total train batch size (w. parallel, distributed & accumulation) = 128
[01.30 08:22:59] ┇ INFO    ┇ transformers.trainer ┇   Gradient Accumulation steps = 4
[01.30 08:22:59] ┇ INFO    ┇ transformers.trainer ┇   Total optimization steps = 846
[01.30 08:22:59] ┇ INFO    ┇ transformers.trainer ┇   Number of trainable parameters = 1,341,247,488
[01.30 08:23:01] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   0%|                    |   1/846 [0:00:02<0:28:40, 0.49Hz] | (Ep 0.007)
[01.30 08:23:06] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   1%|▏                   |   7/846 [0:00:07<0:15:23, 0.91Hz] | (Ep 0.050)
[01.30 08:23:12] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   2%|▎                   |  13/846 [0:00:12<0:13:50, 1.00Hz] | (Ep 0.092)
[01.30 08:23:13] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:23:13] ┇ INFO    ┇             DeepKNLP ┇ >>     14    0.099  0.8548      2.45651      1.49676e-05
[01.30 08:23:17] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   2%|▍                   |  19/846 [0:00:18<0:13:16, 1.04Hz] | (Ep 0.135)
[01.30 08:23:22] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   3%|▌                   |  25/846 [0:00:23<0:12:52, 1.06Hz] | (Ep 0.177)
[01.30 08:23:25] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:23:25] ┇ INFO    ┇             DeepKNLP ┇ >>     28    0.199  0.1176     0.354743      1.88988e-05
[01.30 08:23:28] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   4%|▋                   |  31/846 [0:00:29<0:12:48, 1.06Hz] | (Ep 0.220)
[01.30 08:23:33] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   4%|▊                   |  37/846 [0:00:34<0:12:34, 1.07Hz] | (Ep 0.262)
[01.30 08:23:38] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:23:38] ┇ INFO    ┇             DeepKNLP ┇ >>     42    0.298  0.0599     0.171747      1.98276e-05
[01.30 08:23:39] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   5%|█                   |  43/846 [0:00:40<0:12:35, 1.06Hz] | (Ep 0.305)
[01.30 08:23:43] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:23:43] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:23:43] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:23:49] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 47297.23Hz]
[01.30 08:23:56] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:07<0:00:14, 0.28Hz]
[01.30 08:24:04] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:14<0:00:14, 0.21Hz]
[01.30 08:24:11] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:21<0:00:10, 0.19Hz]
[01.30 08:24:19] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.20Hz]
[01.30 08:24:20] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:24:20] ┇ INFO    ┇             DeepKNLP ┇ >>     47    0.333            0.299094                    0.343066               0.318501                  0.416305                 0.399535           0.79902                   0.66        0.462217           37.25                     18.792                    0.161
[01.30 08:24:21] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   6%|█▏                  |  48/846 [0:01:22<0:22:47, 0.58Hz] | (Ep 0.340)
[01.30 08:24:26] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   6%|█▎                  |  54/846 [0:01:27<0:21:28, 0.61Hz] | (Ep 0.383)
[01.30 08:24:28] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:24:28] ┇ INFO    ┇             DeepKNLP ┇ >>     56    0.397  0.0433     0.122508      1.94828e-05
[01.30 08:24:32] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   7%|█▍                  |  60/846 [0:01:33<0:20:23, 0.64Hz] | (Ep 0.426)
[01.30 08:24:37] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   8%|█▌                  |  66/846 [0:01:38<0:19:32, 0.67Hz] | (Ep 0.468)
[01.30 08:24:41] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:24:41] ┇ INFO    ┇             DeepKNLP ┇ >>     70    0.496  0.0378     0.161347      1.91379e-05
[01.30 08:24:42] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   9%|█▋                  |  72/846 [0:01:43<0:19:16, 0.67Hz] | (Ep 0.511)
[01.30 08:24:47] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   9%|█▊                  |  78/846 [0:01:48<0:19:06, 0.67Hz] | (Ep 0.553)
[01.30 08:24:53] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  10%|█▉                  |  84/846 [0:01:54<0:19:01, 0.67Hz] | (Ep 0.596)
[01.30 08:24:54] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:24:54] ┇ INFO    ┇             DeepKNLP ┇ >>     85    0.603  0.0355    0.0808698      1.87685e-05
[01.30 08:24:58] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  11%|██                  |  89/846 [0:01:59<0:19:06, 0.66Hz] | (Ep 0.631)
[01.30 08:25:03] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:25:03] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:25:03] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:25:10] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 71841.42Hz]
[01.30 08:25:17] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:07<0:00:14, 0.28Hz]
[01.30 08:25:24] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:14<0:00:14, 0.21Hz]
[01.30 08:25:32] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:21<0:00:10, 0.19Hz]
[01.30 08:25:40] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.20Hz]
[01.30 08:25:41] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:25:41] ┇ INFO    ┇             DeepKNLP ┇ >>     94    0.667            0.413392                    0.532731                0.56108                   0.50535                 0.559188          0.879607               0.794258        0.606515         37.0996                     18.868                    0.162
[01.30 08:25:41] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  11%|██▏                 |  95/846 [0:02:42<0:26:24, 0.47Hz] | (Ep 0.674)
[01.30 08:25:45] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:25:45] ┇ INFO    ┇             DeepKNLP ┇ >>     99    0.702  0.0337    0.0830959      1.84236e-05
[01.30 08:25:47] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  12%|██▍                 | 101/846 [0:02:48<0:26:12, 0.47Hz] | (Ep 0.716)
[01.30 08:25:52] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  13%|██▌                 | 106/846 [0:02:53<0:26:04, 0.47Hz] | (Ep 0.752)
[01.30 08:25:58] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  13%|██▋                 | 112/846 [0:02:59<0:18:39, 0.66Hz] | (Ep 0.794)
[01.30 08:25:59] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:25:59] ┇ INFO    ┇             DeepKNLP ┇ >>    113    0.801  0.0318     0.114175      1.80788e-05
[01.30 08:26:04] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  14%|██▊                 | 118/846 [0:03:04<0:18:32, 0.65Hz] | (Ep 0.837)
[01.30 08:26:09] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  15%|██▉                 | 123/846 [0:03:09<0:18:28, 0.65Hz] | (Ep 0.872)
[01.30 08:26:12] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:26:12] ┇ INFO    ┇             DeepKNLP ┇ >>    127    0.901  0.0331     0.105214       1.7734e-05
[01.30 08:26:14] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  15%|███                 | 129/846 [0:03:15<0:18:22, 0.65Hz] | (Ep 0.915)
[01.30 08:26:20] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  16%|███▏                | 135/846 [0:03:21<0:18:19, 0.65Hz] | (Ep 0.957)
[01.30 08:26:25] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  17%|███▎                | 141/846 [0:03:26<0:18:15, 0.64Hz] | (Ep 1.000)
[01.30 08:26:25] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:26:25] ┇ INFO    ┇             DeepKNLP ┇ >>    141        1  0.0306    0.0838265      1.73892e-05
[01.30 08:26:25] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:26:25] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:26:25] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:26:32] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 109453.80Hz]
[01.30 08:26:39] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:06<0:00:13, 0.29Hz]
[01.30 08:26:46] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:13<0:00:13, 0.21Hz]
[01.30 08:26:53] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:21<0:00:10, 0.19Hz]
[01.30 08:27:01] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.20Hz]
[01.30 08:27:02] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:27:02] ┇ INFO    ┇             DeepKNLP ┇ >>    141        1            0.471111                    0.599068               0.586861                  0.589492                  0.62543          0.866995               0.807786        0.649535         36.8322                     19.005                    0.163
[01.30 08:27:03] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  17%|███▎                | 142/846 [0:04:04<0:25:00, 0.47Hz] | (Ep 1.007)
[01.30 08:27:03] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:27:03] ┇ INFO    ┇             DeepKNLP ┇ >>    142    1.007  0.0268    0.0838265      1.73892e-05
[01.30 08:27:08] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  17%|███▍                | 148/846 [0:04:09<0:24:47, 0.47Hz] | (Ep 1.050)
[01.30 08:27:14] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  18%|███▋                | 154/846 [0:04:15<0:24:29, 0.47Hz] | (Ep 1.092)
[01.30 08:27:15] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:27:15] ┇ INFO    ┇             DeepKNLP ┇ >>    155    1.099  0.0228    0.0284609       1.7069e-05
[01.30 08:27:20] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  19%|███▊                | 160/846 [0:04:21<0:17:27, 0.65Hz] | (Ep 1.135)
[01.30 08:27:25] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  20%|███▉                | 166/846 [0:04:26<0:17:18, 0.65Hz] | (Ep 1.177)
[01.30 08:27:28] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:27:28] ┇ INFO    ┇             DeepKNLP ┇ >>    169    1.199  0.0237    0.0301451      1.67241e-05
[01.30 08:27:31] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  20%|████                | 172/846 [0:04:32<0:17:05, 0.66Hz] | (Ep 1.220)
[01.30 08:27:36] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  21%|████▏               | 178/846 [0:04:37<0:16:53, 0.66Hz] | (Ep 1.262)
[01.30 08:27:41] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  22%|████▎               | 183/846 [0:04:42<0:16:50, 0.66Hz] | (Ep 1.298)
[01.30 08:27:42] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:27:42] ┇ INFO    ┇             DeepKNLP ┇ >>    183    1.298  0.0222    0.0427867      1.63793e-05
[01.30 08:27:46] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:27:46] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:27:46] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:27:53] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 69696.34Hz]
[01.30 08:28:00] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:06<0:00:13, 0.29Hz]
[01.30 08:28:07] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:13<0:00:13, 0.21Hz]
[01.30 08:28:14] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:21<0:00:10, 0.19Hz]
[01.30 08:28:22] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.20Hz]
[01.30 08:28:23] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:28:23] ┇ INFO    ┇             DeepKNLP ┇ >>    188    1.333            0.497791                    0.513357               0.639485                  0.686258                  0.53085          0.879607               0.755981        0.643333         37.1058                     18.865                    0.162
[01.30 08:28:24] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  22%|████▍               | 189/846 [0:05:25<0:23:07, 0.47Hz] | (Ep 1.340)
[01.30 08:28:30] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  23%|████▌               | 195/846 [0:05:31<0:22:50, 0.48Hz] | (Ep 1.383)
[01.30 08:28:32] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:28:32] ┇ INFO    ┇             DeepKNLP ┇ >>    197    1.397  0.0239    0.0417651      1.60345e-05
[01.30 08:28:35] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  24%|████▊               | 201/846 [0:05:36<0:22:37, 0.48Hz] | (Ep 1.426)
[01.30 08:28:41] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  24%|████▉               | 207/846 [0:05:42<0:16:12, 0.66Hz] | (Ep 1.468)
[01.30 08:28:44] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:28:44] ┇ INFO    ┇             DeepKNLP ┇ >>    211    1.496  0.0223    0.0379755      1.56897e-05
[01.30 08:28:46] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  25%|█████               | 213/846 [0:05:47<0:16:04, 0.66Hz] | (Ep 1.511)
[01.30 08:28:52] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  26%|█████▏              | 219/846 [0:05:53<0:15:52, 0.66Hz] | (Ep 1.553)
[01.30 08:28:57] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  27%|█████▎              | 225/846 [0:05:58<0:15:40, 0.66Hz] | (Ep 1.596)
[01.30 08:28:59] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:28:59] ┇ INFO    ┇             DeepKNLP ┇ >>    226    1.603  0.0222    0.0281647      1.53202e-05
[01.30 08:29:03] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  27%|█████▍              | 231/846 [0:06:04<0:15:33, 0.66Hz] | (Ep 1.638)
[01.30 08:29:07] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:29:07] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:29:07] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:29:14] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 85082.55Hz]
[01.30 08:29:20] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:06<0:00:13, 0.29Hz]
[01.30 08:29:28] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:13<0:00:13, 0.21Hz]
[01.30 08:29:34] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:20<0:00:10, 0.19Hz]
[01.30 08:29:43] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.20Hz]
[01.30 08:29:44] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:29:44] ┇ INFO    ┇             DeepKNLP ┇ >>    235    1.667            0.505232                    0.589499               0.690058                  0.679862                 0.621336           0.89604               0.774194         0.67946         36.9972                      18.92                    0.162
[01.30 08:29:45] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  28%|█████▌              | 236/846 [0:06:46<0:21:26, 0.47Hz] | (Ep 1.674)
[01.30 08:29:48] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:29:48] ┇ INFO    ┇             DeepKNLP ┇ >>    240    1.702   0.021    0.0501381      1.49754e-05
[01.30 08:29:50] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  29%|█████▋              | 242/846 [0:06:51<0:21:15, 0.47Hz] | (Ep 1.716)
[01.30 08:29:56] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  29%|█████▊              | 248/846 [0:06:57<0:20:58, 0.48Hz] | (Ep 1.759)
[01.30 08:30:01] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  30%|██████              | 254/846 [0:07:02<0:14:56, 0.66Hz] | (Ep 1.801)
[01.30 08:30:02] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:30:02] ┇ INFO    ┇             DeepKNLP ┇ >>    254    1.801  0.0212    0.0365287      1.46305e-05
[01.30 08:30:07] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  31%|██████▏             | 260/846 [0:07:08<0:14:50, 0.66Hz] | (Ep 1.844)
[01.30 08:30:13] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  31%|██████▎             | 266/846 [0:07:14<0:14:40, 0.66Hz] | (Ep 1.887)
[01.30 08:30:15] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:30:15] ┇ INFO    ┇             DeepKNLP ┇ >>    268    1.901   0.022    0.0268715      1.42857e-05
[01.30 08:30:18] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  32%|██████▍             | 272/846 [0:07:19<0:14:31, 0.66Hz] | (Ep 1.929)
[01.30 08:30:24] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  33%|██████▌             | 278/846 [0:07:25<0:14:24, 0.66Hz] | (Ep 1.972)
[01.30 08:30:28] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:30:28] ┇ INFO    ┇             DeepKNLP ┇ >>    282        2  0.0228    0.0436238      1.39409e-05
[01.30 08:30:28] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:30:28] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:30:28] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:30:35] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 61112.23Hz]
[01.30 08:30:42] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:06<0:00:13, 0.29Hz]
[01.30 08:30:49] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:13<0:00:13, 0.21Hz]
[01.30 08:30:56] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:20<0:00:10, 0.19Hz]
[01.30 08:31:04] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.20Hz]
[01.30 08:31:05] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:31:05] ┇ INFO    ┇             DeepKNLP ┇ >>    282        2            0.393701                    0.565164                0.63609                  0.686067                 0.616114          0.852217               0.813559        0.651845         37.4895                     18.672                     0.16
[01.30 08:31:06] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  33%|██████▋             | 283/846 [0:08:07<0:19:51, 0.47Hz] | (Ep 2.007)
[01.30 08:31:07] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:31:07] ┇ INFO    ┇             DeepKNLP ┇ >>    284    2.014  0.0186    0.0706236      1.38916e-05
[01.30 08:31:12] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  34%|██████▊             | 289/846 [0:08:12<0:19:35, 0.47Hz] | (Ep 2.050)
[01.30 08:31:17] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  35%|██████▉             | 295/846 [0:08:18<0:19:19, 0.48Hz] | (Ep 2.092)
[01.30 08:31:18] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:31:18] ┇ INFO    ┇             DeepKNLP ┇ >>    296    2.099  0.0195    0.0400683      1.35961e-05
[01.30 08:31:22] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  36%|███████             | 301/846 [0:08:23<0:13:49, 0.66Hz] | (Ep 2.135)
[01.30 08:31:28] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  36%|███████▎            | 307/846 [0:08:29<0:13:38, 0.66Hz] | (Ep 2.177)
[01.30 08:31:31] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:31:31] ┇ INFO    ┇             DeepKNLP ┇ >>    310    2.199  0.0195    0.0525864      1.32512e-05
[01.30 08:31:33] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  37%|███████▍            | 313/846 [0:08:34<0:13:25, 0.66Hz] | (Ep 2.220)
[01.30 08:31:39] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  38%|███████▌            | 319/846 [0:08:40<0:13:17, 0.66Hz] | (Ep 2.262)
[01.30 08:31:44] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  38%|███████▋            | 324/846 [0:08:45<0:13:16, 0.66Hz] | (Ep 2.298)
[01.30 08:31:44] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:31:44] ┇ INFO    ┇             DeepKNLP ┇ >>    324    2.298    0.02    0.0597878      1.29064e-05
[01.30 08:31:48] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:31:48] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:31:48] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:31:56] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 102047.31Hz]
[01.30 08:32:03] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:06<0:00:13, 0.29Hz]
[01.30 08:32:10] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:13<0:00:13, 0.21Hz]
[01.30 08:32:17] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:21<0:00:10, 0.19Hz]
[01.30 08:32:25] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.20Hz]
[01.30 08:32:26] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:32:26] ┇ INFO    ┇             DeepKNLP ┇ >>    329    2.333            0.507463                    0.602353               0.681319                  0.692177                 0.671186          0.899756               0.796117         0.69291         37.7266                     18.555                    0.159
[01.30 08:32:27] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  39%|███████▊            | 330/846 [0:09:28<0:18:13, 0.47Hz] | (Ep 2.340)
[01.30 08:32:33] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  40%|███████▉            | 336/846 [0:09:34<0:18:03, 0.47Hz] | (Ep 2.383)
[01.30 08:32:35] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:32:35] ┇ INFO    ┇             DeepKNLP ┇ >>    338    2.397  0.0184    0.0520882      1.25616e-05
[01.30 08:32:38] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  40%|████████            | 342/846 [0:09:39<0:17:47, 0.47Hz] | (Ep 2.426)
[01.30 08:32:44] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  41%|████████▏           | 348/846 [0:09:45<0:12:41, 0.65Hz] | (Ep 2.468)
[01.30 08:32:48] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:32:48] ┇ INFO    ┇             DeepKNLP ┇ >>    352    2.496  0.0186    0.0425893      1.22167e-05
[01.30 08:32:50] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  42%|████████▎           | 354/846 [0:09:51<0:12:32, 0.65Hz] | (Ep 2.511)
[01.30 08:32:55] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  43%|████████▌           | 360/846 [0:09:56<0:12:24, 0.65Hz] | (Ep 2.553)
[01.30 08:33:00] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  43%|████████▋           | 365/846 [0:10:01<0:12:21, 0.65Hz] | (Ep 2.589)
[01.30 08:33:02] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:33:02] ┇ INFO    ┇             DeepKNLP ┇ >>    367    2.603  0.0197    0.0391255      1.18473e-05
[01.30 08:33:06] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  44%|████████▊           | 371/846 [0:10:07<0:12:09, 0.65Hz] | (Ep 2.631)
[01.30 08:33:10] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:33:10] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:33:10] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:33:17] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 75604.97Hz]
[01.30 08:33:24] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:06<0:00:13, 0.29Hz]
[01.30 08:33:31] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:13<0:00:13, 0.21Hz]
[01.30 08:33:38] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:20<0:00:10, 0.19Hz]
[01.30 08:33:47] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.21Hz]
[01.30 08:33:47] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:33:47] ┇ INFO    ┇             DeepKNLP ┇ >>    376    2.667            0.530928                    0.583991               0.697183                  0.682372                 0.665193          0.889976               0.793349        0.691856         37.1193                     18.858                    0.162
[01.30 08:33:48] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  45%|████████▉           | 377/846 [0:10:49<0:16:38, 0.47Hz] | (Ep 2.674)
[01.30 08:33:52] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:33:52] ┇ INFO    ┇             DeepKNLP ┇ >>    381    2.702  0.0193    0.0434824      1.15025e-05
[01.30 08:33:54] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  45%|█████████           | 383/846 [0:10:55<0:16:23, 0.47Hz] | (Ep 2.716)
[01.30 08:33:59] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  46%|█████████▏          | 389/846 [0:11:00<0:16:07, 0.47Hz] | (Ep 2.759)
[01.30 08:34:04] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  47%|█████████▎          | 395/846 [0:11:05<0:11:20, 0.66Hz] | (Ep 2.801)
[01.30 08:34:04] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:34:04] ┇ INFO    ┇             DeepKNLP ┇ >>    395    2.801  0.0215    0.0511133      1.11576e-05
[01.30 08:34:10] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  47%|█████████▍          | 400/846 [0:11:11<0:11:19, 0.66Hz] | (Ep 2.837)
[01.30 08:34:15] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  48%|█████████▌          | 405/846 [0:11:16<0:11:17, 0.65Hz] | (Ep 2.872)
[01.30 08:34:18] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:34:18] ┇ INFO    ┇             DeepKNLP ┇ >>    409    2.901  0.0221    0.0455739      1.08128e-05
[01.30 08:34:20] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  49%|█████████▋          | 411/846 [0:11:21<0:11:04, 0.65Hz] | (Ep 2.915)
[01.30 08:34:26] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  49%|█████████▊          | 417/846 [0:11:27<0:10:55, 0.65Hz] | (Ep 2.957)
[01.30 08:34:32] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  50%|██████████          | 423/846 [0:11:32<0:10:48, 0.65Hz] | (Ep 3.000)
[01.30 08:34:32] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:34:32] ┇ INFO    ┇             DeepKNLP ┇ >>    423        3  0.0196    0.0395683       1.0468e-05
[01.30 08:34:32] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:34:32] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:34:32] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:34:39] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 96890.62Hz]
[01.30 08:34:46] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:07<0:00:14, 0.28Hz]
[01.30 08:34:53] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:14<0:00:14, 0.21Hz]
[01.30 08:35:00] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:21<0:00:10, 0.19Hz]
[01.30 08:35:08] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.20Hz]
[01.30 08:35:09] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:35:09] ┇ INFO    ┇             DeepKNLP ┇ >>    423        3            0.568336                    0.605856               0.670471                  0.713344                 0.685457          0.900726               0.781176        0.703624         37.3154                     18.759                    0.161
[01.30 08:35:10] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  50%|██████████          | 424/846 [0:12:11<0:14:55, 0.47Hz] | (Ep 3.007)
[01.30 08:35:12] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:35:12] ┇ INFO    ┇             DeepKNLP ┇ >>    426    3.021  0.0177     0.048852      1.03941e-05
[01.30 08:35:15] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  51%|██████████▏         | 430/846 [0:12:16<0:14:38, 0.47Hz] | (Ep 3.050)
[01.30 08:35:21] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  52%|██████████▎         | 436/846 [0:12:22<0:14:25, 0.47Hz] | (Ep 3.092)
[01.30 08:35:22] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:35:22] ┇ INFO    ┇             DeepKNLP ┇ >>    437    3.099  0.0156    0.0498247      1.01232e-05
[01.30 08:35:26] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  52%|██████████▍         | 442/846 [0:12:27<0:10:17, 0.65Hz] | (Ep 3.135)
[01.30 08:35:32] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  53%|██████████▌         | 448/846 [0:12:33<0:10:07, 0.66Hz] | (Ep 3.177)
[01.30 08:35:34] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:35:34] ┇ INFO    ┇             DeepKNLP ┇ >>    451    3.199  0.0141     0.070268      9.77833e-06
[01.30 08:35:37] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  54%|██████████▋         | 454/846 [0:12:38<0:10:00, 0.65Hz] | (Ep 3.220)
[01.30 08:35:43] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  54%|██████████▊         | 460/846 [0:12:44<0:09:50, 0.65Hz] | (Ep 3.262)
[01.30 08:35:47] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:35:47] ┇ INFO    ┇             DeepKNLP ┇ >>    465    3.298  0.0156    0.0495124       9.4335e-06
[01.30 08:35:48] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  55%|███████████         | 466/846 [0:12:49<0:09:33, 0.66Hz] | (Ep 3.305)
[01.30 08:35:52] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:35:52] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:35:52] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:35:59] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 123931.42Hz]
[01.30 08:36:06] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:06<0:00:13, 0.29Hz]
[01.30 08:36:13] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:13<0:00:13, 0.21Hz]
[01.30 08:36:20] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:20<0:00:10, 0.19Hz]
[01.30 08:36:28] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.21Hz]
[01.30 08:36:29] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:36:29] ┇ INFO    ┇             DeepKNLP ┇ >>    470    3.333            0.598621                         0.6                0.64956                  0.710744                  0.66129          0.909976               0.774347        0.700648         36.9021                     18.969                    0.163
[01.30 08:36:30] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  56%|███████████▏        | 471/846 [0:13:31<0:13:06, 0.48Hz] | (Ep 3.340)
[01.30 08:36:35] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  56%|███████████▎        | 477/846 [0:13:36<0:12:52, 0.48Hz] | (Ep 3.383)
[01.30 08:36:37] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:36:37] ┇ INFO    ┇             DeepKNLP ┇ >>    479    3.397  0.0153    0.0547671      9.08867e-06
[01.30 08:36:41] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  57%|███████████▍        | 483/846 [0:13:42<0:12:42, 0.48Hz] | (Ep 3.426)
[01.30 08:36:47] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  58%|███████████▌        | 489/846 [0:13:48<0:09:00, 0.66Hz] | (Ep 3.468)
[01.30 08:36:50] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:36:50] ┇ INFO    ┇             DeepKNLP ┇ >>    493    3.496   0.015    0.0594606      8.74384e-06
[01.30 08:36:52] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  59%|███████████▋        | 495/846 [0:13:53<0:08:49, 0.66Hz] | (Ep 3.511)
[01.30 08:36:58] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  59%|███████████▊        | 501/846 [0:13:59<0:08:41, 0.66Hz] | (Ep 3.553)
[01.30 08:37:03] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  60%|███████████▉        | 507/846 [0:14:04<0:08:32, 0.66Hz] | (Ep 3.596)
[01.30 08:37:04] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:37:04] ┇ INFO    ┇             DeepKNLP ┇ >>    508    3.603  0.0158    0.0360586      8.37438e-06
[01.30 08:37:09] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  61%|████████████▏       | 513/846 [0:14:10<0:08:23, 0.66Hz] | (Ep 3.638)
[01.30 08:37:13] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:37:13] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:37:13] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:37:20] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 79137.81Hz]
[01.30 08:37:26] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:06<0:00:13, 0.29Hz]
[01.30 08:37:33] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:13<0:00:13, 0.21Hz]
[01.30 08:37:40] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:20<0:00:10, 0.19Hz]
[01.30 08:37:49] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.21Hz]
[01.30 08:37:50] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:37:50] ┇ INFO    ┇             DeepKNLP ┇ >>    517    3.667             0.60078                    0.614679               0.698917                  0.734095                  0.70903          0.893204               0.754717         0.71506         36.9594                      18.94                    0.162
[01.30 08:37:50] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  61%|████████████▏       | 518/846 [0:14:51<0:11:28, 0.48Hz] | (Ep 3.674)
[01.30 08:37:54] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:37:54] ┇ INFO    ┇             DeepKNLP ┇ >>    522    3.702  0.0146    0.0576139      8.02956e-06
[01.30 08:37:56] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  62%|████████████▍       | 524/846 [0:14:57<0:11:17, 0.48Hz] | (Ep 3.716)
[01.30 08:38:02] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  63%|████████████▌       | 530/846 [0:15:03<0:11:05, 0.47Hz] | (Ep 3.759)
[01.30 08:38:07] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  63%|████████████▋       | 536/846 [0:15:08<0:07:50, 0.66Hz] | (Ep 3.801)
[01.30 08:38:07] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:38:07] ┇ INFO    ┇             DeepKNLP ┇ >>    536    3.801  0.0147    0.0530139      7.68473e-06
[01.30 08:38:12] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  64%|████████████▊       | 541/846 [0:15:13<0:07:45, 0.66Hz] | (Ep 3.837)
[01.30 08:38:18] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  65%|████████████▉       | 547/846 [0:15:19<0:07:37, 0.65Hz] | (Ep 3.879)
[01.30 08:38:21] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:38:21] ┇ INFO    ┇             DeepKNLP ┇ >>    550    3.901  0.0154    0.0691062       7.3399e-06
[01.30 08:38:24] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  65%|█████████████       | 553/846 [0:15:25<0:07:27, 0.65Hz] | (Ep 3.922)
[01.30 08:38:29] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  66%|█████████████▏      | 559/846 [0:15:30<0:07:19, 0.65Hz] | (Ep 3.965)
[01.30 08:38:34] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:38:34] ┇ INFO    ┇             DeepKNLP ┇ >>    564        4  0.0155    0.0529246      6.99507e-06
[01.30 08:38:34] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:38:34] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:38:34] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:38:41] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 111778.25Hz]
[01.30 08:38:48] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:07<0:00:14, 0.28Hz]
[01.30 08:38:55] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:14<0:00:14, 0.21Hz]
[01.30 08:39:02] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:21<0:00:10, 0.19Hz]
[01.30 08:39:10] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.20Hz]
[01.30 08:39:11] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:39:11] ┇ INFO    ┇             DeepKNLP ┇ >>    564        4            0.606897                    0.596737               0.686131                  0.703259                 0.665901          0.892683                0.80285         0.70778         37.0431                     18.897                    0.162
[01.30 08:39:12] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  67%|█████████████▎      | 565/846 [0:16:13<0:09:54, 0.47Hz] | (Ep 4.007)
[01.30 08:39:15] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:39:15] ┇ INFO    ┇             DeepKNLP ┇ >>    568    4.028  0.0133    0.0467527      6.89655e-06
[01.30 08:39:18] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  67%|█████████████▍      | 571/846 [0:16:19<0:09:42, 0.47Hz] | (Ep 4.050)
[01.30 08:39:23] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  68%|█████████████▋      | 577/846 [0:16:24<0:09:29, 0.47Hz] | (Ep 4.092)
[01.30 08:39:24] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:39:24] ┇ INFO    ┇             DeepKNLP ┇ >>    578    4.099  0.0091    0.0600183      6.65025e-06
[01.30 08:39:29] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  69%|█████████████▊      | 583/846 [0:16:30<0:06:42, 0.65Hz] | (Ep 4.135)
[01.30 08:39:35] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  70%|█████████████▉      | 589/846 [0:16:36<0:06:34, 0.65Hz] | (Ep 4.177)
[01.30 08:39:37] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:39:37] ┇ INFO    ┇             DeepKNLP ┇ >>    592    4.199   0.009    0.0957984      6.30542e-06
[01.30 08:39:40] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  70%|██████████████      | 594/846 [0:16:41<0:06:28, 0.65Hz] | (Ep 4.213)
[01.30 08:39:45] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  71%|██████████████▏     | 600/846 [0:16:46<0:06:17, 0.65Hz] | (Ep 4.255)
[01.30 08:39:51] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  72%|██████████████▎     | 606/846 [0:16:52<0:06:07, 0.65Hz] | (Ep 4.298)
[01.30 08:39:51] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:39:51] ┇ INFO    ┇             DeepKNLP ┇ >>    606    4.298  0.0097    0.0765931      5.96059e-06
[01.30 08:39:55] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:39:55] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:39:55] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:40:02] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 140542.12Hz]
[01.30 08:40:09] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:06<0:00:13, 0.29Hz]
[01.30 08:40:16] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:13<0:00:13, 0.21Hz]
[01.30 08:40:23] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:20<0:00:10, 0.19Hz]
[01.30 08:40:31] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.21Hz]
[01.30 08:40:32] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:40:32] ┇ INFO    ┇             DeepKNLP ┇ >>    611    4.333            0.603567                    0.606272               0.605091                  0.704082                 0.660529          0.899756               0.744076        0.689053          36.885                     18.978                    0.163
[01.30 08:40:33] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  72%|██████████████▍     | 612/846 [0:17:34<0:08:12, 0.47Hz] | (Ep 4.340)
[01.30 08:40:39] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  73%|██████████████▌     | 618/846 [0:17:39<0:08:01, 0.47Hz] | (Ep 4.383)
[01.30 08:40:41] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:40:41] ┇ INFO    ┇             DeepKNLP ┇ >>    620    4.397  0.0103    0.0586782      5.61576e-06
[01.30 08:40:44] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  74%|██████████████▋     | 623/846 [0:17:45<0:07:54, 0.47Hz] | (Ep 4.418)
[01.30 08:40:49] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  74%|██████████████▊     | 628/846 [0:17:50<0:05:37, 0.65Hz] | (Ep 4.454)
[01.30 08:40:55] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  75%|██████████████▉     | 634/846 [0:17:56<0:05:27, 0.65Hz] | (Ep 4.496)
[01.30 08:40:55] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:40:55] ┇ INFO    ┇             DeepKNLP ┇ >>    634    4.496  0.0101    0.0601875      5.27094e-06
[01.30 08:41:00] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  76%|███████████████▏    | 640/846 [0:18:01<0:05:18, 0.65Hz] | (Ep 4.539)
[01.30 08:41:06] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  76%|███████████████▎    | 646/846 [0:18:07<0:05:08, 0.65Hz] | (Ep 4.582)
[01.30 08:41:09] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:41:09] ┇ INFO    ┇             DeepKNLP ┇ >>    649    4.603  0.0104    0.0930674      4.90148e-06
[01.30 08:41:11] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  77%|███████████████▍    | 652/846 [0:18:12<0:04:58, 0.65Hz] | (Ep 4.624)
[01.30 08:41:17] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  78%|███████████████▌    | 658/846 [0:18:18<0:04:47, 0.65Hz] | (Ep 4.667)
[01.30 08:41:17] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:41:17] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:41:17] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:41:24] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 101124.68Hz]
[01.30 08:41:31] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:07<0:00:14, 0.28Hz]
[01.30 08:41:38] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:14<0:00:14, 0.21Hz]
[01.30 08:41:45] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:21<0:00:10, 0.19Hz]
[01.30 08:41:53] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.20Hz]
[01.30 08:41:54] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:41:54] ┇ INFO    ┇             DeepKNLP ┇ >>    658    4.667            0.574176                    0.644936               0.672566                  0.697714                 0.664352          0.887805               0.781775        0.703332         37.0839                     18.876                    0.162
[01.30 08:41:55] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  78%|███████████████▌    | 659/846 [0:18:56<0:06:35, 0.47Hz] | (Ep 4.674)
[01.30 08:41:58] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:41:58] ┇ INFO    ┇             DeepKNLP ┇ >>    663    4.702  0.0095     0.073693      4.55665e-06
[01.30 08:42:00] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  79%|███████████████▋    | 665/846 [0:19:01<0:06:22, 0.47Hz] | (Ep 4.716)
[01.30 08:42:06] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  79%|███████████████▊    | 671/846 [0:19:07<0:06:11, 0.47Hz] | (Ep 4.759)
[01.30 08:42:11] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  80%|████████████████    | 677/846 [0:19:12<0:04:19, 0.65Hz] | (Ep 4.801)
[01.30 08:42:11] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:42:11] ┇ INFO    ┇             DeepKNLP ┇ >>    677    4.801  0.0076    0.0743404      4.21182e-06
[01.30 08:42:17] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  81%|████████████████▏   | 683/846 [0:19:18<0:04:10, 0.65Hz] | (Ep 4.844)
[01.30 08:42:23] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  81%|████████████████▎   | 689/846 [0:19:24<0:03:57, 0.66Hz] | (Ep 4.887)
[01.30 08:42:25] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:42:25] ┇ INFO    ┇             DeepKNLP ┇ >>    691    4.901  0.0103     0.176651        3.867e-06
[01.30 08:42:28] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  82%|████████████████▍   | 695/846 [0:19:29<0:03:48, 0.66Hz] | (Ep 4.929)
[01.30 08:42:34] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  83%|████████████████▌   | 701/846 [0:19:35<0:03:39, 0.66Hz] | (Ep 4.972)
[01.30 08:42:38] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:42:38] ┇ INFO    ┇             DeepKNLP ┇ >>    705        5  0.0091    0.0635551      3.52217e-06
[01.30 08:42:38] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:42:38] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:42:38] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:42:45] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 103743.17Hz]
[01.30 08:42:52] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:06<0:00:13, 0.29Hz]
[01.30 08:42:59] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:13<0:00:13, 0.21Hz]
[01.30 08:43:06] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:20<0:00:10, 0.19Hz]
[01.30 08:43:14] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.21Hz]
[01.30 08:43:15] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:43:15] ┇ INFO    ┇             DeepKNLP ┇ >>    705        5            0.620321                    0.631818                 0.6686                  0.720795                 0.693785          0.890511               0.817308        0.720448         37.0423                     18.897                    0.162
[01.30 08:43:16] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  83%|████████████████▋   | 706/846 [0:20:17<0:04:54, 0.48Hz] | (Ep 5.007)
[01.30 08:43:19] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:43:19] ┇ INFO    ┇             DeepKNLP ┇ >>    710    5.035  0.0062    0.0618717      3.42365e-06
[01.30 08:43:21] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  84%|████████████████▊   | 712/846 [0:20:22<0:04:42, 0.48Hz] | (Ep 5.050)
[01.30 08:43:27] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  85%|████████████████▉   | 718/846 [0:20:28<0:04:29, 0.48Hz] | (Ep 5.092)
[01.30 08:43:28] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:43:28] ┇ INFO    ┇             DeepKNLP ┇ >>    719    5.099  0.0045     0.054938      3.20197e-06
[01.30 08:43:32] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  86%|█████████████████   | 724/846 [0:20:33<0:03:05, 0.66Hz] | (Ep 5.135)
[01.30 08:43:38] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  86%|█████████████████▎  | 730/846 [0:20:39<0:02:56, 0.66Hz] | (Ep 5.177)
[01.30 08:43:41] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:43:41] ┇ INFO    ┇             DeepKNLP ┇ >>    733    5.199  0.0041   0.00706877      2.85714e-06
[01.30 08:43:44] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  87%|█████████████████▍  | 736/846 [0:20:45<0:02:47, 0.66Hz] | (Ep 5.220)
[01.30 08:43:49] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  88%|█████████████████▌  | 742/846 [0:20:50<0:02:38, 0.66Hz] | (Ep 5.262)
[01.30 08:43:54] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:43:54] ┇ INFO    ┇             DeepKNLP ┇ >>    747    5.298  0.0033    0.0140961      2.51232e-06
[01.30 08:43:55] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  88%|█████████████████▋  | 748/846 [0:20:56<0:02:29, 0.66Hz] | (Ep 5.305)
[01.30 08:43:59] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:43:59] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:43:59] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:44:06] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 128468.75Hz]
[01.30 08:44:13] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:06<0:00:13, 0.29Hz]
[01.30 08:44:20] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:13<0:00:13, 0.21Hz]
[01.30 08:44:27] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:20<0:00:10, 0.19Hz]
[01.30 08:44:35] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.21Hz]
[01.30 08:44:36] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:44:36] ┇ INFO    ┇             DeepKNLP ┇ >>    752    5.333            0.615797                    0.614512               0.672476                  0.713576                 0.680045          0.900726               0.806604        0.714819         36.8891                     18.976                    0.163
[01.30 08:44:37] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  89%|█████████████████▊  | 753/846 [0:21:38<0:03:16, 0.47Hz] | (Ep 5.340)
[01.30 08:44:42] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  90%|█████████████████▉  | 759/846 [0:21:43<0:03:03, 0.48Hz] | (Ep 5.383)
[01.30 08:44:44] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:44:44] ┇ INFO    ┇             DeepKNLP ┇ >>    761    5.397   0.004     0.128909      2.16749e-06
[01.30 08:44:48] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  90%|██████████████████  | 765/846 [0:21:49<0:02:50, 0.48Hz] | (Ep 5.426)
[01.30 08:44:53] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  91%|██████████████████▏ | 770/846 [0:21:54<0:01:56, 0.65Hz] | (Ep 5.461)
[01.30 08:44:58] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:44:58] ┇ INFO    ┇             DeepKNLP ┇ >>    775    5.496  0.0038    0.0322227      1.82266e-06
[01.30 08:44:59] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  92%|██████████████████▎ | 776/846 [0:22:00<0:01:47, 0.65Hz] | (Ep 5.504)
[01.30 08:45:04] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  92%|██████████████████▍ | 782/846 [0:22:05<0:01:37, 0.65Hz] | (Ep 5.546)
[01.30 08:45:09] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  93%|██████████████████▋ | 788/846 [0:22:10<0:01:28, 0.66Hz] | (Ep 5.589)
[01.30 08:45:11] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:45:11] ┇ INFO    ┇             DeepKNLP ┇ >>    790    5.603  0.0035    0.0259899       1.4532e-06
[01.30 08:45:15] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  94%|██████████████████▊ | 794/846 [0:22:16<0:01:19, 0.66Hz] | (Ep 5.631)
[01.30 08:45:20] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:45:20] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:45:20] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:45:27] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 63317.72Hz]
[01.30 08:45:33] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:06<0:00:13, 0.29Hz]
[01.30 08:45:41] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:13<0:00:13, 0.21Hz]
[01.30 08:45:47] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:20<0:00:10, 0.19Hz]
[01.30 08:45:56] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.21Hz]
[01.30 08:45:57] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:45:57] ┇ INFO    ┇             DeepKNLP ┇ >>    799    5.667            0.611702                    0.618347               0.670066                  0.699834                 0.677095          0.900726               0.804706        0.711782         36.8892                     18.976                    0.163
[01.30 08:45:57] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  95%|██████████████████▉ | 800/846 [0:22:58<0:01:37, 0.47Hz] | (Ep 5.674)
[01.30 08:46:01] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:46:01] ┇ INFO    ┇             DeepKNLP ┇ >>    804    5.702  0.0033    0.0118761      1.10837e-06
[01.30 08:46:03] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  95%|███████████████████ | 806/846 [0:23:04<0:01:24, 0.47Hz] | (Ep 5.716)
[01.30 08:46:08] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  96%|███████████████████▏| 812/846 [0:23:09<0:01:11, 0.48Hz] | (Ep 5.759)
[01.30 08:46:14] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  97%|███████████████████▎| 818/846 [0:23:15<0:00:42, 0.66Hz] | (Ep 5.801)
[01.30 08:46:14] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:46:14] ┇ INFO    ┇             DeepKNLP ┇ >>    818    5.801   0.003    0.0542853      7.63547e-07
[01.30 08:46:19] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  97%|███████████████████▍| 824/846 [0:23:20<0:00:33, 0.66Hz] | (Ep 5.844)
[01.30 08:46:25] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  98%|███████████████████▌| 830/846 [0:23:26<0:00:24, 0.66Hz] | (Ep 5.887)
[01.30 08:46:27] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:46:27] ┇ INFO    ┇             DeepKNLP ┇ >>    832    5.901  0.0035    0.0333267      4.18719e-07
[01.30 08:46:31] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  99%|███████████████████▊| 836/846 [0:23:32<0:00:15, 0.66Hz] | (Ep 5.929)
[01.30 08:46:36] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING] 100%|███████████████████▉| 842/846 [0:23:37<0:00:06, 0.66Hz] | (Ep 5.972)
[01.30 08:46:40] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:46:40] ┇ INFO    ┇             DeepKNLP ┇ >>    846        6  0.0036    0.0156468      7.38916e-08
[01.30 08:46:40] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:46:40] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:46:40] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:46:46] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 82279.07Hz]
[01.30 08:46:53] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:06<0:00:13, 0.29Hz]
[01.30 08:47:00] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:13<0:00:13, 0.21Hz]
[01.30 08:47:07] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:20<0:00:10, 0.19Hz]
[01.30 08:47:16] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:29<0:00:00, 0.21Hz]
[01.30 08:47:17] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:47:17] ┇ INFO    ┇             DeepKNLP ┇ >>    846        6            0.617608                    0.622472               0.676768                  0.709942                 0.681465          0.900726               0.795294        0.714897          36.884                     18.978                    0.163
[01.30 08:47:17] ┇ INFO    ┇ transformers.trainer ┇ 

Training completed. Do not forget to share your model on huggingface.co/models =)


[01.30 08:47:17] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    train_runtime    train_samples_per_second    train_steps_per_second    total_flos    train_loss
[01.30 08:47:17] ┇ INFO    ┇             DeepKNLP ┇ >>    846        6          1457.99                       74.63                      0.58   2.69224e+17     0.0329981
[01.30 08:47:17] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING] 100%|████████████████████| 846/846 [0:23:41<0:00:00, 0.67Hz] | (Ep 6.000)
[01.30 08:47:17] ┇ INFO    ┇             DeepKNLP ┇ Train result: TrainOutput(global_step=846, training_loss=0.03299811101974325, metrics={'train_runtime': 1457.9903, 'train_samples_per_second': 74.63, 'train_steps_per_second': 0.58, 'total_flos': 2.692241488305193e+17, 'train_loss': 0.03299811101974325, 'epoch': 6.0})
[01.30 08:47:17] ┇ INFO    ┇       chrisbase.data ┇ =========================================================================================================================================
[01.30 08:47:17] ┇ INFO    ┇       chrisbase.data ┇ [EXIT] python task2-nerG-trainer2.py --local_rank=0 --trainer_deepspeed configs/deepspeed/ds1_llama.json ($=00:24:52.368)
[01.30 08:47:17] ┇ INFO    ┇       chrisbase.data ┇ =========================================================================================================================================
