[01.30 08:30:21] ┇ INFO    ┇             DeepKNLP ┇ Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ =========================================================================================================================================
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ [INIT] python task2-nerG-trainer2.py --local_rank=0 --trainer_deepspeed configs/deepspeed/ds1_llama.json
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ =========================================================================================================================================
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ -----------------------------------------------------------------------------------------------------------------------------------------
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇   #	TrainingArgumentsForAccelerator              	value
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ -----------------------------------------------------------------------------------------------------------------------------------------
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇   1	env.hostname                                 	ptlm3
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇   2	env.hostaddr                                 	129.254.121.74
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇   3	env.global_rank                              	-1
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇   4	env.local_rank                               	0
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇   5	env.node_rank                                	-1
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇   6	env.world_size                               	4
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇   7	env.time_stamp                               	0130.083017
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇   8	env.python_path                              	/home/chrisjihee/miniforge3/envs/DeepKNLP/bin/python
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇   9	env.current_dir                              	/home/chrisjihee/proj/DeepKNLP
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  10	env.current_file                             	task2-nerG-trainer2.py
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  11	env.command_args                             	['--local_rank=0', '--trainer_deepspeed', 'configs/deepspeed/ds1_llama.json']
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  12	env.output_home                              	output
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  13	env.output_name                              	GNER
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  14	env.run_version                              	EAGLE-1B-debug
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  15	env.output_file                              	train-metrics-0130.083017.csv
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  16	env.logging_file                             	train-loggings-0130.083017.out
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  17	env.logging_level                            	30
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  18	env.logging_format                           	%(asctime)s ┇ %(levelname)-7s ┇ %(name)20s ┇ %(message)s
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  19	env.datetime_format                          	[%m.%d %H:%M:%S]
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  20	env.argument_file                            	train-arguments-0130.083017.json
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  21	env.random_seed                              	7
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  22	env.max_workers                              	4
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  23	env.debugging                                	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  24	env.output_dir                               	output/GNER/EAGLE-1B-debug
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  25	time.t1                                      	2025-01-30 08:30:22.371760
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  26	time.t2                                      	2025-01-30 08:30:17.463998
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  27	time.started                                 	[01.30 08:30:22]
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  28	time.settled
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  29	time.elapsed
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  30	data.pretrained                              	etri-lirs/egpt-1.3b-preview
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  31	data.train_file                              	data/gner/zero-shot-train.jsonl
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  32	data.study_file
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  33	data.eval_file                               	data/gner/zero-shot-dev-100.jsonl
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  34	data.pred_file                               	data/gner/zero-shot-test-100.jsonl
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  35	data.max_train_samples                       	-1
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  36	data.max_study_samples                       	-1
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  37	data.max_eval_samples                        	-1
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  38	data.max_pred_samples                        	-1
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  39	data.max_source_length                       	640
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  40	data.max_target_length                       	640
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  41	data.use_cache_data                          	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  42	data.ignore_pad_token_for_loss               	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  43	train.output_dir                             	output/GNER/EAGLE-1B-debug
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  44	train.overwrite_output_dir                   	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  45	train.do_train                               	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  46	train.do_eval                                	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  47	train.do_predict                             	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  48	train.eval_strategy                          	IntervalStrategy.NO
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  49	train.prediction_loss_only                   	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  50	train.per_device_train_batch_size            	8
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  51	train.per_device_eval_batch_size             	32
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  52	train.per_gpu_train_batch_size
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  53	train.per_gpu_eval_batch_size
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  54	train.gradient_accumulation_steps            	4
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  55	train.eval_accumulation_steps                	1
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  56	train.eval_delay                             	0
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  57	train.torch_empty_cache_steps
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  58	train.learning_rate                          	2e-05
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  59	train.weight_decay                           	0.0
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  60	train.adam_beta1                             	0.9
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  61	train.adam_beta2                             	0.999
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  62	train.adam_epsilon                           	1e-08
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  63	train.max_grad_norm                          	1.0
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  64	train.num_train_epochs                       	6.0
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  65	train.max_steps                              	-1
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  66	train.lr_scheduler_type                      	SchedulerType.COSINE
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  67	train.lr_scheduler_kwargs                    	{}
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  68	train.warmup_ratio                           	0.04
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  69	train.warmup_steps                           	0
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  70	train.log_level                              	warning
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  71	train.log_level_replica                      	error
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  72	train.log_on_each_node                       	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  73	train.logging_dir                            	output/GNER/EAGLE-1B-debug/runs/Jan30_08-30-21_ptlm3
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  74	train.logging_strategy                       	IntervalStrategy.NO
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  75	train.logging_first_step                     	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  76	train.logging_steps                          	0.1
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  77	train.logging_nan_inf_filter                 	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  78	train.save_strategy                          	SaveStrategy.NO
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  79	train.save_steps                             	9223372036854775807
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  80	train.save_total_limit
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  81	train.save_safetensors                       	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  82	train.save_on_each_node                      	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  83	train.save_only_model                        	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  84	train.restore_callback_states_from_checkpoint	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  85	train.no_cuda                                	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  86	train.use_cpu                                	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  87	train.use_mps_device                         	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  88	train.seed                                   	7
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  89	train.data_seed
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  90	train.jit_mode_eval                          	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  91	train.use_ipex                               	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  92	train.bf16                                   	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  93	train.fp16                                   	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  94	train.fp16_opt_level                         	O1
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  95	train.half_precision_backend                 	auto
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  96	train.bf16_full_eval                         	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  97	train.fp16_full_eval                         	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  98	train.tf32                                   	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇  99	train.local_rank                             	0
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 100	train.ddp_backend
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 101	train.tpu_num_cores
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 102	train.tpu_metrics_debug                      	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 103	train.debug                                  	[]
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 104	train.dataloader_drop_last                   	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 105	train.eval_steps                             	0.3333333333333333
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 106	train.dataloader_num_workers                 	0
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 107	train.dataloader_prefetch_factor
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 108	train.past_index                             	-1
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 109	train.run_name                               	output/GNER/EAGLE-1B-debug
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 110	train.disable_tqdm                           	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 111	train.remove_unused_columns                  	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 112	train.label_names
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 113	train.load_best_model_at_end                 	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 114	train.metric_for_best_model
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 115	train.greater_is_better
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 116	train.ignore_data_skip                       	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 117	train.fsdp                                   	[]
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 118	train.fsdp_min_num_params                    	0
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 119	train.fsdp_config                            	{'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 120	train.fsdp_transformer_layer_cls_to_wrap
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 121	train.accelerator_config                     	{'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 122	train.deepspeed                              	configs/deepspeed/ds1_llama.json
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 123	train.label_smoothing_factor                 	0.0
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 124	train.optim                                  	OptimizerNames.ADAMW_TORCH
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 125	train.optim_args
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 126	train.adafactor                              	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 127	train.group_by_length                        	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 128	train.length_column_name                     	length
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 129	train.report_to                              	[]
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 130	train.ddp_find_unused_parameters
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 131	train.ddp_bucket_cap_mb
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 132	train.ddp_broadcast_buffers
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 133	train.dataloader_pin_memory                  	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 134	train.dataloader_persistent_workers          	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 135	train.skip_memory_metrics                    	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 136	train.use_legacy_prediction_loop             	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 137	train.push_to_hub                            	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 138	train.resume_from_checkpoint
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 139	train.hub_model_id
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 140	train.hub_strategy                           	HubStrategy.EVERY_SAVE
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 141	train.hub_token
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 142	train.hub_private_repo
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 143	train.hub_always_push                        	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 144	train.gradient_checkpointing                 	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 145	train.gradient_checkpointing_kwargs
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 146	train.include_inputs_for_metrics             	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 147	train.include_for_metrics                    	[]
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 148	train.eval_do_concat_batches                 	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 149	train.fp16_backend                           	auto
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 150	train.evaluation_strategy
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 151	train.push_to_hub_model_id
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 152	train.push_to_hub_organization
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 153	train.push_to_hub_token
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 154	train._n_gpu                                 	1
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 155	train.mp_parameters
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 156	train.auto_find_batch_size                   	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 157	train.full_determinism                       	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 158	train.torchdynamo
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 159	train.ray_scope                              	last
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 160	train.ddp_timeout                            	1800
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 161	train.torch_compile                          	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 162	train.torch_compile_backend
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 163	train.torch_compile_mode
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 164	train.dispatch_batches
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 165	train.split_batches
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 166	train.include_tokens_per_second              	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 167	train.include_num_input_tokens_seen          	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 168	train.neftune_noise_alpha
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 169	train.optim_target_modules
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 170	train.batch_eval_metrics                     	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 171	train.eval_on_start                          	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 172	train.use_liger_kernel                       	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 173	train.eval_use_gather_object                 	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 174	train.average_tokens_across_devices          	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 175	train.sortish_sampler                        	False
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 176	train.predict_with_generate                  	True
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 177	train.generation_max_length                  	640
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 178	train.generation_num_beams
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ 179	train.generation_config
[01.30 08:30:22] ┇ INFO    ┇       chrisbase.data ┇ -----------------------------------------------------------------------------------------------------------------------------------------
[01.30 08:30:26] ┇ INFO    ┇             DeepKNLP ┇ type(model)=<class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>
[01.30 08:30:26] ┇ INFO    ┇             DeepKNLP ┇ model.generation_config.pad_token_id=0
[01.30 08:30:27] ┇ INFO    ┇             DeepKNLP ┇ Loaded raw train_dataset (#=18135): data/gner/zero-shot-train.jsonl
[01.30 08:30:27] ┇ INFO    ┇             DeepKNLP ┇ Completed preprocessing for train_dataset at [01.29 17:08:17]
[01.30 08:30:27] ┇ INFO    ┇             DeepKNLP ┇ Loaded raw eval_dataset (#=700): data/gner/zero-shot-dev-100.jsonl
[01.30 08:30:27] ┇ INFO    ┇             DeepKNLP ┇ Completed preprocessing for eval_dataset at [01.29 17:08:21]
[01.30 08:30:27] ┇ INFO    ┇ transformers.trainer ┇ Using auto half precision backend
[01.30 08:30:27] ┇ INFO    ┇             DeepKNLP ┇ Using deepspeed configuration:
{
  "fp16": {
    "enabled": false
  },
  "bf16": {
    "enabled": true
  },
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 2e-05,
      "betas": [
        0.9,
        0.999
      ],
      "eps": 1e-08,
      "weight_decay": 0.0
    }
  },
  "scheduler": {
    "type": "WarmupDecayLR",
    "params": {
      "warmup_min_lr": 0,
      "warmup_max_lr": 2e-05,
      "warmup_num_steps": "auto",
      "total_num_steps": "auto"
    }
  },
  "zero_optimization": {
    "stage": 1,
    "allgather_partitions": true,
    "allgather_bucket_size": 200000000.0,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 200000000.0,
    "contiguous_gradients": true
  },
  "gradient_accumulation_steps": 4,
  "gradient_clipping": 1.0,
  "train_batch_size": 128,
  "train_micro_batch_size_per_gpu": 8,
  "steps_per_print": Infinity
}
[01.30 08:30:37] ┇ WARNING ┇            DeepSpeed ┇ Attempting to get learning rate from scheduler before it has started
[01.30 08:30:37] ┇ INFO    ┇ transformers.trainer ┇ ***** Running training *****
[01.30 08:30:37] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 18,135
[01.30 08:30:37] ┇ INFO    ┇ transformers.trainer ┇   Num Epochs = 6
[01.30 08:30:37] ┇ INFO    ┇ transformers.trainer ┇   Instantaneous batch size per device = 8
[01.30 08:30:37] ┇ INFO    ┇ transformers.trainer ┇   Total train batch size (w. parallel, distributed & accumulation) = 128
[01.30 08:30:37] ┇ INFO    ┇ transformers.trainer ┇   Gradient Accumulation steps = 4
[01.30 08:30:37] ┇ INFO    ┇ transformers.trainer ┇   Total optimization steps = 846
[01.30 08:30:37] ┇ INFO    ┇ transformers.trainer ┇   Number of trainable parameters = 1,341,247,488
[01.30 08:30:41] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   0%|                    |   1/846 [0:00:03<0:53:06, 0.27Hz] | (Ep 0.007)
[01.30 08:30:48] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   0%|                    |   4/846 [0:00:10<0:38:01, 0.37Hz] | (Ep 0.028)
[01.30 08:30:55] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   1%|▏                   |   7/846 [0:00:17<0:34:40, 0.40Hz] | (Ep 0.050)
[01.30 08:31:01] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   1%|▏                   |  10/846 [0:00:23<0:33:12, 0.42Hz] | (Ep 0.071)
[01.30 08:31:06] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   2%|▎                   |  13/846 [0:00:29<0:31:15, 0.44Hz] | (Ep 0.092)
[01.30 08:31:09] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:31:09] ┇ INFO    ┇             DeepKNLP ┇ >>     14    0.099  0.8535      2.43354      1.49676e-05
[01.30 08:31:13] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   2%|▍                   |  16/846 [0:00:35<0:30:40, 0.45Hz] | (Ep 0.113)
[01.30 08:31:19] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   2%|▍                   |  19/846 [0:00:41<0:30:02, 0.46Hz] | (Ep 0.135)
[01.30 08:31:24] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   3%|▌                   |  22/846 [0:00:47<0:29:27, 0.47Hz] | (Ep 0.156)
[01.30 08:31:31] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   3%|▌                   |  25/846 [0:00:53<0:29:14, 0.47Hz] | (Ep 0.177)
[01.30 08:31:37] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   3%|▋                   |  28/846 [0:00:59<0:29:04, 0.47Hz] | (Ep 0.199)
[01.30 08:31:37] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:31:37] ┇ INFO    ┇             DeepKNLP ┇ >>     28    0.199  0.1143     0.386279      1.88988e-05
[01.30 08:31:44] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   4%|▋                   |  31/846 [0:01:06<0:29:07, 0.47Hz] | (Ep 0.220)
[01.30 08:31:50] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   4%|▊                   |  34/846 [0:01:12<0:28:47, 0.47Hz] | (Ep 0.241)
[01.30 08:31:56] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   4%|▊                   |  37/846 [0:01:18<0:28:33, 0.47Hz] | (Ep 0.262)
[01.30 08:32:02] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   5%|▉                   |  40/846 [0:01:24<0:28:19, 0.47Hz] | (Ep 0.284)
[01.30 08:32:06] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:32:06] ┇ INFO    ┇             DeepKNLP ┇ >>     42    0.298   0.059     0.125829      1.98276e-05
[01.30 08:32:08] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   5%|█                   |  43/846 [0:01:31<0:28:22, 0.47Hz] | (Ep 0.305)
[01.30 08:32:14] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   5%|█                   |  46/846 [0:01:36<0:28:04, 0.47Hz] | (Ep 0.326)
[01.30 08:32:17] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:32:17] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:32:17] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:32:25] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 101123.49Hz]
[01.30 08:32:34] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:17, 0.23Hz]
[01.30 08:32:43] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:17<0:00:17, 0.17Hz]
[01.30 08:32:51] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:26<0:00:13, 0.15Hz]
[01.30 08:33:01] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:36<0:00:00, 0.17Hz]
[01.30 08:33:03] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:33:03] ┇ INFO    ┇             DeepKNLP ┇ >>     47    0.333            0.294643                    0.370283               0.367939                  0.413322                 0.415051          0.820388               0.690998        0.481803         45.9342                     15.239                    0.131
[01.30 08:33:05] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   6%|█▏                  |  48/846 [0:02:27<0:40:54, 0.33Hz] | (Ep 0.340)
[01.30 08:33:11] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   6%|█▏                  |  51/846 [0:02:34<0:40:03, 0.33Hz] | (Ep 0.362)
[01.30 08:33:18] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   6%|█▎                  |  54/846 [0:02:40<0:39:13, 0.34Hz] | (Ep 0.383)
[01.30 08:33:22] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:33:22] ┇ INFO    ┇             DeepKNLP ┇ >>     56    0.397  0.0429     0.130389      1.94828e-05
[01.30 08:33:24] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   7%|█▎                  |  57/846 [0:02:46<0:38:25, 0.34Hz] | (Ep 0.404)
[01.30 08:33:31] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   7%|█▍                  |  60/846 [0:02:53<0:37:51, 0.35Hz] | (Ep 0.426)
[01.30 08:33:37] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   7%|█▍                  |  63/846 [0:02:59<0:37:08, 0.35Hz] | (Ep 0.447)
[01.30 08:33:42] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   8%|█▌                  |  66/846 [0:03:05<0:36:26, 0.36Hz] | (Ep 0.468)
[01.30 08:33:49] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   8%|█▋                  |  69/846 [0:03:11<0:36:09, 0.36Hz] | (Ep 0.489)
[01.30 08:33:51] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:33:51] ┇ INFO    ┇             DeepKNLP ┇ >>     70    0.496  0.0382      0.15838      1.91379e-05
[01.30 08:33:54] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   9%|█▋                  |  72/846 [0:03:17<0:35:56, 0.36Hz] | (Ep 0.511)
[01.30 08:34:01] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   9%|█▊                  |  75/846 [0:03:23<0:35:53, 0.36Hz] | (Ep 0.532)
[01.30 08:34:06] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]   9%|█▊                  |  78/846 [0:03:29<0:35:40, 0.36Hz] | (Ep 0.553)
[01.30 08:34:13] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  10%|█▉                  |  81/846 [0:03:35<0:35:42, 0.36Hz] | (Ep 0.574)
[01.30 08:34:19] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  10%|█▉                  |  84/846 [0:03:42<0:35:35, 0.36Hz] | (Ep 0.596)
[01.30 08:34:21] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:34:21] ┇ INFO    ┇             DeepKNLP ┇ >>     85    0.603  0.0352    0.0780821      1.87685e-05
[01.30 08:34:25] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  10%|██                  |  87/846 [0:03:48<0:35:30, 0.36Hz] | (Ep 0.617)
[01.30 08:34:30] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  11%|██                  |  89/846 [0:03:53<0:35:34, 0.35Hz] | (Ep 0.631)
[01.30 08:34:37] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  11%|██▏                 |  92/846 [0:04:00<0:35:31, 0.35Hz] | (Ep 0.652)
[01.30 08:34:42] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:34:42] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:34:42] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:34:50] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 91903.39Hz]
[01.30 08:34:58] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:16, 0.24Hz]
[01.30 08:35:07] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 08:35:16] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 08:35:25] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:35<0:00:00, 0.17Hz]
[01.30 08:35:27] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:35:27] ┇ INFO    ┇             DeepKNLP ┇ >>     94    0.667            0.437956                    0.523322                0.55761                  0.507832                 0.558296          0.880196               0.810552        0.610823         44.5338                     15.718                    0.135
[01.30 08:35:29] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  11%|██▏                 |  95/846 [0:04:51<0:44:21, 0.28Hz] | (Ep 0.674)
[01.30 08:35:35] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  12%|██▎                 |  98/846 [0:04:58<0:44:18, 0.28Hz] | (Ep 0.695)
[01.30 08:35:37] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:35:37] ┇ INFO    ┇             DeepKNLP ┇ >>     99    0.702  0.0336    0.0830496      1.84236e-05
[01.30 08:35:42] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  12%|██▍                 | 101/846 [0:05:04<0:44:12, 0.28Hz] | (Ep 0.716)
[01.30 08:35:49] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  12%|██▍                 | 104/846 [0:05:11<0:44:13, 0.28Hz] | (Ep 0.738)
[01.30 08:35:56] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  13%|██▌                 | 107/846 [0:05:18<0:44:00, 0.28Hz] | (Ep 0.759)
[01.30 08:36:02] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  13%|██▌                 | 110/846 [0:05:24<0:43:55, 0.28Hz] | (Ep 0.780)
[01.30 08:36:08] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  13%|██▋                 | 113/846 [0:05:31<0:34:43, 0.35Hz] | (Ep 0.801)
[01.30 08:36:09] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:36:09] ┇ INFO    ┇             DeepKNLP ┇ >>    113    0.801  0.0318     0.119978      1.80788e-05
[01.30 08:36:15] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  14%|██▋                 | 116/846 [0:05:37<0:34:40, 0.35Hz] | (Ep 0.823)
[01.30 08:36:22] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  14%|██▊                 | 119/846 [0:05:44<0:34:36, 0.35Hz] | (Ep 0.844)
[01.30 08:36:29] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  14%|██▉                 | 122/846 [0:05:51<0:34:34, 0.35Hz] | (Ep 0.865)
[01.30 08:36:35] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  15%|██▉                 | 125/846 [0:05:58<0:34:28, 0.35Hz] | (Ep 0.887)
[01.30 08:36:40] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:36:40] ┇ INFO    ┇             DeepKNLP ┇ >>    127    0.901  0.0329     0.106078       1.7734e-05
[01.30 08:36:42] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  15%|███                 | 128/846 [0:06:04<0:34:23, 0.35Hz] | (Ep 0.908)
[01.30 08:36:48] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  15%|███                 | 131/846 [0:06:11<0:34:21, 0.35Hz] | (Ep 0.929)
[01.30 08:36:54] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  16%|███▏                | 134/846 [0:06:17<0:34:15, 0.35Hz] | (Ep 0.950)
[01.30 08:37:01] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  16%|███▏                | 137/846 [0:06:24<0:34:16, 0.34Hz] | (Ep 0.972)
[01.30 08:37:08] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  17%|███▎                | 140/846 [0:06:30<0:34:17, 0.34Hz] | (Ep 0.993)
[01.30 08:37:10] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:37:10] ┇ INFO    ┇             DeepKNLP ┇ >>    141        1  0.0309    0.0826758      1.73892e-05
[01.30 08:37:10] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:37:10] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:37:10] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:37:18] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 84089.73Hz]
[01.30 08:37:27] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:16, 0.24Hz]
[01.30 08:37:35] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 08:37:44] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 08:37:54] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:35<0:00:00, 0.17Hz]
[01.30 08:37:55] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:37:55] ┇ INFO    ┇             DeepKNLP ┇ >>    141        1            0.478708                    0.604215                0.58309                  0.605684                 0.639629          0.866337               0.811594        0.655608          45.483                      15.39                    0.132
[01.30 08:37:56] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  17%|███▎                | 142/846 [0:07:19<0:42:25, 0.28Hz] | (Ep 1.007)
[01.30 08:37:58] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:37:58] ┇ INFO    ┇             DeepKNLP ┇ >>    142    1.007  0.0267    0.0826758      1.73892e-05
[01.30 08:38:02] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  17%|███▍                | 144/846 [0:07:24<0:42:30, 0.28Hz] | (Ep 1.021)
[01.30 08:38:08] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  17%|███▍                | 147/846 [0:07:30<0:42:13, 0.28Hz] | (Ep 1.043)
[01.30 08:38:14] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  18%|███▌                | 150/846 [0:07:37<0:42:08, 0.28Hz] | (Ep 1.064)
[01.30 08:38:21] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  18%|███▌                | 153/846 [0:07:44<0:41:53, 0.28Hz] | (Ep 1.085)
[01.30 08:38:26] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:38:26] ┇ INFO    ┇             DeepKNLP ┇ >>    155    1.099  0.0227    0.0278127       1.7069e-05
[01.30 08:38:28] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  18%|███▋                | 156/846 [0:07:50<0:41:38, 0.28Hz] | (Ep 1.106)
[01.30 08:38:34] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  19%|███▊                | 159/846 [0:07:56<0:33:17, 0.34Hz] | (Ep 1.128)
[01.30 08:38:41] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  19%|███▊                | 162/846 [0:08:03<0:33:09, 0.34Hz] | (Ep 1.149)
[01.30 08:38:47] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  20%|███▉                | 165/846 [0:08:09<0:32:50, 0.35Hz] | (Ep 1.170)
[01.30 08:38:53] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  20%|███▉                | 168/846 [0:08:16<0:32:46, 0.34Hz] | (Ep 1.191)
[01.30 08:38:56] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:38:56] ┇ INFO    ┇             DeepKNLP ┇ >>    169    1.199  0.0235    0.0306983      1.67241e-05
[01.30 08:39:00] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  20%|████                | 171/846 [0:08:22<0:32:31, 0.35Hz] | (Ep 1.213)
[01.30 08:39:06] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  21%|████                | 174/846 [0:08:28<0:32:17, 0.35Hz] | (Ep 1.234)
[01.30 08:39:12] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  21%|████▏               | 177/846 [0:08:34<0:32:03, 0.35Hz] | (Ep 1.255)
[01.30 08:39:20] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  21%|████▎               | 180/846 [0:08:42<0:32:11, 0.34Hz] | (Ep 1.277)
[01.30 08:39:26] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  22%|████▎               | 183/846 [0:08:49<0:32:00, 0.35Hz] | (Ep 1.298)
[01.30 08:39:27] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:39:27] ┇ INFO    ┇             DeepKNLP ┇ >>    183    1.298  0.0227     0.138841      1.63793e-05
[01.30 08:39:33] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  22%|████▍               | 186/846 [0:08:55<0:31:40, 0.35Hz] | (Ep 1.319)
[01.30 08:39:37] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:39:37] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:39:37] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:39:47] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 90326.23Hz]
[01.30 08:39:56] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:17, 0.22Hz]
[01.30 08:40:05] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:17<0:00:17, 0.17Hz]
[01.30 08:40:13] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:26<0:00:13, 0.15Hz]
[01.30 08:40:24] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:37<0:00:00, 0.16Hz]
[01.30 08:40:25] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:40:25] ┇ INFO    ┇             DeepKNLP ┇ >>    188    1.333            0.501466                      0.4947               0.617373                  0.701031                 0.562061          0.872549               0.754808        0.643427         47.6075                     14.704                    0.126
[01.30 08:40:27] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  22%|████▍               | 189/846 [0:09:50<0:39:57, 0.27Hz] | (Ep 1.340)
[01.30 08:40:33] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  23%|████▌               | 192/846 [0:09:56<0:39:42, 0.27Hz] | (Ep 1.362)
[01.30 08:40:40] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  23%|████▌               | 195/846 [0:10:02<0:39:25, 0.28Hz] | (Ep 1.383)
[01.30 08:40:44] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:40:44] ┇ INFO    ┇             DeepKNLP ┇ >>    197    1.397  0.0252    0.0295071      1.60345e-05
[01.30 08:40:46] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  23%|████▋               | 198/846 [0:10:08<0:39:15, 0.28Hz] | (Ep 1.404)
[01.30 08:40:52] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  24%|████▊               | 201/846 [0:10:15<0:39:03, 0.28Hz] | (Ep 1.426)
[01.30 08:40:58] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  24%|████▊               | 204/846 [0:10:20<0:38:46, 0.28Hz] | (Ep 1.447)
[01.30 08:41:05] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  24%|████▉               | 207/846 [0:10:27<0:30:56, 0.34Hz] | (Ep 1.468)
[01.30 08:41:11] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  25%|████▉               | 210/846 [0:10:34<0:30:55, 0.34Hz] | (Ep 1.489)
[01.30 08:41:13] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:41:13] ┇ INFO    ┇             DeepKNLP ┇ >>    211    1.496   0.023    0.0432147      1.56897e-05
[01.30 08:41:18] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  25%|█████               | 213/846 [0:10:40<0:30:43, 0.34Hz] | (Ep 1.511)
[01.30 08:41:24] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  26%|█████               | 216/846 [0:10:46<0:30:27, 0.34Hz] | (Ep 1.532)
[01.30 08:41:31] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  26%|█████▏              | 219/846 [0:10:53<0:30:19, 0.34Hz] | (Ep 1.553)
[01.30 08:41:37] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  26%|█████▏              | 222/846 [0:10:59<0:30:10, 0.34Hz] | (Ep 1.574)
[01.30 08:41:43] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  27%|█████▎              | 225/846 [0:11:05<0:29:56, 0.35Hz] | (Ep 1.596)
[01.30 08:41:46] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:41:46] ┇ INFO    ┇             DeepKNLP ┇ >>    226    1.603  0.0223    0.0267211      1.53202e-05
[01.30 08:41:48] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  27%|█████▎              | 227/846 [0:11:10<0:30:02, 0.34Hz] | (Ep 1.610)
[01.30 08:41:54] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  27%|█████▍              | 230/846 [0:11:17<0:29:46, 0.34Hz] | (Ep 1.631)
[01.30 08:42:01] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  28%|█████▌              | 233/846 [0:11:23<0:29:44, 0.34Hz] | (Ep 1.652)
[01.30 08:42:05] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:42:05] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:42:05] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:42:13] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 92669.80Hz]
[01.30 08:42:22] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:16, 0.24Hz]
[01.30 08:42:30] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 08:42:39] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 08:42:49] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:35<0:00:00, 0.17Hz]
[01.30 08:42:50] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:42:50] ┇ INFO    ┇             DeepKNLP ┇ >>    235    1.667            0.499234                    0.597122                0.65243                  0.675958                 0.632197          0.880597               0.793017        0.675794         44.5486                     15.713                    0.135
[01.30 08:42:52] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  28%|█████▌              | 236/846 [0:12:14<0:36:45, 0.28Hz] | (Ep 1.674)
[01.30 08:42:58] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  28%|█████▋              | 239/846 [0:12:20<0:36:38, 0.28Hz] | (Ep 1.695)
[01.30 08:43:01] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:43:01] ┇ INFO    ┇             DeepKNLP ┇ >>    240    1.702  0.0213    0.0533446      1.49754e-05
[01.30 08:43:05] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  29%|█████▋              | 242/846 [0:12:27<0:36:27, 0.28Hz] | (Ep 1.716)
[01.30 08:43:11] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  29%|█████▊              | 245/846 [0:12:34<0:36:05, 0.28Hz] | (Ep 1.738)
[01.30 08:43:18] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  29%|█████▊              | 248/846 [0:12:41<0:36:00, 0.28Hz] | (Ep 1.759)
[01.30 08:43:26] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  30%|█████▉              | 251/846 [0:12:48<0:35:55, 0.28Hz] | (Ep 1.780)
[01.30 08:43:31] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  30%|██████              | 254/846 [0:12:53<0:28:10, 0.35Hz] | (Ep 1.801)
[01.30 08:43:32] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:43:32] ┇ INFO    ┇             DeepKNLP ┇ >>    254    1.801  0.0214    0.0368338      1.46305e-05
[01.30 08:43:38] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  30%|██████              | 257/846 [0:13:00<0:28:09, 0.35Hz] | (Ep 1.823)
[01.30 08:43:44] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  31%|██████▏             | 260/846 [0:13:07<0:28:01, 0.35Hz] | (Ep 1.844)
[01.30 08:43:50] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  31%|██████▏             | 263/846 [0:13:13<0:27:51, 0.35Hz] | (Ep 1.865)
[01.30 08:43:57] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  31%|██████▎             | 266/846 [0:13:19<0:27:42, 0.35Hz] | (Ep 1.887)
[01.30 08:44:02] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:44:02] ┇ INFO    ┇             DeepKNLP ┇ >>    268    1.901  0.0218    0.0269629      1.42857e-05
[01.30 08:44:04] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  32%|██████▎             | 269/846 [0:13:26<0:27:38, 0.35Hz] | (Ep 1.908)
[01.30 08:44:10] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  32%|██████▍             | 272/846 [0:13:32<0:27:25, 0.35Hz] | (Ep 1.929)
[01.30 08:44:17] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  33%|██████▌             | 275/846 [0:13:39<0:27:27, 0.35Hz] | (Ep 1.950)
[01.30 08:44:23] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  33%|██████▌             | 278/846 [0:13:45<0:27:12, 0.35Hz] | (Ep 1.972)
[01.30 08:44:30] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  33%|██████▋             | 281/846 [0:13:52<0:27:07, 0.35Hz] | (Ep 1.993)
[01.30 08:44:32] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:44:32] ┇ INFO    ┇             DeepKNLP ┇ >>    282        2  0.0229    0.0599978      1.39409e-05
[01.30 08:44:32] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:44:32] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:44:32] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:44:40] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 99314.79Hz]
[01.30 08:44:48] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:16, 0.24Hz]
[01.30 08:44:57] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 08:45:05] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 08:45:14] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:34<0:00:00, 0.18Hz]
[01.30 08:45:15] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:45:15] ┇ INFO    ┇             DeepKNLP ┇ >>    282        2            0.398703                    0.558426                 0.6162                  0.700794                 0.631829          0.862069                0.79902        0.652434         43.3214                     16.158                    0.138
[01.30 08:45:17] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  33%|██████▋             | 283/846 [0:14:39<0:33:26, 0.28Hz] | (Ep 2.007)
[01.30 08:45:19] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:45:19] ┇ INFO    ┇             DeepKNLP ┇ >>    284    2.014  0.0179    0.0517225      1.38916e-05
[01.30 08:45:23] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  34%|██████▊             | 286/846 [0:14:45<0:33:11, 0.28Hz] | (Ep 2.028)
[01.30 08:45:30] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  34%|██████▊             | 289/846 [0:14:52<0:32:57, 0.28Hz] | (Ep 2.050)
[01.30 08:45:36] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  35%|██████▉             | 292/846 [0:14:58<0:32:43, 0.28Hz] | (Ep 2.071)
[01.30 08:45:42] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  35%|██████▉             | 295/846 [0:15:04<0:32:27, 0.28Hz] | (Ep 2.092)
[01.30 08:45:44] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:45:44] ┇ INFO    ┇             DeepKNLP ┇ >>    296    2.099  0.0197    0.0387717      1.35961e-05
[01.30 08:45:48] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  35%|███████             | 298/846 [0:15:10<0:32:18, 0.28Hz] | (Ep 2.113)
[01.30 08:45:55] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  36%|███████             | 301/846 [0:15:17<0:25:45, 0.35Hz] | (Ep 2.135)
[01.30 08:46:01] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  36%|███████▏            | 304/846 [0:15:23<0:25:34, 0.35Hz] | (Ep 2.156)
[01.30 08:46:07] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  36%|███████▎            | 307/846 [0:15:30<0:25:21, 0.35Hz] | (Ep 2.177)
[01.30 08:46:14] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  37%|███████▎            | 310/846 [0:15:36<0:25:13, 0.35Hz] | (Ep 2.199)
[01.30 08:46:14] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:46:14] ┇ INFO    ┇             DeepKNLP ┇ >>    310    2.199  0.0197    0.0491381      1.32512e-05
[01.30 08:46:20] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  37%|███████▍            | 313/846 [0:15:42<0:24:54, 0.36Hz] | (Ep 2.220)
[01.30 08:46:27] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  37%|███████▍            | 316/846 [0:15:49<0:24:53, 0.35Hz] | (Ep 2.241)
[01.30 08:46:33] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  38%|███████▌            | 319/846 [0:15:55<0:24:41, 0.36Hz] | (Ep 2.262)
[01.30 08:46:40] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  38%|███████▌            | 322/846 [0:16:03<0:24:41, 0.35Hz] | (Ep 2.284)
[01.30 08:46:45] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:46:45] ┇ INFO    ┇             DeepKNLP ┇ >>    324    2.298  0.0202    0.0604014      1.29064e-05
[01.30 08:46:47] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  38%|███████▋            | 325/846 [0:16:09<0:24:38, 0.35Hz] | (Ep 2.305)
[01.30 08:46:53] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  39%|███████▊            | 328/846 [0:16:15<0:24:27, 0.35Hz] | (Ep 2.326)
[01.30 08:46:55] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:46:55] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:46:55] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:47:03] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 97475.54Hz]
[01.30 08:47:12] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:16, 0.24Hz]
[01.30 08:47:20] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 08:47:29] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 08:47:39] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:35<0:00:00, 0.17Hz]
[01.30 08:47:40] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:47:40] ┇ INFO    ┇             DeepKNLP ┇ >>    329    2.333            0.556522                    0.587678               0.673897                  0.687395                 0.652968          0.897561               0.783654        0.691382         44.4863                     15.735                    0.135
[01.30 08:47:42] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  39%|███████▊            | 330/846 [0:17:04<0:30:22, 0.28Hz] | (Ep 2.340)
[01.30 08:47:49] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  39%|███████▊            | 333/846 [0:17:11<0:30:19, 0.28Hz] | (Ep 2.362)
[01.30 08:47:55] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  40%|███████▉            | 336/846 [0:17:17<0:30:05, 0.28Hz] | (Ep 2.383)
[01.30 08:47:59] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:47:59] ┇ INFO    ┇             DeepKNLP ┇ >>    338    2.397  0.0186    0.0524055      1.25616e-05
[01.30 08:48:01] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  40%|████████            | 339/846 [0:17:23<0:29:49, 0.28Hz] | (Ep 2.404)
[01.30 08:48:07] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  40%|████████            | 342/846 [0:17:30<0:29:38, 0.28Hz] | (Ep 2.426)
[01.30 08:48:14] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  41%|████████▏           | 345/846 [0:17:36<0:29:26, 0.28Hz] | (Ep 2.447)
[01.30 08:48:21] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  41%|████████▏           | 348/846 [0:17:43<0:23:41, 0.35Hz] | (Ep 2.468)
[01.30 08:48:27] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  41%|████████▎           | 351/846 [0:17:49<0:23:28, 0.35Hz] | (Ep 2.489)
[01.30 08:48:30] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:48:30] ┇ INFO    ┇             DeepKNLP ┇ >>    352    2.496  0.0185    0.0397461      1.22167e-05
[01.30 08:48:34] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  42%|████████▎           | 354/846 [0:17:56<0:23:24, 0.35Hz] | (Ep 2.511)
[01.30 08:48:40] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  42%|████████▍           | 357/846 [0:18:02<0:23:16, 0.35Hz] | (Ep 2.532)
[01.30 08:48:46] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  43%|████████▌           | 360/846 [0:18:08<0:23:10, 0.35Hz] | (Ep 2.553)
[01.30 08:48:51] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  43%|████████▌           | 362/846 [0:18:13<0:23:09, 0.35Hz] | (Ep 2.567)
[01.30 08:48:58] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  43%|████████▋           | 365/846 [0:18:20<0:23:04, 0.35Hz] | (Ep 2.589)
[01.30 08:49:03] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:49:03] ┇ INFO    ┇             DeepKNLP ┇ >>    367    2.603  0.0195    0.0399596      1.18473e-05
[01.30 08:49:05] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  43%|████████▋           | 368/846 [0:18:27<0:22:57, 0.35Hz] | (Ep 2.610)
[01.30 08:49:11] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  44%|████████▊           | 371/846 [0:18:33<0:22:43, 0.35Hz] | (Ep 2.631)
[01.30 08:49:17] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  44%|████████▊           | 374/846 [0:18:39<0:22:34, 0.35Hz] | (Ep 2.652)
[01.30 08:49:21] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:49:21] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:49:21] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:49:30] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 99910.84Hz]
[01.30 08:49:38] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:16, 0.24Hz]
[01.30 08:49:47] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 08:49:55] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 08:50:04] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:33<0:00:00, 0.18Hz]
[01.30 08:50:05] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:50:05] ┇ INFO    ┇             DeepKNLP ┇ >>    376    2.667            0.540682                    0.588367               0.680224                   0.68932                 0.676856          0.878049                0.79717        0.692953         43.5154                     16.086                    0.138
[01.30 08:50:07] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  45%|████████▉           | 377/846 [0:19:29<0:27:53, 0.28Hz] | (Ep 2.674)
[01.30 08:50:13] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  45%|████████▉           | 380/846 [0:19:35<0:27:38, 0.28Hz] | (Ep 2.695)
[01.30 08:50:16] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:50:16] ┇ INFO    ┇             DeepKNLP ┇ >>    381    2.702  0.0192    0.0449418      1.15025e-05
[01.30 08:50:20] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  45%|█████████           | 383/846 [0:19:42<0:27:28, 0.28Hz] | (Ep 2.716)
[01.30 08:50:26] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  46%|█████████▏          | 386/846 [0:19:48<0:27:11, 0.28Hz] | (Ep 2.738)
[01.30 08:50:32] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  46%|█████████▏          | 389/846 [0:19:55<0:26:59, 0.28Hz] | (Ep 2.759)
[01.30 08:50:38] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  46%|█████████▎          | 392/846 [0:20:00<0:26:46, 0.28Hz] | (Ep 2.780)
[01.30 08:50:44] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  47%|█████████▎          | 395/846 [0:20:06<0:21:11, 0.35Hz] | (Ep 2.801)
[01.30 08:50:44] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:50:44] ┇ INFO    ┇             DeepKNLP ┇ >>    395    2.801  0.0212    0.0494773      1.11576e-05
[01.30 08:50:52] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  47%|█████████▍          | 398/846 [0:20:14<0:21:14, 0.35Hz] | (Ep 2.823)
[01.30 08:50:59] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  47%|█████████▍          | 401/846 [0:20:21<0:21:05, 0.35Hz] | (Ep 2.844)
[01.30 08:51:04] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  48%|█████████▌          | 403/846 [0:20:26<0:21:09, 0.35Hz] | (Ep 2.858)
[01.30 08:51:11] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  48%|█████████▌          | 406/846 [0:20:33<0:21:04, 0.35Hz] | (Ep 2.879)
[01.30 08:51:17] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  48%|█████████▋          | 409/846 [0:20:39<0:20:49, 0.35Hz] | (Ep 2.901)
[01.30 08:51:17] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:51:17] ┇ INFO    ┇             DeepKNLP ┇ >>    409    2.901  0.0214    0.0465114      1.08128e-05
[01.30 08:51:23] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  49%|█████████▋          | 412/846 [0:20:46<0:20:43, 0.35Hz] | (Ep 2.922)
[01.30 08:51:30] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  49%|█████████▊          | 415/846 [0:20:52<0:20:35, 0.35Hz] | (Ep 2.943)
[01.30 08:51:36] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  49%|█████████▉          | 418/846 [0:20:59<0:20:27, 0.35Hz] | (Ep 2.965)
[01.30 08:51:42] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  50%|█████████▉          | 421/846 [0:21:05<0:20:17, 0.35Hz] | (Ep 2.986)
[01.30 08:51:47] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:51:47] ┇ INFO    ┇             DeepKNLP ┇ >>    423        3  0.0195    0.0408246       1.0468e-05
[01.30 08:51:47] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:51:47] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:51:47] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:51:56] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 98338.14Hz]
[01.30 08:52:04] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:17, 0.23Hz]
[01.30 08:52:13] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:17<0:00:17, 0.18Hz]
[01.30 08:52:21] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 08:52:31] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:35<0:00:00, 0.17Hz]
[01.30 08:52:32] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:52:32] ┇ INFO    ┇             DeepKNLP ┇ >>    423        3            0.587755                    0.611872                0.68237                    0.6875                 0.680995          0.891041               0.799054        0.705798         44.7498                     15.643                    0.134
[01.30 08:52:34] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  50%|██████████          | 424/846 [0:21:56<0:25:08, 0.28Hz] | (Ep 3.007)
[01.30 08:52:38] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:52:38] ┇ INFO    ┇             DeepKNLP ┇ >>    426    3.021  0.0171    0.0429688      1.03941e-05
[01.30 08:52:40] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  50%|██████████          | 427/846 [0:22:03<0:24:53, 0.28Hz] | (Ep 3.028)
[01.30 08:52:46] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  51%|██████████▏         | 430/846 [0:22:09<0:24:39, 0.28Hz] | (Ep 3.050)
[01.30 08:52:53] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  51%|██████████▏         | 433/846 [0:22:15<0:24:27, 0.28Hz] | (Ep 3.071)
[01.30 08:52:59] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  52%|██████████▎         | 436/846 [0:22:21<0:24:17, 0.28Hz] | (Ep 3.092)
[01.30 08:53:02] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:53:02] ┇ INFO    ┇             DeepKNLP ┇ >>    437    3.099  0.0155    0.0526492      1.01232e-05
[01.30 08:53:06] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  52%|██████████▍         | 439/846 [0:22:29<0:24:12, 0.28Hz] | (Ep 3.113)
[01.30 08:53:12] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  52%|██████████▍         | 442/846 [0:22:35<0:19:22, 0.35Hz] | (Ep 3.135)
[01.30 08:53:19] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  53%|██████████▌         | 445/846 [0:22:41<0:19:12, 0.35Hz] | (Ep 3.156)
[01.30 08:53:25] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  53%|██████████▌         | 448/846 [0:22:47<0:19:03, 0.35Hz] | (Ep 3.177)
[01.30 08:53:31] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  53%|██████████▋         | 451/846 [0:22:53<0:18:50, 0.35Hz] | (Ep 3.199)
[01.30 08:53:31] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:53:31] ┇ INFO    ┇             DeepKNLP ┇ >>    451    3.199  0.0144    0.0752338      9.77833e-06
[01.30 08:53:38] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  54%|██████████▋         | 454/846 [0:23:00<0:18:49, 0.35Hz] | (Ep 3.220)
[01.30 08:53:44] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  54%|██████████▊         | 457/846 [0:23:06<0:18:41, 0.35Hz] | (Ep 3.241)
[01.30 08:53:50] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  54%|██████████▊         | 460/846 [0:23:12<0:18:30, 0.35Hz] | (Ep 3.262)
[01.30 08:53:57] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  55%|██████████▉         | 463/846 [0:23:19<0:18:14, 0.35Hz] | (Ep 3.284)
[01.30 08:54:00] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:54:00] ┇ INFO    ┇             DeepKNLP ┇ >>    465    3.298  0.0155    0.0517153       9.4335e-06
[01.30 08:54:02] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  55%|███████████         | 466/846 [0:23:25<0:17:56, 0.35Hz] | (Ep 3.305)
[01.30 08:54:09] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  55%|███████████         | 469/846 [0:23:32<0:17:48, 0.35Hz] | (Ep 3.326)
[01.30 08:54:11] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:54:11] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:54:11] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:54:20] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 91482.52Hz]
[01.30 08:54:28] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:17, 0.23Hz]
[01.30 08:54:37] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 08:54:45] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 08:54:55] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:35<0:00:00, 0.17Hz]
[01.30 08:54:56] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:54:56] ┇ INFO    ┇             DeepKNLP ┇ >>    470    3.333            0.583446                    0.602125               0.652399                  0.711554                 0.658933          0.909976               0.782816        0.700178         44.7632                     15.638                    0.134
[01.30 08:54:58] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  56%|███████████▏        | 471/846 [0:24:21<0:22:09, 0.28Hz] | (Ep 3.340)
[01.30 08:55:04] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  56%|███████████▏        | 474/846 [0:24:27<0:21:57, 0.28Hz] | (Ep 3.362)
[01.30 08:55:11] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  56%|███████████▎        | 477/846 [0:24:34<0:21:48, 0.28Hz] | (Ep 3.383)
[01.30 08:55:16] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:55:16] ┇ INFO    ┇             DeepKNLP ┇ >>    479    3.397  0.0154    0.0581101      9.08867e-06
[01.30 08:55:18] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  57%|███████████▎        | 480/846 [0:24:40<0:21:41, 0.28Hz] | (Ep 3.404)
[01.30 08:55:25] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  57%|███████████▍        | 483/846 [0:24:47<0:21:32, 0.28Hz] | (Ep 3.426)
[01.30 08:55:31] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  57%|███████████▍        | 486/846 [0:24:53<0:21:20, 0.28Hz] | (Ep 3.447)
[01.30 08:55:38] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  58%|███████████▌        | 489/846 [0:25:00<0:17:00, 0.35Hz] | (Ep 3.468)
[01.30 08:55:44] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  58%|███████████▋        | 492/846 [0:25:06<0:16:47, 0.35Hz] | (Ep 3.489)
[01.30 08:55:46] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:55:46] ┇ INFO    ┇             DeepKNLP ┇ >>    493    3.496  0.0148    0.0608851      8.74384e-06
[01.30 08:55:50] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  59%|███████████▋        | 495/846 [0:25:12<0:16:40, 0.35Hz] | (Ep 3.511)
[01.30 08:55:57] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  59%|███████████▊        | 498/846 [0:25:19<0:16:32, 0.35Hz] | (Ep 3.532)
[01.30 08:56:04] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  59%|███████████▊        | 501/846 [0:25:26<0:16:23, 0.35Hz] | (Ep 3.553)
[01.30 08:56:10] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  60%|███████████▉        | 504/846 [0:25:32<0:16:13, 0.35Hz] | (Ep 3.574)
[01.30 08:56:16] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  60%|███████████▉        | 507/846 [0:25:38<0:16:05, 0.35Hz] | (Ep 3.596)
[01.30 08:56:18] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:56:18] ┇ INFO    ┇             DeepKNLP ┇ >>    508    3.603  0.0162    0.0337352      8.37438e-06
[01.30 08:56:22] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  60%|████████████        | 510/846 [0:25:44<0:15:55, 0.35Hz] | (Ep 3.617)
[01.30 08:56:28] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  61%|████████████▏       | 513/846 [0:25:51<0:15:48, 0.35Hz] | (Ep 3.638)
[01.30 08:56:35] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  61%|████████████▏       | 516/846 [0:25:58<0:15:42, 0.35Hz] | (Ep 3.660)
[01.30 08:56:38] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:56:38] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:56:38] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:56:46] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 99910.84Hz]
[01.30 08:56:55] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:17, 0.23Hz]
[01.30 08:57:03] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:17<0:00:17, 0.18Hz]
[01.30 08:57:12] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 08:57:22] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:35<0:00:00, 0.17Hz]
[01.30 08:57:23] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:57:23] ┇ INFO    ┇             DeepKNLP ┇ >>    517    3.667            0.623514                    0.606768               0.678913                  0.733718                 0.689655          0.898058               0.764151         0.71354         45.0447                      15.54                    0.133
[01.30 08:57:25] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  61%|████████████▏       | 518/846 [0:26:47<0:19:32, 0.28Hz] | (Ep 3.674)
[01.30 08:57:32] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  62%|████████████▎       | 521/846 [0:26:54<0:19:25, 0.28Hz] | (Ep 3.695)
[01.30 08:57:34] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:57:34] ┇ INFO    ┇             DeepKNLP ┇ >>    522    3.702  0.0149    0.0599436      8.02956e-06
[01.30 08:57:38] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  62%|████████████▍       | 524/846 [0:27:00<0:19:12, 0.28Hz] | (Ep 3.716)
[01.30 08:57:44] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  62%|████████████▍       | 527/846 [0:27:07<0:19:04, 0.28Hz] | (Ep 3.738)
[01.30 08:57:51] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  63%|████████████▌       | 530/846 [0:27:13<0:18:54, 0.28Hz] | (Ep 3.759)
[01.30 08:57:57] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  63%|████████████▌       | 533/846 [0:27:19<0:18:41, 0.28Hz] | (Ep 3.780)
[01.30 08:58:04] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  63%|████████████▋       | 536/846 [0:27:26<0:14:50, 0.35Hz] | (Ep 3.801)
[01.30 08:58:04] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:58:04] ┇ INFO    ┇             DeepKNLP ┇ >>    536    3.801  0.0146    0.0535669      7.68473e-06
[01.30 08:58:09] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  64%|████████████▋       | 538/846 [0:27:31<0:14:52, 0.35Hz] | (Ep 3.816)
[01.30 08:58:15] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  64%|████████████▊       | 541/846 [0:27:37<0:14:40, 0.35Hz] | (Ep 3.837)
[01.30 08:58:22] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  64%|████████████▊       | 544/846 [0:27:44<0:14:31, 0.35Hz] | (Ep 3.858)
[01.30 08:58:29] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  65%|████████████▉       | 547/846 [0:27:51<0:14:24, 0.35Hz] | (Ep 3.879)
[01.30 08:58:35] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  65%|█████████████       | 550/846 [0:27:57<0:14:15, 0.35Hz] | (Ep 3.901)
[01.30 08:58:35] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:58:35] ┇ INFO    ┇             DeepKNLP ┇ >>    550    3.901  0.0158    0.0655218       7.3399e-06
[01.30 08:58:42] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  65%|█████████████       | 553/846 [0:28:04<0:14:05, 0.35Hz] | (Ep 3.922)
[01.30 08:58:48] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  66%|█████████████▏      | 556/846 [0:28:10<0:13:56, 0.35Hz] | (Ep 3.943)
[01.30 08:58:54] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  66%|█████████████▏      | 559/846 [0:28:16<0:13:50, 0.35Hz] | (Ep 3.965)
[01.30 08:59:01] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  66%|█████████████▎      | 562/846 [0:28:24<0:13:41, 0.35Hz] | (Ep 3.986)
[01.30 08:59:05] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 08:59:05] ┇ INFO    ┇             DeepKNLP ┇ >>    564        4  0.0161    0.0550543      6.99507e-06
[01.30 08:59:05] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 08:59:05] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 08:59:05] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 08:59:14] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 99216.14Hz]
[01.30 08:59:23] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:17, 0.23Hz]
[01.30 08:59:31] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 08:59:40] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 08:59:50] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:36<0:00:00, 0.17Hz]
[01.30 08:59:51] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 08:59:51] ┇ INFO    ┇             DeepKNLP ┇ >>    564        4            0.578512                    0.595294               0.679328                  0.689537                  0.67191          0.875306               0.808612        0.699786         45.7726                     15.293                    0.131
[01.30 08:59:54] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  67%|█████████████▎      | 565/846 [0:29:16<0:16:55, 0.28Hz] | (Ep 4.007)
[01.30 09:00:00] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  67%|█████████████▍      | 568/846 [0:29:22<0:16:45, 0.28Hz] | (Ep 4.028)
[01.30 09:00:00] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:00:00] ┇ INFO    ┇             DeepKNLP ┇ >>    568    4.028  0.0134    0.0456581      6.89655e-06
[01.30 09:00:07] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  67%|█████████████▍      | 571/846 [0:29:29<0:16:36, 0.28Hz] | (Ep 4.050)
[01.30 09:00:13] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  68%|█████████████▌      | 574/846 [0:29:36<0:16:29, 0.27Hz] | (Ep 4.071)
[01.30 09:00:19] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  68%|█████████████▋      | 577/846 [0:29:42<0:16:15, 0.28Hz] | (Ep 4.092)
[01.30 09:00:21] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:00:21] ┇ INFO    ┇             DeepKNLP ┇ >>    578    4.099  0.0094    0.0667444      6.65025e-06
[01.30 09:00:26] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  69%|█████████████▋      | 580/846 [0:29:48<0:16:01, 0.28Hz] | (Ep 4.113)
[01.30 09:00:32] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  69%|█████████████▊      | 583/846 [0:29:55<0:12:44, 0.34Hz] | (Ep 4.135)
[01.30 09:00:39] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  69%|█████████████▊      | 586/846 [0:30:02<0:12:37, 0.34Hz] | (Ep 4.156)
[01.30 09:00:46] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  70%|█████████████▉      | 589/846 [0:30:08<0:12:28, 0.34Hz] | (Ep 4.177)
[01.30 09:00:52] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  70%|█████████████▉      | 592/846 [0:30:15<0:12:22, 0.34Hz] | (Ep 4.199)
[01.30 09:00:52] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:00:52] ┇ INFO    ┇             DeepKNLP ┇ >>    592    4.199  0.0096    0.0980941      6.30542e-06
[01.30 09:00:58] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  70%|██████████████      | 594/846 [0:30:20<0:12:19, 0.34Hz] | (Ep 4.213)
[01.30 09:01:04] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  71%|██████████████      | 597/846 [0:30:26<0:12:09, 0.34Hz] | (Ep 4.234)
[01.30 09:01:10] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  71%|██████████████▏     | 600/846 [0:30:32<0:11:57, 0.34Hz] | (Ep 4.255)
[01.30 09:01:16] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  71%|██████████████▎     | 603/846 [0:30:39<0:11:48, 0.34Hz] | (Ep 4.277)
[01.30 09:01:23] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  72%|██████████████▎     | 606/846 [0:30:45<0:11:39, 0.34Hz] | (Ep 4.298)
[01.30 09:01:23] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:01:23] ┇ INFO    ┇             DeepKNLP ┇ >>    606    4.298  0.0103      0.07588      5.96059e-06
[01.30 09:01:29] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  72%|██████████████▍     | 609/846 [0:30:51<0:11:27, 0.34Hz] | (Ep 4.319)
[01.30 09:01:33] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 09:01:33] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 09:01:33] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 09:01:41] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 98144.88Hz]
[01.30 09:01:50] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:17, 0.23Hz]
[01.30 09:01:58] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 09:02:07] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 09:02:17] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:35<0:00:00, 0.17Hz]
[01.30 09:02:18] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 09:02:18] ┇ INFO    ┇             DeepKNLP ┇ >>    611    4.333             0.60745                    0.602871               0.672023                  0.702472                 0.685185          0.897059                0.76259        0.704236         44.7405                     15.646                    0.134
[01.30 09:02:20] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  72%|██████████████▍     | 612/846 [0:31:42<0:14:02, 0.28Hz] | (Ep 4.340)
[01.30 09:02:27] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  73%|██████████████▌     | 615/846 [0:31:49<0:13:52, 0.28Hz] | (Ep 4.362)
[01.30 09:02:33] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  73%|██████████████▌     | 618/846 [0:31:56<0:13:43, 0.28Hz] | (Ep 4.383)
[01.30 09:02:38] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:02:38] ┇ INFO    ┇             DeepKNLP ┇ >>    620    4.397  0.0099    0.0697421      5.61576e-06
[01.30 09:02:41] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  73%|██████████████▋     | 621/846 [0:32:03<0:13:35, 0.28Hz] | (Ep 4.404)
[01.30 09:02:47] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  74%|██████████████▊     | 624/846 [0:32:10<0:13:25, 0.28Hz] | (Ep 4.426)
[01.30 09:02:53] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  74%|██████████████▊     | 626/846 [0:32:15<0:13:22, 0.27Hz] | (Ep 4.440)
[01.30 09:03:00] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  74%|██████████████▊     | 629/846 [0:32:22<0:10:33, 0.34Hz] | (Ep 4.461)
[01.30 09:03:06] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  75%|██████████████▉     | 632/846 [0:32:28<0:10:25, 0.34Hz] | (Ep 4.482)
[01.30 09:03:11] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:03:11] ┇ INFO    ┇             DeepKNLP ┇ >>    634    4.496  0.0102    0.0665092      5.27094e-06
[01.30 09:03:13] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  75%|███████████████     | 635/846 [0:32:35<0:10:16, 0.34Hz] | (Ep 4.504)
[01.30 09:03:19] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  75%|███████████████     | 638/846 [0:32:41<0:10:06, 0.34Hz] | (Ep 4.525)
[01.30 09:03:25] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  76%|███████████████▏    | 641/846 [0:32:47<0:09:58, 0.34Hz] | (Ep 4.546)
[01.30 09:03:31] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  76%|███████████████▏    | 644/846 [0:32:54<0:09:49, 0.34Hz] | (Ep 4.567)
[01.30 09:03:38] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  76%|███████████████▎    | 647/846 [0:33:00<0:09:38, 0.34Hz] | (Ep 4.589)
[01.30 09:03:42] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:03:42] ┇ INFO    ┇             DeepKNLP ┇ >>    649    4.603  0.0104    0.0813396      4.90148e-06
[01.30 09:03:44] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  77%|███████████████▎    | 650/846 [0:33:06<0:09:27, 0.35Hz] | (Ep 4.610)
[01.30 09:03:50] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  77%|███████████████▍    | 653/846 [0:33:13<0:09:18, 0.35Hz] | (Ep 4.631)
[01.30 09:03:57] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  78%|███████████████▌    | 656/846 [0:33:19<0:09:05, 0.35Hz] | (Ep 4.652)
[01.30 09:04:01] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 09:04:01] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 09:04:01] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 09:04:09] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 86490.07Hz]
[01.30 09:04:17] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:16, 0.24Hz]
[01.30 09:04:26] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 09:04:34] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 09:04:44] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:34<0:00:00, 0.17Hz]
[01.30 09:04:45] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 09:04:45] ┇ INFO    ┇             DeepKNLP ┇ >>    658    4.667             0.58597                    0.634994               0.690407                  0.714047                 0.681922          0.882353               0.794258        0.711993         44.0832                     15.879                    0.136
[01.30 09:04:47] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  78%|███████████████▌    | 659/846 [0:34:09<0:11:07, 0.28Hz] | (Ep 4.674)
[01.30 09:04:53] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  78%|███████████████▋    | 662/846 [0:34:15<0:10:57, 0.28Hz] | (Ep 4.695)
[01.30 09:04:55] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:04:55] ┇ INFO    ┇             DeepKNLP ┇ >>    663    4.702  0.0094    0.0717677      4.55665e-06
[01.30 09:04:59] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  79%|███████████████▋    | 665/846 [0:34:22<0:10:45, 0.28Hz] | (Ep 4.716)
[01.30 09:05:06] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  79%|███████████████▊    | 668/846 [0:34:28<0:10:35, 0.28Hz] | (Ep 4.738)
[01.30 09:05:12] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  79%|███████████████▊    | 671/846 [0:34:35<0:10:26, 0.28Hz] | (Ep 4.759)
[01.30 09:05:19] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  80%|███████████████▉    | 674/846 [0:34:41<0:10:16, 0.28Hz] | (Ep 4.780)
[01.30 09:05:25] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  80%|████████████████    | 677/846 [0:34:48<0:08:04, 0.35Hz] | (Ep 4.801)
[01.30 09:05:25] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:05:25] ┇ INFO    ┇             DeepKNLP ┇ >>    677    4.801  0.0075     0.105747      4.21182e-06
[01.30 09:05:32] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  80%|████████████████    | 680/846 [0:34:54<0:07:55, 0.35Hz] | (Ep 4.823)
[01.30 09:05:39] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  81%|████████████████▏   | 683/846 [0:35:01<0:07:47, 0.35Hz] | (Ep 4.844)
[01.30 09:05:46] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  81%|████████████████▏   | 686/846 [0:35:08<0:07:37, 0.35Hz] | (Ep 4.865)
[01.30 09:05:52] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  81%|████████████████▎   | 689/846 [0:35:14<0:07:25, 0.35Hz] | (Ep 4.887)
[01.30 09:05:56] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:05:56] ┇ INFO    ┇             DeepKNLP ┇ >>    691    4.901  0.0107    0.0669846        3.867e-06
[01.30 09:05:58] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  82%|████████████████▎   | 692/846 [0:35:21<0:07:16, 0.35Hz] | (Ep 4.908)
[01.30 09:06:05] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  82%|████████████████▍   | 695/846 [0:35:27<0:07:08, 0.35Hz] | (Ep 4.929)
[01.30 09:06:10] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  82%|████████████████▍   | 697/846 [0:35:32<0:07:03, 0.35Hz] | (Ep 4.943)
[01.30 09:06:16] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  83%|████████████████▌   | 700/846 [0:35:38<0:06:55, 0.35Hz] | (Ep 4.965)
[01.30 09:06:22] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  83%|████████████████▌   | 703/846 [0:35:45<0:06:47, 0.35Hz] | (Ep 4.986)
[01.30 09:06:27] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:06:27] ┇ INFO    ┇             DeepKNLP ┇ >>    705        5  0.0093    0.0672454      3.52217e-06
[01.30 09:06:27] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 09:06:27] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 09:06:27] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 09:06:35] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 90163.16Hz]
[01.30 09:06:43] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:16, 0.24Hz]
[01.30 09:06:52] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 09:07:00] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 09:07:10] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:35<0:00:00, 0.17Hz]
[01.30 09:07:11] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 09:07:11] ┇ INFO    ┇             DeepKNLP ┇ >>    705        5            0.605087                    0.633222               0.694203                  0.708023                 0.696629          0.894866               0.810552        0.720369         44.4372                     15.753                    0.135
[01.30 09:07:13] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  83%|████████████████▋   | 706/846 [0:36:35<0:08:16, 0.28Hz] | (Ep 5.007)
[01.30 09:07:19] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  84%|████████████████▊   | 709/846 [0:36:42<0:08:06, 0.28Hz] | (Ep 5.028)
[01.30 09:07:21] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:07:21] ┇ INFO    ┇             DeepKNLP ┇ >>    710    5.035  0.0066    0.0472729      3.42365e-06
[01.30 09:07:25] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  84%|████████████████▊   | 712/846 [0:36:48<0:07:55, 0.28Hz] | (Ep 5.050)
[01.30 09:07:32] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  85%|████████████████▉   | 715/846 [0:36:54<0:07:44, 0.28Hz] | (Ep 5.071)
[01.30 09:07:38] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  85%|████████████████▉   | 718/846 [0:37:00<0:07:33, 0.28Hz] | (Ep 5.092)
[01.30 09:07:40] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:07:40] ┇ INFO    ┇             DeepKNLP ┇ >>    719    5.099  0.0044     0.015479      3.20197e-06
[01.30 09:07:45] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  85%|█████████████████   | 721/846 [0:37:07<0:07:25, 0.28Hz] | (Ep 5.113)
[01.30 09:07:51] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  86%|█████████████████   | 724/846 [0:37:13<0:05:48, 0.35Hz] | (Ep 5.135)
[01.30 09:07:58] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  86%|█████████████████▏  | 727/846 [0:37:20<0:05:41, 0.35Hz] | (Ep 5.156)
[01.30 09:08:04] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  86%|█████████████████▎  | 730/846 [0:37:26<0:05:31, 0.35Hz] | (Ep 5.177)
[01.30 09:08:10] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  87%|█████████████████▎  | 733/846 [0:37:33<0:05:23, 0.35Hz] | (Ep 5.199)
[01.30 09:08:10] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:08:10] ┇ INFO    ┇             DeepKNLP ┇ >>    733    5.199  0.0038   0.00935036      2.85714e-06
[01.30 09:08:17] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  87%|█████████████████▍  | 736/846 [0:37:40<0:05:15, 0.35Hz] | (Ep 5.220)
[01.30 09:08:24] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  87%|█████████████████▍  | 739/846 [0:37:46<0:05:07, 0.35Hz] | (Ep 5.241)
[01.30 09:08:30] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  88%|█████████████████▌  | 742/846 [0:37:53<0:04:58, 0.35Hz] | (Ep 5.262)
[01.30 09:08:36] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  88%|█████████████████▌  | 745/846 [0:37:59<0:04:48, 0.35Hz] | (Ep 5.284)
[01.30 09:08:41] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:08:41] ┇ INFO    ┇             DeepKNLP ┇ >>    747    5.298  0.0036    0.0248965      2.51232e-06
[01.30 09:08:44] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  88%|█████████████████▋  | 748/846 [0:38:06<0:04:40, 0.35Hz] | (Ep 5.305)
[01.30 09:08:50] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  89%|█████████████████▊  | 751/846 [0:38:13<0:04:32, 0.35Hz] | (Ep 5.326)
[01.30 09:08:52] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 09:08:52] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 09:08:52] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 09:09:00] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 88090.15Hz]
[01.30 09:09:08] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:16, 0.24Hz]
[01.30 09:09:17] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 09:09:25] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 09:09:35] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:34<0:00:00, 0.17Hz]
[01.30 09:09:36] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 09:09:36] ┇ INFO    ┇             DeepKNLP ┇ >>    752    5.333            0.590909                    0.638732               0.679328                  0.717949                 0.697309          0.902439               0.782816         0.71564         43.5928                     16.058                    0.138
[01.30 09:09:38] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  89%|█████████████████▊  | 753/846 [0:39:00<0:05:31, 0.28Hz] | (Ep 5.340)
[01.30 09:09:44] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  89%|█████████████████▊  | 756/846 [0:39:07<0:05:19, 0.28Hz] | (Ep 5.362)
[01.30 09:09:50] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  90%|█████████████████▉  | 759/846 [0:39:13<0:05:08, 0.28Hz] | (Ep 5.383)
[01.30 09:09:55] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:09:55] ┇ INFO    ┇             DeepKNLP ┇ >>    761    5.397  0.0039     0.025813      2.16749e-06
[01.30 09:09:57] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  90%|██████████████████  | 762/846 [0:39:19<0:04:56, 0.28Hz] | (Ep 5.404)
[01.30 09:10:03] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  90%|██████████████████  | 765/846 [0:39:26<0:04:47, 0.28Hz] | (Ep 5.426)
[01.30 09:10:10] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  91%|██████████████████▏ | 768/846 [0:39:32<0:04:36, 0.28Hz] | (Ep 5.447)
[01.30 09:10:16] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  91%|██████████████████▏ | 770/846 [0:39:38<0:03:37, 0.35Hz] | (Ep 5.461)
[01.30 09:10:22] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  91%|██████████████████▎ | 773/846 [0:39:44<0:03:30, 0.35Hz] | (Ep 5.482)
[01.30 09:10:26] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:10:26] ┇ INFO    ┇             DeepKNLP ┇ >>    775    5.496  0.0036    0.0278278      1.82266e-06
[01.30 09:10:29] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  92%|██████████████████▎ | 776/846 [0:39:51<0:03:21, 0.35Hz] | (Ep 5.504)
[01.30 09:10:35] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  92%|██████████████████▍ | 779/846 [0:39:57<0:03:12, 0.35Hz] | (Ep 5.525)
[01.30 09:10:41] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  92%|██████████████████▍ | 782/846 [0:40:03<0:03:03, 0.35Hz] | (Ep 5.546)
[01.30 09:10:48] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  93%|██████████████████▌ | 785/846 [0:40:10<0:02:54, 0.35Hz] | (Ep 5.567)
[01.30 09:10:53] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  93%|██████████████████▋ | 788/846 [0:40:16<0:02:45, 0.35Hz] | (Ep 5.589)
[01.30 09:10:58] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:10:58] ┇ INFO    ┇             DeepKNLP ┇ >>    790    5.603  0.0035    0.0201652       1.4532e-06
[01.30 09:11:00] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  93%|██████████████████▋ | 791/846 [0:40:22<0:02:37, 0.35Hz] | (Ep 5.610)
[01.30 09:11:07] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  94%|██████████████████▊ | 794/846 [0:40:29<0:02:28, 0.35Hz] | (Ep 5.631)
[01.30 09:11:13] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  94%|██████████████████▊ | 797/846 [0:40:35<0:02:19, 0.35Hz] | (Ep 5.652)
[01.30 09:11:17] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 09:11:17] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 09:11:17] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 09:11:26] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 96805.45Hz]
[01.30 09:11:34] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:16, 0.24Hz]
[01.30 09:11:43] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 09:11:51] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 09:12:01] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:34<0:00:00, 0.17Hz]
[01.30 09:12:02] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 09:12:02] ┇ INFO    ┇             DeepKNLP ┇ >>    799    5.667               0.592                    0.636364               0.685212                  0.715824                 0.701124          0.907317                0.79717        0.719287         44.5201                     15.723                    0.135
[01.30 09:12:04] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  95%|██████████████████▉ | 800/846 [0:41:26<0:02:44, 0.28Hz] | (Ep 5.674)
[01.30 09:12:11] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  95%|██████████████████▉ | 803/846 [0:41:33<0:02:33, 0.28Hz] | (Ep 5.695)
[01.30 09:12:13] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:12:13] ┇ INFO    ┇             DeepKNLP ┇ >>    804    5.702  0.0036    0.0210306      1.10837e-06
[01.30 09:12:17] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  95%|███████████████████ | 806/846 [0:41:39<0:02:22, 0.28Hz] | (Ep 5.716)
[01.30 09:12:23] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  96%|███████████████████▏| 809/846 [0:41:46<0:02:11, 0.28Hz] | (Ep 5.738)
[01.30 09:12:29] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  96%|███████████████████▏| 812/846 [0:41:52<0:02:00, 0.28Hz] | (Ep 5.759)
[01.30 09:12:35] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  96%|███████████████████▎| 815/846 [0:41:58<0:01:49, 0.28Hz] | (Ep 5.780)
[01.30 09:12:42] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  97%|███████████████████▎| 818/846 [0:42:04<0:01:19, 0.35Hz] | (Ep 5.801)
[01.30 09:12:42] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:12:42] ┇ INFO    ┇             DeepKNLP ┇ >>    818    5.801  0.0031    0.0566313      7.63547e-07
[01.30 09:12:48] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  97%|███████████████████▍| 821/846 [0:42:10<0:01:11, 0.35Hz] | (Ep 5.823)
[01.30 09:12:55] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  97%|███████████████████▍| 824/846 [0:42:17<0:01:02, 0.35Hz] | (Ep 5.844)
[01.30 09:13:01] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  98%|███████████████████▌| 827/846 [0:42:23<0:00:54, 0.35Hz] | (Ep 5.865)
[01.30 09:13:08] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  98%|███████████████████▌| 830/846 [0:42:30<0:00:45, 0.35Hz] | (Ep 5.887)
[01.30 09:13:12] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:13:12] ┇ INFO    ┇             DeepKNLP ┇ >>    832    5.901  0.0033    0.0287597      4.18719e-07
[01.30 09:13:14] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  98%|███████████████████▋| 833/846 [0:42:36<0:00:36, 0.35Hz] | (Ep 5.908)
[01.30 09:13:21] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  99%|███████████████████▊| 836/846 [0:42:43<0:00:28, 0.35Hz] | (Ep 5.929)
[01.30 09:13:27] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING]  99%|███████████████████▊| 839/846 [0:42:49<0:00:19, 0.35Hz] | (Ep 5.950)
[01.30 09:13:33] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING] 100%|███████████████████▉| 842/846 [0:42:55<0:00:11, 0.35Hz] | (Ep 5.972)
[01.30 09:13:39] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING] 100%|███████████████████▉| 845/846 [0:43:01<0:00:02, 0.35Hz] | (Ep 5.993)
[01.30 09:13:41] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    loss    grad_norm    learning_rate
[01.30 09:13:41] ┇ INFO    ┇             DeepKNLP ┇ >>    846        6  0.0035    0.0178158      7.38916e-08
[01.30 09:13:41] ┇ INFO    ┇ transformers.trainer ┇ ***** Running Evaluation *****
[01.30 09:13:41] ┇ INFO    ┇ transformers.trainer ┇   Num examples = 700
[01.30 09:13:41] ┇ INFO    ┇ transformers.trainer ┇   Batch size = 32
[01.30 09:13:50] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  17%|███▎                | 1/6 [0:00:00<0:00:00, 83451.71Hz]
[01.30 09:13:58] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  33%|██████▋             | 2/6 [0:00:08<0:00:16, 0.24Hz]
[01.30 09:14:06] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  50%|██████████          | 3/6 [0:00:16<0:00:16, 0.18Hz]
[01.30 09:14:15] ┇ INFO    ┇             DeepKNLP ┇ [METERING]  67%|█████████████▎      | 4/6 [0:00:25<0:00:12, 0.16Hz]
[01.30 09:14:25] ┇ INFO    ┇             DeepKNLP ┇ [METERING] 100%|████████████████████| 6/6 [0:00:34<0:00:00, 0.17Hz]
[01.30 09:14:26] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    eval_crossner_ai    eval_crossner_literature    eval_crossner_music    eval_crossner_politics    eval_crossner_science    eval_mit-movie    eval_mit-restaurant    eval_AVERAGE    eval_runtime    eval_samples_per_second    eval_steps_per_second
[01.30 09:14:26] ┇ INFO    ┇             DeepKNLP ┇ >>    846        6            0.596817                    0.645527               0.675872                   0.72802                 0.703247          0.907317               0.783217        0.720002         44.2705                     15.812                    0.136
[01.30 09:14:26] ┇ INFO    ┇ transformers.trainer ┇ 

Training completed. Do not forget to share your model on huggingface.co/models =)


[01.30 09:14:26] ┇ INFO    ┇             DeepKNLP ┇ >>   step    epoch    train_runtime    train_samples_per_second    train_steps_per_second    total_flos    train_loss
[01.30 09:14:26] ┇ INFO    ┇             DeepKNLP ┇ >>    846        6          2628.38                      41.398                     0.322   2.69224e+17     0.0329793
[01.30 09:14:26] ┇ INFO    ┇             DeepKNLP ┇ [TRAINING] 100%|████████████████████| 846/846 [0:43:03<0:00:00, 0.35Hz] | (Ep 6.000)
[01.30 09:14:26] ┇ INFO    ┇             DeepKNLP ┇ Train result: TrainOutput(global_step=846, training_loss=0.03297932731344345, metrics={'train_runtime': 2628.3772, 'train_samples_per_second': 41.398, 'train_steps_per_second': 0.322, 'total_flos': 2.692241488305193e+17, 'train_loss': 0.03297932731344345, 'epoch': 6.0})
[01.30 09:14:26] ┇ INFO    ┇       chrisbase.data ┇ =========================================================================================================================================
[01.30 09:14:26] ┇ INFO    ┇       chrisbase.data ┇ [EXIT] python task2-nerG-trainer2.py --local_rank=0 --trainer_deepspeed configs/deepspeed/ds1_llama.json ($=00:44:03.750)
[01.30 09:14:26] ┇ INFO    ┇       chrisbase.data ┇ =========================================================================================================================================
