{
  "best_metric": 0.5604548525281161,
  "best_model_checkpoint": "output/GNER-zeroshot/FlanT5-Base-BL/checkpoint-8250",
  "epoch": 11.50060606060606,
  "eval_steps": 9223372036854775807,
  "global_step": 9488,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006060606060606061,
      "grad_norm": 8.103461265563965,
      "learning_rate": 5.381462829997033e-06,
      "loss": 1.602,
      "step": 5
    },
    {
      "epoch": 0.012121212121212121,
      "grad_norm": 7.274684429168701,
      "learning_rate": 7.699132719020058e-06,
      "loss": 1.5494,
      "step": 10
    },
    {
      "epoch": 0.01818181818181818,
      "grad_norm": 5.7299485206604,
      "learning_rate": 9.05488269314909e-06,
      "loss": 1.3619,
      "step": 15
    },
    {
      "epoch": 0.024242424242424242,
      "grad_norm": 4.250746726989746,
      "learning_rate": 1.001680260804308e-05,
      "loss": 1.1807,
      "step": 20
    },
    {
      "epoch": 0.030303030303030304,
      "grad_norm": 2.4217615127563477,
      "learning_rate": 1.0762925659994066e-05,
      "loss": 0.8938,
      "step": 25
    },
    {
      "epoch": 0.03636363636363636,
      "grad_norm": 1.8147835731506348,
      "learning_rate": 1.1372552582172113e-05,
      "loss": 0.7923,
      "step": 30
    },
    {
      "epoch": 0.04242424242424243,
      "grad_norm": 1.3787541389465332,
      "learning_rate": 1.1887984800650519e-05,
      "loss": 0.6639,
      "step": 35
    },
    {
      "epoch": 0.048484848484848485,
      "grad_norm": 0.9853449463844299,
      "learning_rate": 1.2334472497066103e-05,
      "loss": 0.5723,
      "step": 40
    },
    {
      "epoch": 0.05454545454545454,
      "grad_norm": 0.7752466201782227,
      "learning_rate": 1.2728302556301145e-05,
      "loss": 0.5008,
      "step": 45
    },
    {
      "epoch": 0.06060606060606061,
      "grad_norm": 0.6402214169502258,
      "learning_rate": 1.308059554901709e-05,
      "loss": 0.4495,
      "step": 50
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.5710866451263428,
      "learning_rate": 1.3399283325646878e-05,
      "loss": 0.3792,
      "step": 55
    },
    {
      "epoch": 0.07272727272727272,
      "grad_norm": 0.4922749996185303,
      "learning_rate": 1.3690222471195137e-05,
      "loss": 0.3458,
      "step": 60
    },
    {
      "epoch": 0.07878787878787878,
      "grad_norm": 0.4429765045642853,
      "learning_rate": 1.3957860540877486e-05,
      "loss": 0.3263,
      "step": 65
    },
    {
      "epoch": 0.08484848484848485,
      "grad_norm": 0.4703295826911926,
      "learning_rate": 1.4205654689673544e-05,
      "loss": 0.3027,
      "step": 70
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.39922890067100525,
      "learning_rate": 1.4436345523146123e-05,
      "loss": 0.2938,
      "step": 75
    },
    {
      "epoch": 0.09696969696969697,
      "grad_norm": 0.3947434723377228,
      "learning_rate": 1.4652142386089125e-05,
      "loss": 0.2622,
      "step": 80
    },
    {
      "epoch": 0.10303030303030303,
      "grad_norm": 0.35621803998947144,
      "learning_rate": 1.4854852379663441e-05,
      "loss": 0.2549,
      "step": 85
    },
    {
      "epoch": 0.10909090909090909,
      "grad_norm": 0.3626471161842346,
      "learning_rate": 1.5045972445324168e-05,
      "loss": 0.2433,
      "step": 90
    },
    {
      "epoch": 0.11515151515151516,
      "grad_norm": 0.3072195053100586,
      "learning_rate": 1.5226756518657677e-05,
      "loss": 0.227,
      "step": 95
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 0.32572507858276367,
      "learning_rate": 1.5398265438040116e-05,
      "loss": 0.2288,
      "step": 100
    },
    {
      "epoch": 0.12727272727272726,
      "grad_norm": 0.2735559344291687,
      "learning_rate": 1.5561404663802574e-05,
      "loss": 0.2107,
      "step": 105
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.28182291984558105,
      "learning_rate": 1.5716953214669903e-05,
      "loss": 0.2085,
      "step": 110
    },
    {
      "epoch": 0.1393939393939394,
      "grad_norm": 0.263134241104126,
      "learning_rate": 1.5865586166680462e-05,
      "loss": 0.1934,
      "step": 115
    },
    {
      "epoch": 0.14545454545454545,
      "grad_norm": 0.25466546416282654,
      "learning_rate": 1.600789236021816e-05,
      "loss": 0.19,
      "step": 120
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 0.267037570476532,
      "learning_rate": 1.6144388489991102e-05,
      "loss": 0.1926,
      "step": 125
    },
    {
      "epoch": 0.15757575757575756,
      "grad_norm": 0.24684621393680573,
      "learning_rate": 1.627553042990051e-05,
      "loss": 0.1781,
      "step": 130
    },
    {
      "epoch": 0.16363636363636364,
      "grad_norm": 0.24782684445381165,
      "learning_rate": 1.6401722419453202e-05,
      "loss": 0.1747,
      "step": 135
    },
    {
      "epoch": 0.1696969696969697,
      "grad_norm": 0.23736652731895447,
      "learning_rate": 1.6523324578696566e-05,
      "loss": 0.1766,
      "step": 140
    },
    {
      "epoch": 0.17575757575757575,
      "grad_norm": 0.226153165102005,
      "learning_rate": 1.664065910385031e-05,
      "loss": 0.1745,
      "step": 145
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.24010683596134186,
      "learning_rate": 1.6754015412169146e-05,
      "loss": 0.1687,
      "step": 150
    },
    {
      "epoch": 0.18787878787878787,
      "grad_norm": 0.20409193634986877,
      "learning_rate": 1.6863654442889655e-05,
      "loss": 0.1672,
      "step": 155
    },
    {
      "epoch": 0.19393939393939394,
      "grad_norm": 0.2341354489326477,
      "learning_rate": 1.696981227511215e-05,
      "loss": 0.1594,
      "step": 160
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.20770524442195892,
      "learning_rate": 1.7072703188798933e-05,
      "loss": 0.1545,
      "step": 165
    },
    {
      "epoch": 0.20606060606060606,
      "grad_norm": 0.22512178122997284,
      "learning_rate": 1.7172522268686466e-05,
      "loss": 0.1568,
      "step": 170
    },
    {
      "epoch": 0.21212121212121213,
      "grad_norm": 0.22916853427886963,
      "learning_rate": 1.7269447630647555e-05,
      "loss": 0.1505,
      "step": 175
    },
    {
      "epoch": 0.21818181818181817,
      "grad_norm": 0.20115238428115845,
      "learning_rate": 1.7363642334347193e-05,
      "loss": 0.1538,
      "step": 180
    },
    {
      "epoch": 0.22424242424242424,
      "grad_norm": 0.20741894841194153,
      "learning_rate": 1.7455256033784897e-05,
      "loss": 0.1489,
      "step": 185
    },
    {
      "epoch": 0.23030303030303031,
      "grad_norm": 0.20122955739498138,
      "learning_rate": 1.75444264076807e-05,
      "loss": 0.1497,
      "step": 190
    },
    {
      "epoch": 0.23636363636363636,
      "grad_norm": 0.21524739265441895,
      "learning_rate": 1.7631280404029543e-05,
      "loss": 0.1388,
      "step": 195
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 0.2007366418838501,
      "learning_rate": 1.7715935327063138e-05,
      "loss": 0.1385,
      "step": 200
    },
    {
      "epoch": 0.24848484848484848,
      "grad_norm": 0.18823440372943878,
      "learning_rate": 1.7798499789975307e-05,
      "loss": 0.1383,
      "step": 205
    },
    {
      "epoch": 0.2545454545454545,
      "grad_norm": 0.17765071988105774,
      "learning_rate": 1.78790745528256e-05,
      "loss": 0.1411,
      "step": 210
    },
    {
      "epoch": 0.2606060606060606,
      "grad_norm": 0.2408422827720642,
      "learning_rate": 1.7957753261836987e-05,
      "loss": 0.1342,
      "step": 215
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.17795483767986298,
      "learning_rate": 1.8034623103692924e-05,
      "loss": 0.1284,
      "step": 220
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.2203710377216339,
      "learning_rate": 1.810976538629818e-05,
      "loss": 0.135,
      "step": 225
    },
    {
      "epoch": 0.2787878787878788,
      "grad_norm": 0.17267049849033356,
      "learning_rate": 1.8183256055703488e-05,
      "loss": 0.1366,
      "step": 230
    },
    {
      "epoch": 0.28484848484848485,
      "grad_norm": 0.2057940810918808,
      "learning_rate": 1.8255166157433265e-05,
      "loss": 0.1317,
      "step": 235
    },
    {
      "epoch": 0.2909090909090909,
      "grad_norm": 0.18540549278259277,
      "learning_rate": 1.8325562249241182e-05,
      "loss": 0.1259,
      "step": 240
    },
    {
      "epoch": 0.296969696969697,
      "grad_norm": 0.16339805722236633,
      "learning_rate": 1.839450677130401e-05,
      "loss": 0.1248,
      "step": 245
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 0.1832202821969986,
      "learning_rate": 1.8462058379014124e-05,
      "loss": 0.1249,
      "step": 250
    },
    {
      "epoch": 0.3090909090909091,
      "grad_norm": 0.16647426784038544,
      "learning_rate": 1.8528272242815496e-05,
      "loss": 0.1245,
      "step": 255
    },
    {
      "epoch": 0.3151515151515151,
      "grad_norm": 0.1584421843290329,
      "learning_rate": 1.8593200318923534e-05,
      "loss": 0.1236,
      "step": 260
    },
    {
      "epoch": 0.3212121212121212,
      "grad_norm": 0.19571547210216522,
      "learning_rate": 1.8656891594257228e-05,
      "loss": 0.1206,
      "step": 265
    },
    {
      "epoch": 0.32727272727272727,
      "grad_norm": 0.20566871762275696,
      "learning_rate": 1.8719392308476227e-05,
      "loss": 0.1271,
      "step": 270
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.17092950642108917,
      "learning_rate": 1.878074615564391e-05,
      "loss": 0.1184,
      "step": 275
    },
    {
      "epoch": 0.3393939393939394,
      "grad_norm": 0.1428927332162857,
      "learning_rate": 1.8840994467719588e-05,
      "loss": 0.114,
      "step": 280
    },
    {
      "epoch": 0.34545454545454546,
      "grad_norm": 0.17646346986293793,
      "learning_rate": 1.8900176381809734e-05,
      "loss": 0.1148,
      "step": 285
    },
    {
      "epoch": 0.3515151515151515,
      "grad_norm": 0.1649862676858902,
      "learning_rate": 1.8958328992873333e-05,
      "loss": 0.1133,
      "step": 290
    },
    {
      "epoch": 0.3575757575757576,
      "grad_norm": 0.20831798017024994,
      "learning_rate": 1.9015487493373553e-05,
      "loss": 0.1173,
      "step": 295
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.15879911184310913,
      "learning_rate": 1.9071685301192168e-05,
      "loss": 0.1157,
      "step": 300
    },
    {
      "epoch": 0.3696969696969697,
      "grad_norm": 0.13806016743183136,
      "learning_rate": 1.912695417697111e-05,
      "loss": 0.115,
      "step": 305
    },
    {
      "epoch": 0.37575757575757573,
      "grad_norm": 0.15725339949131012,
      "learning_rate": 1.9181324331912677e-05,
      "loss": 0.1175,
      "step": 310
    },
    {
      "epoch": 0.38181818181818183,
      "grad_norm": 0.17106567323207855,
      "learning_rate": 1.9234824526954633e-05,
      "loss": 0.1083,
      "step": 315
    },
    {
      "epoch": 0.3878787878787879,
      "grad_norm": 0.16111381351947784,
      "learning_rate": 1.9287482164135173e-05,
      "loss": 0.1079,
      "step": 320
    },
    {
      "epoch": 0.3939393939393939,
      "grad_norm": 0.13799354434013367,
      "learning_rate": 1.933932337087452e-05,
      "loss": 0.1119,
      "step": 325
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.1611790806055069,
      "learning_rate": 1.9390373077821954e-05,
      "loss": 0.1109,
      "step": 330
    },
    {
      "epoch": 0.40606060606060607,
      "grad_norm": 0.1536550521850586,
      "learning_rate": 1.944065509084906e-05,
      "loss": 0.108,
      "step": 335
    },
    {
      "epoch": 0.4121212121212121,
      "grad_norm": 0.13576620817184448,
      "learning_rate": 1.9490192157709488e-05,
      "loss": 0.1041,
      "step": 340
    },
    {
      "epoch": 0.41818181818181815,
      "grad_norm": 0.13724711537361145,
      "learning_rate": 1.9539006029832518e-05,
      "loss": 0.1039,
      "step": 345
    },
    {
      "epoch": 0.42424242424242425,
      "grad_norm": 0.1404501497745514,
      "learning_rate": 1.9587117519670574e-05,
      "loss": 0.1082,
      "step": 350
    },
    {
      "epoch": 0.4303030303030303,
      "grad_norm": 0.15030664205551147,
      "learning_rate": 1.963454655397911e-05,
      "loss": 0.1027,
      "step": 355
    },
    {
      "epoch": 0.43636363636363634,
      "grad_norm": 0.15490742027759552,
      "learning_rate": 1.968131222337022e-05,
      "loss": 0.1074,
      "step": 360
    },
    {
      "epoch": 0.44242424242424244,
      "grad_norm": 0.17126260697841644,
      "learning_rate": 1.9727432828448466e-05,
      "loss": 0.1066,
      "step": 365
    },
    {
      "epoch": 0.4484848484848485,
      "grad_norm": 0.12164664268493652,
      "learning_rate": 1.977292592280792e-05,
      "loss": 0.102,
      "step": 370
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.14716759324073792,
      "learning_rate": 1.9817808353143154e-05,
      "loss": 0.0979,
      "step": 375
    },
    {
      "epoch": 0.46060606060606063,
      "grad_norm": 0.14427918195724487,
      "learning_rate": 1.9862096296703722e-05,
      "loss": 0.1033,
      "step": 380
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.15549440681934357,
      "learning_rate": 1.9905805296300364e-05,
      "loss": 0.1033,
      "step": 385
    },
    {
      "epoch": 0.4727272727272727,
      "grad_norm": 0.14508859813213348,
      "learning_rate": 1.9948950293052568e-05,
      "loss": 0.1003,
      "step": 390
    },
    {
      "epoch": 0.47878787878787876,
      "grad_norm": 0.135864719748497,
      "learning_rate": 1.999154565705013e-05,
      "loss": 0.1028,
      "step": 395
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 0.1580711305141449,
      "learning_rate": 2e-05,
      "loss": 0.0992,
      "step": 400
    },
    {
      "epoch": 0.4909090909090909,
      "grad_norm": 0.14691361784934998,
      "learning_rate": 2e-05,
      "loss": 0.0953,
      "step": 405
    },
    {
      "epoch": 0.49696969696969695,
      "grad_norm": 0.1595073789358139,
      "learning_rate": 2e-05,
      "loss": 0.0969,
      "step": 410
    },
    {
      "epoch": 0.4993939393939394,
      "eval_average": 0.2864838091102752,
      "eval_crossner_ai": 0.24911868385170965,
      "eval_crossner_literature": 0.3961456102284806,
      "eval_crossner_music": 0.36697247701417135,
      "eval_crossner_politics": 0.3988005996504813,
      "eval_crossner_science": 0.38522954086860484,
      "eval_mit-movie": 0.10876132925777604,
      "eval_mit-restaurant": 0.1003584229007027,
      "eval_runtime": 49.3939,
      "eval_samples_per_second": 14.172,
      "eval_steps_per_second": 0.142,
      "step": 412
    },
    {
      "epoch": 0.503030303030303,
      "grad_norm": 0.1395159512758255,
      "learning_rate": 2e-05,
      "loss": 0.0966,
      "step": 415
    },
    {
      "epoch": 0.509090909090909,
      "grad_norm": 0.14009664952754974,
      "learning_rate": 2e-05,
      "loss": 0.0929,
      "step": 420
    },
    {
      "epoch": 0.5151515151515151,
      "grad_norm": 0.1542203724384308,
      "learning_rate": 2e-05,
      "loss": 0.0962,
      "step": 425
    },
    {
      "epoch": 0.5212121212121212,
      "grad_norm": 0.14755982160568237,
      "learning_rate": 2e-05,
      "loss": 0.0987,
      "step": 430
    },
    {
      "epoch": 0.5272727272727272,
      "grad_norm": 0.1324293464422226,
      "learning_rate": 2e-05,
      "loss": 0.0979,
      "step": 435
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.13037532567977905,
      "learning_rate": 2e-05,
      "loss": 0.0965,
      "step": 440
    },
    {
      "epoch": 0.5393939393939394,
      "grad_norm": 0.16084375977516174,
      "learning_rate": 2e-05,
      "loss": 0.0926,
      "step": 445
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.1351572573184967,
      "learning_rate": 2e-05,
      "loss": 0.089,
      "step": 450
    },
    {
      "epoch": 0.5515151515151515,
      "grad_norm": 0.13656823337078094,
      "learning_rate": 2e-05,
      "loss": 0.092,
      "step": 455
    },
    {
      "epoch": 0.5575757575757576,
      "grad_norm": 0.13932734727859497,
      "learning_rate": 2e-05,
      "loss": 0.0887,
      "step": 460
    },
    {
      "epoch": 0.5636363636363636,
      "grad_norm": 0.12846994400024414,
      "learning_rate": 2e-05,
      "loss": 0.0884,
      "step": 465
    },
    {
      "epoch": 0.5696969696969697,
      "grad_norm": 0.1407383382320404,
      "learning_rate": 2e-05,
      "loss": 0.0931,
      "step": 470
    },
    {
      "epoch": 0.5757575757575758,
      "grad_norm": 0.1531527191400528,
      "learning_rate": 2e-05,
      "loss": 0.0911,
      "step": 475
    },
    {
      "epoch": 0.5818181818181818,
      "grad_norm": 0.1366247534751892,
      "learning_rate": 2e-05,
      "loss": 0.0918,
      "step": 480
    },
    {
      "epoch": 0.5878787878787879,
      "grad_norm": 0.12227543443441391,
      "learning_rate": 2e-05,
      "loss": 0.089,
      "step": 485
    },
    {
      "epoch": 0.593939393939394,
      "grad_norm": 0.14887017011642456,
      "learning_rate": 2e-05,
      "loss": 0.0909,
      "step": 490
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.1179116889834404,
      "learning_rate": 2e-05,
      "loss": 0.0887,
      "step": 495
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 0.12933525443077087,
      "learning_rate": 2e-05,
      "loss": 0.0928,
      "step": 500
    },
    {
      "epoch": 0.6121212121212121,
      "grad_norm": 0.16795381903648376,
      "learning_rate": 2e-05,
      "loss": 0.0868,
      "step": 505
    },
    {
      "epoch": 0.6181818181818182,
      "grad_norm": 0.1811503767967224,
      "learning_rate": 2e-05,
      "loss": 0.0923,
      "step": 510
    },
    {
      "epoch": 0.6242424242424243,
      "grad_norm": 0.12879465520381927,
      "learning_rate": 2e-05,
      "loss": 0.0881,
      "step": 515
    },
    {
      "epoch": 0.6303030303030303,
      "grad_norm": 0.11549261957406998,
      "learning_rate": 2e-05,
      "loss": 0.0852,
      "step": 520
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 0.12286560982465744,
      "learning_rate": 2e-05,
      "loss": 0.0858,
      "step": 525
    },
    {
      "epoch": 0.6424242424242425,
      "grad_norm": 0.14187730848789215,
      "learning_rate": 2e-05,
      "loss": 0.084,
      "step": 530
    },
    {
      "epoch": 0.6484848484848484,
      "grad_norm": 0.1630314290523529,
      "learning_rate": 2e-05,
      "loss": 0.0865,
      "step": 535
    },
    {
      "epoch": 0.6545454545454545,
      "grad_norm": 0.1549685001373291,
      "learning_rate": 2e-05,
      "loss": 0.084,
      "step": 540
    },
    {
      "epoch": 0.6606060606060606,
      "grad_norm": 0.2114831954240799,
      "learning_rate": 2e-05,
      "loss": 0.092,
      "step": 545
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.14504003524780273,
      "learning_rate": 2e-05,
      "loss": 0.0878,
      "step": 550
    },
    {
      "epoch": 0.6727272727272727,
      "grad_norm": 0.14291881024837494,
      "learning_rate": 2e-05,
      "loss": 0.0902,
      "step": 555
    },
    {
      "epoch": 0.6787878787878788,
      "grad_norm": 0.12386384606361389,
      "learning_rate": 2e-05,
      "loss": 0.0831,
      "step": 560
    },
    {
      "epoch": 0.6848484848484848,
      "grad_norm": 0.1351095288991928,
      "learning_rate": 2e-05,
      "loss": 0.0875,
      "step": 565
    },
    {
      "epoch": 0.6909090909090909,
      "grad_norm": 0.13259272277355194,
      "learning_rate": 2e-05,
      "loss": 0.0837,
      "step": 570
    },
    {
      "epoch": 0.696969696969697,
      "grad_norm": 0.13227032124996185,
      "learning_rate": 2e-05,
      "loss": 0.0846,
      "step": 575
    },
    {
      "epoch": 0.703030303030303,
      "grad_norm": 0.12238829582929611,
      "learning_rate": 2e-05,
      "loss": 0.0844,
      "step": 580
    },
    {
      "epoch": 0.7090909090909091,
      "grad_norm": 0.1287188082933426,
      "learning_rate": 2e-05,
      "loss": 0.0863,
      "step": 585
    },
    {
      "epoch": 0.7151515151515152,
      "grad_norm": 0.12178998440504074,
      "learning_rate": 2e-05,
      "loss": 0.084,
      "step": 590
    },
    {
      "epoch": 0.7212121212121212,
      "grad_norm": 0.12388824671506882,
      "learning_rate": 2e-05,
      "loss": 0.0832,
      "step": 595
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.11086518317461014,
      "learning_rate": 2e-05,
      "loss": 0.0859,
      "step": 600
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.11887922137975693,
      "learning_rate": 2e-05,
      "loss": 0.0803,
      "step": 605
    },
    {
      "epoch": 0.7393939393939394,
      "grad_norm": 0.12907519936561584,
      "learning_rate": 2e-05,
      "loss": 0.0844,
      "step": 610
    },
    {
      "epoch": 0.7454545454545455,
      "grad_norm": 0.10813210159540176,
      "learning_rate": 2e-05,
      "loss": 0.0798,
      "step": 615
    },
    {
      "epoch": 0.7515151515151515,
      "grad_norm": 0.11431533098220825,
      "learning_rate": 2e-05,
      "loss": 0.0804,
      "step": 620
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 0.13838884234428406,
      "learning_rate": 2e-05,
      "loss": 0.0827,
      "step": 625
    },
    {
      "epoch": 0.7636363636363637,
      "grad_norm": 0.13393528759479523,
      "learning_rate": 2e-05,
      "loss": 0.081,
      "step": 630
    },
    {
      "epoch": 0.7696969696969697,
      "grad_norm": 0.11257970333099365,
      "learning_rate": 2e-05,
      "loss": 0.0807,
      "step": 635
    },
    {
      "epoch": 0.7757575757575758,
      "grad_norm": 0.10307560116052628,
      "learning_rate": 2e-05,
      "loss": 0.0788,
      "step": 640
    },
    {
      "epoch": 0.7818181818181819,
      "grad_norm": 0.1161804273724556,
      "learning_rate": 2e-05,
      "loss": 0.0836,
      "step": 645
    },
    {
      "epoch": 0.7878787878787878,
      "grad_norm": 0.10771061480045319,
      "learning_rate": 2e-05,
      "loss": 0.0787,
      "step": 650
    },
    {
      "epoch": 0.793939393939394,
      "grad_norm": 0.11523876339197159,
      "learning_rate": 2e-05,
      "loss": 0.0799,
      "step": 655
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.1186438798904419,
      "learning_rate": 2e-05,
      "loss": 0.0816,
      "step": 660
    },
    {
      "epoch": 0.806060606060606,
      "grad_norm": 0.11634395271539688,
      "learning_rate": 2e-05,
      "loss": 0.079,
      "step": 665
    },
    {
      "epoch": 0.8121212121212121,
      "grad_norm": 0.10538166016340256,
      "learning_rate": 2e-05,
      "loss": 0.0818,
      "step": 670
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 0.15534935891628265,
      "learning_rate": 2e-05,
      "loss": 0.0823,
      "step": 675
    },
    {
      "epoch": 0.8242424242424242,
      "grad_norm": 0.11559600383043289,
      "learning_rate": 2e-05,
      "loss": 0.0794,
      "step": 680
    },
    {
      "epoch": 0.8303030303030303,
      "grad_norm": 0.1222706139087677,
      "learning_rate": 2e-05,
      "loss": 0.0782,
      "step": 685
    },
    {
      "epoch": 0.8363636363636363,
      "grad_norm": 0.11663861572742462,
      "learning_rate": 2e-05,
      "loss": 0.0788,
      "step": 690
    },
    {
      "epoch": 0.8424242424242424,
      "grad_norm": 0.11618614941835403,
      "learning_rate": 2e-05,
      "loss": 0.0798,
      "step": 695
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 0.11376786977052689,
      "learning_rate": 2e-05,
      "loss": 0.0798,
      "step": 700
    },
    {
      "epoch": 0.8545454545454545,
      "grad_norm": 0.1474304348230362,
      "learning_rate": 2e-05,
      "loss": 0.0806,
      "step": 705
    },
    {
      "epoch": 0.8606060606060606,
      "grad_norm": 0.12863756716251373,
      "learning_rate": 2e-05,
      "loss": 0.0768,
      "step": 710
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.10426876693964005,
      "learning_rate": 2e-05,
      "loss": 0.0773,
      "step": 715
    },
    {
      "epoch": 0.8727272727272727,
      "grad_norm": 0.10349389910697937,
      "learning_rate": 2e-05,
      "loss": 0.0783,
      "step": 720
    },
    {
      "epoch": 0.8787878787878788,
      "grad_norm": 0.12173037976026535,
      "learning_rate": 2e-05,
      "loss": 0.0765,
      "step": 725
    },
    {
      "epoch": 0.8848484848484849,
      "grad_norm": 0.10867821425199509,
      "learning_rate": 2e-05,
      "loss": 0.0776,
      "step": 730
    },
    {
      "epoch": 0.8909090909090909,
      "grad_norm": 0.13554811477661133,
      "learning_rate": 2e-05,
      "loss": 0.0753,
      "step": 735
    },
    {
      "epoch": 0.896969696969697,
      "grad_norm": 0.111661396920681,
      "learning_rate": 2e-05,
      "loss": 0.077,
      "step": 740
    },
    {
      "epoch": 0.9030303030303031,
      "grad_norm": 0.11887486279010773,
      "learning_rate": 2e-05,
      "loss": 0.079,
      "step": 745
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.11557162553071976,
      "learning_rate": 2e-05,
      "loss": 0.0751,
      "step": 750
    },
    {
      "epoch": 0.9151515151515152,
      "grad_norm": 0.09950236231088638,
      "learning_rate": 2e-05,
      "loss": 0.0775,
      "step": 755
    },
    {
      "epoch": 0.9212121212121213,
      "grad_norm": 0.10932691395282745,
      "learning_rate": 2e-05,
      "loss": 0.0801,
      "step": 760
    },
    {
      "epoch": 0.9272727272727272,
      "grad_norm": 0.11008119583129883,
      "learning_rate": 2e-05,
      "loss": 0.0766,
      "step": 765
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.10527563840150833,
      "learning_rate": 2e-05,
      "loss": 0.0769,
      "step": 770
    },
    {
      "epoch": 0.9393939393939394,
      "grad_norm": 0.1048276275396347,
      "learning_rate": 2e-05,
      "loss": 0.0714,
      "step": 775
    },
    {
      "epoch": 0.9454545454545454,
      "grad_norm": 0.11235509067773819,
      "learning_rate": 2e-05,
      "loss": 0.074,
      "step": 780
    },
    {
      "epoch": 0.9515151515151515,
      "grad_norm": 0.11550861597061157,
      "learning_rate": 2e-05,
      "loss": 0.076,
      "step": 785
    },
    {
      "epoch": 0.9575757575757575,
      "grad_norm": 0.11406799405813217,
      "learning_rate": 2e-05,
      "loss": 0.0777,
      "step": 790
    },
    {
      "epoch": 0.9636363636363636,
      "grad_norm": 0.13980014622211456,
      "learning_rate": 2e-05,
      "loss": 0.0749,
      "step": 795
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 0.12040958553552628,
      "learning_rate": 2e-05,
      "loss": 0.0766,
      "step": 800
    },
    {
      "epoch": 0.9757575757575757,
      "grad_norm": 0.12771187722682953,
      "learning_rate": 2e-05,
      "loss": 0.0742,
      "step": 805
    },
    {
      "epoch": 0.9818181818181818,
      "grad_norm": 0.1272190660238266,
      "learning_rate": 2e-05,
      "loss": 0.0729,
      "step": 810
    },
    {
      "epoch": 0.9878787878787879,
      "grad_norm": 0.09860717505216599,
      "learning_rate": 2e-05,
      "loss": 0.0698,
      "step": 815
    },
    {
      "epoch": 0.9939393939393939,
      "grad_norm": 0.15016545355319977,
      "learning_rate": 2e-05,
      "loss": 0.0764,
      "step": 820
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.11473816633224487,
      "learning_rate": 2e-05,
      "loss": 0.078,
      "step": 825
    },
    {
      "epoch": 1.0,
      "eval_average": 0.34405346585443725,
      "eval_crossner_ai": 0.3119469026059299,
      "eval_crossner_literature": 0.4380341879842999,
      "eval_crossner_music": 0.4206896551223672,
      "eval_crossner_politics": 0.49924812025104415,
      "eval_crossner_science": 0.47424684154447394,
      "eval_mit-movie": 0.13609467450827703,
      "eval_mit-restaurant": 0.12811387896466864,
      "eval_runtime": 37.6361,
      "eval_samples_per_second": 18.599,
      "eval_steps_per_second": 0.186,
      "step": 825
    },
    {
      "epoch": 1.006060606060606,
      "grad_norm": 0.12253543734550476,
      "learning_rate": 2e-05,
      "loss": 0.0672,
      "step": 830
    },
    {
      "epoch": 1.0121212121212122,
      "grad_norm": 0.12306667864322662,
      "learning_rate": 2e-05,
      "loss": 0.0716,
      "step": 835
    },
    {
      "epoch": 1.018181818181818,
      "grad_norm": 0.10358674079179764,
      "learning_rate": 2e-05,
      "loss": 0.0708,
      "step": 840
    },
    {
      "epoch": 1.0242424242424242,
      "grad_norm": 0.1314278393983841,
      "learning_rate": 2e-05,
      "loss": 0.0746,
      "step": 845
    },
    {
      "epoch": 1.0303030303030303,
      "grad_norm": 0.11669009178876877,
      "learning_rate": 2e-05,
      "loss": 0.0755,
      "step": 850
    },
    {
      "epoch": 1.0363636363636364,
      "grad_norm": 0.09572337567806244,
      "learning_rate": 2e-05,
      "loss": 0.0741,
      "step": 855
    },
    {
      "epoch": 1.0424242424242425,
      "grad_norm": 0.12921887636184692,
      "learning_rate": 2e-05,
      "loss": 0.0708,
      "step": 860
    },
    {
      "epoch": 1.0484848484848486,
      "grad_norm": 0.1293361634016037,
      "learning_rate": 2e-05,
      "loss": 0.0714,
      "step": 865
    },
    {
      "epoch": 1.0545454545454545,
      "grad_norm": 0.10471057146787643,
      "learning_rate": 2e-05,
      "loss": 0.0754,
      "step": 870
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 0.10683651268482208,
      "learning_rate": 2e-05,
      "loss": 0.0718,
      "step": 875
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.10217452049255371,
      "learning_rate": 2e-05,
      "loss": 0.0716,
      "step": 880
    },
    {
      "epoch": 1.0727272727272728,
      "grad_norm": 0.13816019892692566,
      "learning_rate": 2e-05,
      "loss": 0.0722,
      "step": 885
    },
    {
      "epoch": 1.0787878787878789,
      "grad_norm": 0.11558044701814651,
      "learning_rate": 2e-05,
      "loss": 0.0738,
      "step": 890
    },
    {
      "epoch": 1.084848484848485,
      "grad_norm": 0.12974366545677185,
      "learning_rate": 2e-05,
      "loss": 0.0702,
      "step": 895
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 0.13629621267318726,
      "learning_rate": 2e-05,
      "loss": 0.0701,
      "step": 900
    },
    {
      "epoch": 1.096969696969697,
      "grad_norm": 0.113337941467762,
      "learning_rate": 2e-05,
      "loss": 0.0706,
      "step": 905
    },
    {
      "epoch": 1.103030303030303,
      "grad_norm": 0.11302217096090317,
      "learning_rate": 2e-05,
      "loss": 0.0709,
      "step": 910
    },
    {
      "epoch": 1.1090909090909091,
      "grad_norm": 0.12805776298046112,
      "learning_rate": 2e-05,
      "loss": 0.0732,
      "step": 915
    },
    {
      "epoch": 1.1151515151515152,
      "grad_norm": 0.1129978746175766,
      "learning_rate": 2e-05,
      "loss": 0.0706,
      "step": 920
    },
    {
      "epoch": 1.121212121212121,
      "grad_norm": 0.10619095712900162,
      "learning_rate": 2e-05,
      "loss": 0.0723,
      "step": 925
    },
    {
      "epoch": 1.1272727272727272,
      "grad_norm": 0.12212296575307846,
      "learning_rate": 2e-05,
      "loss": 0.0723,
      "step": 930
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.1095682680606842,
      "learning_rate": 2e-05,
      "loss": 0.074,
      "step": 935
    },
    {
      "epoch": 1.1393939393939394,
      "grad_norm": 0.11696702241897583,
      "learning_rate": 2e-05,
      "loss": 0.0691,
      "step": 940
    },
    {
      "epoch": 1.1454545454545455,
      "grad_norm": 0.09973741322755814,
      "learning_rate": 2e-05,
      "loss": 0.0713,
      "step": 945
    },
    {
      "epoch": 1.1515151515151516,
      "grad_norm": 0.12478262186050415,
      "learning_rate": 2e-05,
      "loss": 0.0697,
      "step": 950
    },
    {
      "epoch": 1.1575757575757575,
      "grad_norm": 0.09891806542873383,
      "learning_rate": 2e-05,
      "loss": 0.0684,
      "step": 955
    },
    {
      "epoch": 1.1636363636363636,
      "grad_norm": 0.12180036306381226,
      "learning_rate": 2e-05,
      "loss": 0.0719,
      "step": 960
    },
    {
      "epoch": 1.1696969696969697,
      "grad_norm": 0.1005549430847168,
      "learning_rate": 2e-05,
      "loss": 0.0692,
      "step": 965
    },
    {
      "epoch": 1.1757575757575758,
      "grad_norm": 0.10224059224128723,
      "learning_rate": 2e-05,
      "loss": 0.0674,
      "step": 970
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 0.09907817840576172,
      "learning_rate": 2e-05,
      "loss": 0.0686,
      "step": 975
    },
    {
      "epoch": 1.187878787878788,
      "grad_norm": 0.10422774404287338,
      "learning_rate": 2e-05,
      "loss": 0.0699,
      "step": 980
    },
    {
      "epoch": 1.1939393939393939,
      "grad_norm": 0.14453040063381195,
      "learning_rate": 2e-05,
      "loss": 0.0706,
      "step": 985
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.08794312924146652,
      "learning_rate": 2e-05,
      "loss": 0.0699,
      "step": 990
    },
    {
      "epoch": 1.206060606060606,
      "grad_norm": 0.12066875398159027,
      "learning_rate": 2e-05,
      "loss": 0.067,
      "step": 995
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 0.10107310116291046,
      "learning_rate": 2e-05,
      "loss": 0.068,
      "step": 1000
    },
    {
      "epoch": 1.2181818181818183,
      "grad_norm": 0.09312600642442703,
      "learning_rate": 2e-05,
      "loss": 0.0671,
      "step": 1005
    },
    {
      "epoch": 1.2242424242424241,
      "grad_norm": 0.09719859808683395,
      "learning_rate": 2e-05,
      "loss": 0.0696,
      "step": 1010
    },
    {
      "epoch": 1.2303030303030302,
      "grad_norm": 0.11095621436834335,
      "learning_rate": 2e-05,
      "loss": 0.0703,
      "step": 1015
    },
    {
      "epoch": 1.2363636363636363,
      "grad_norm": 0.10493703931570053,
      "learning_rate": 2e-05,
      "loss": 0.0667,
      "step": 1020
    },
    {
      "epoch": 1.2424242424242424,
      "grad_norm": 0.098637156188488,
      "learning_rate": 2e-05,
      "loss": 0.0716,
      "step": 1025
    },
    {
      "epoch": 1.2484848484848485,
      "grad_norm": 0.10044530034065247,
      "learning_rate": 2e-05,
      "loss": 0.0686,
      "step": 1030
    },
    {
      "epoch": 1.2545454545454546,
      "grad_norm": 0.10532357543706894,
      "learning_rate": 2e-05,
      "loss": 0.0671,
      "step": 1035
    },
    {
      "epoch": 1.2606060606060605,
      "grad_norm": 0.09209601581096649,
      "learning_rate": 2e-05,
      "loss": 0.0721,
      "step": 1040
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.0928976759314537,
      "learning_rate": 2e-05,
      "loss": 0.0674,
      "step": 1045
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 0.16801901161670685,
      "learning_rate": 2e-05,
      "loss": 0.0686,
      "step": 1050
    },
    {
      "epoch": 1.2787878787878788,
      "grad_norm": 0.10385696589946747,
      "learning_rate": 2e-05,
      "loss": 0.065,
      "step": 1055
    },
    {
      "epoch": 1.284848484848485,
      "grad_norm": 0.09447010606527328,
      "learning_rate": 2e-05,
      "loss": 0.0694,
      "step": 1060
    },
    {
      "epoch": 1.290909090909091,
      "grad_norm": 0.10375440865755081,
      "learning_rate": 2e-05,
      "loss": 0.0682,
      "step": 1065
    },
    {
      "epoch": 1.2969696969696969,
      "grad_norm": 0.10327611863613129,
      "learning_rate": 2e-05,
      "loss": 0.0691,
      "step": 1070
    },
    {
      "epoch": 1.303030303030303,
      "grad_norm": 0.10580986738204956,
      "learning_rate": 2e-05,
      "loss": 0.068,
      "step": 1075
    },
    {
      "epoch": 1.309090909090909,
      "grad_norm": 0.10319223254919052,
      "learning_rate": 2e-05,
      "loss": 0.0667,
      "step": 1080
    },
    {
      "epoch": 1.3151515151515152,
      "grad_norm": 0.10483373701572418,
      "learning_rate": 2e-05,
      "loss": 0.0664,
      "step": 1085
    },
    {
      "epoch": 1.3212121212121213,
      "grad_norm": 0.13225941359996796,
      "learning_rate": 2e-05,
      "loss": 0.0688,
      "step": 1090
    },
    {
      "epoch": 1.3272727272727274,
      "grad_norm": 0.1208401545882225,
      "learning_rate": 2e-05,
      "loss": 0.0669,
      "step": 1095
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.10545514523983002,
      "learning_rate": 2e-05,
      "loss": 0.0698,
      "step": 1100
    },
    {
      "epoch": 1.3393939393939394,
      "grad_norm": 0.09716565907001495,
      "learning_rate": 2e-05,
      "loss": 0.0657,
      "step": 1105
    },
    {
      "epoch": 1.3454545454545455,
      "grad_norm": 0.1022256389260292,
      "learning_rate": 2e-05,
      "loss": 0.0657,
      "step": 1110
    },
    {
      "epoch": 1.3515151515151516,
      "grad_norm": 0.1597536951303482,
      "learning_rate": 2e-05,
      "loss": 0.0697,
      "step": 1115
    },
    {
      "epoch": 1.3575757575757577,
      "grad_norm": 0.09823883324861526,
      "learning_rate": 2e-05,
      "loss": 0.0697,
      "step": 1120
    },
    {
      "epoch": 1.3636363636363635,
      "grad_norm": 0.1072874367237091,
      "learning_rate": 2e-05,
      "loss": 0.0634,
      "step": 1125
    },
    {
      "epoch": 1.3696969696969696,
      "grad_norm": 0.11723940819501877,
      "learning_rate": 2e-05,
      "loss": 0.068,
      "step": 1130
    },
    {
      "epoch": 1.3757575757575757,
      "grad_norm": 0.11068478971719742,
      "learning_rate": 2e-05,
      "loss": 0.0679,
      "step": 1135
    },
    {
      "epoch": 1.3818181818181818,
      "grad_norm": 0.10673754662275314,
      "learning_rate": 2e-05,
      "loss": 0.0667,
      "step": 1140
    },
    {
      "epoch": 1.387878787878788,
      "grad_norm": 0.09531150758266449,
      "learning_rate": 2e-05,
      "loss": 0.0665,
      "step": 1145
    },
    {
      "epoch": 1.393939393939394,
      "grad_norm": 0.10757917165756226,
      "learning_rate": 2e-05,
      "loss": 0.068,
      "step": 1150
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.10809773951768875,
      "learning_rate": 2e-05,
      "loss": 0.0657,
      "step": 1155
    },
    {
      "epoch": 1.406060606060606,
      "grad_norm": 0.1037774458527565,
      "learning_rate": 2e-05,
      "loss": 0.0696,
      "step": 1160
    },
    {
      "epoch": 1.412121212121212,
      "grad_norm": 0.1168590784072876,
      "learning_rate": 2e-05,
      "loss": 0.0673,
      "step": 1165
    },
    {
      "epoch": 1.4181818181818182,
      "grad_norm": 0.09578242897987366,
      "learning_rate": 2e-05,
      "loss": 0.0649,
      "step": 1170
    },
    {
      "epoch": 1.4242424242424243,
      "grad_norm": 0.08864474296569824,
      "learning_rate": 2e-05,
      "loss": 0.0651,
      "step": 1175
    },
    {
      "epoch": 1.4303030303030304,
      "grad_norm": 0.09808044135570526,
      "learning_rate": 2e-05,
      "loss": 0.0659,
      "step": 1180
    },
    {
      "epoch": 1.4363636363636363,
      "grad_norm": 0.08862804621458054,
      "learning_rate": 2e-05,
      "loss": 0.0656,
      "step": 1185
    },
    {
      "epoch": 1.4424242424242424,
      "grad_norm": 0.09245201200246811,
      "learning_rate": 2e-05,
      "loss": 0.063,
      "step": 1190
    },
    {
      "epoch": 1.4484848484848485,
      "grad_norm": 0.10353873670101166,
      "learning_rate": 2e-05,
      "loss": 0.0641,
      "step": 1195
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 0.13441655039787292,
      "learning_rate": 2e-05,
      "loss": 0.07,
      "step": 1200
    },
    {
      "epoch": 1.4606060606060607,
      "grad_norm": 0.11727897822856903,
      "learning_rate": 2e-05,
      "loss": 0.0643,
      "step": 1205
    },
    {
      "epoch": 1.4666666666666666,
      "grad_norm": 0.11497071385383606,
      "learning_rate": 2e-05,
      "loss": 0.0619,
      "step": 1210
    },
    {
      "epoch": 1.4727272727272727,
      "grad_norm": 0.08833365142345428,
      "learning_rate": 2e-05,
      "loss": 0.0629,
      "step": 1215
    },
    {
      "epoch": 1.4787878787878788,
      "grad_norm": 0.09657435864210129,
      "learning_rate": 2e-05,
      "loss": 0.0623,
      "step": 1220
    },
    {
      "epoch": 1.4848484848484849,
      "grad_norm": 0.11252450942993164,
      "learning_rate": 2e-05,
      "loss": 0.063,
      "step": 1225
    },
    {
      "epoch": 1.490909090909091,
      "grad_norm": 0.08266250044107437,
      "learning_rate": 2e-05,
      "loss": 0.0648,
      "step": 1230
    },
    {
      "epoch": 1.496969696969697,
      "grad_norm": 0.12204933911561966,
      "learning_rate": 2e-05,
      "loss": 0.066,
      "step": 1235
    },
    {
      "epoch": 1.5006060606060605,
      "eval_average": 0.3788242332986798,
      "eval_crossner_ai": 0.3714285713797072,
      "eval_crossner_literature": 0.4836601306689536,
      "eval_crossner_music": 0.4819112627486009,
      "eval_crossner_politics": 0.5267175572020854,
      "eval_crossner_science": 0.4829600778474573,
      "eval_mit-movie": 0.15976331356151746,
      "eval_mit-restaurant": 0.14532871968243677,
      "eval_runtime": 37.3907,
      "eval_samples_per_second": 18.721,
      "eval_steps_per_second": 0.187,
      "step": 1238
    },
    {
      "epoch": 1.503030303030303,
      "grad_norm": 0.10289295762777328,
      "learning_rate": 2e-05,
      "loss": 0.0635,
      "step": 1240
    },
    {
      "epoch": 1.509090909090909,
      "grad_norm": 0.09419567883014679,
      "learning_rate": 2e-05,
      "loss": 0.0659,
      "step": 1245
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 0.12625913321971893,
      "learning_rate": 2e-05,
      "loss": 0.0648,
      "step": 1250
    },
    {
      "epoch": 1.5212121212121212,
      "grad_norm": 0.12631703913211823,
      "learning_rate": 2e-05,
      "loss": 0.0641,
      "step": 1255
    },
    {
      "epoch": 1.5272727272727273,
      "grad_norm": 0.09279792010784149,
      "learning_rate": 2e-05,
      "loss": 0.0614,
      "step": 1260
    },
    {
      "epoch": 1.5333333333333334,
      "grad_norm": 0.09155553579330444,
      "learning_rate": 2e-05,
      "loss": 0.0633,
      "step": 1265
    },
    {
      "epoch": 1.5393939393939393,
      "grad_norm": 0.09768284112215042,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 1270
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 0.09276561439037323,
      "learning_rate": 2e-05,
      "loss": 0.0643,
      "step": 1275
    },
    {
      "epoch": 1.5515151515151515,
      "grad_norm": 0.09534168243408203,
      "learning_rate": 2e-05,
      "loss": 0.0625,
      "step": 1280
    },
    {
      "epoch": 1.5575757575757576,
      "grad_norm": 0.1643574833869934,
      "learning_rate": 2e-05,
      "loss": 0.0655,
      "step": 1285
    },
    {
      "epoch": 1.5636363636363637,
      "grad_norm": 0.0932525023818016,
      "learning_rate": 2e-05,
      "loss": 0.0642,
      "step": 1290
    },
    {
      "epoch": 1.5696969696969696,
      "grad_norm": 0.09728793054819107,
      "learning_rate": 2e-05,
      "loss": 0.0607,
      "step": 1295
    },
    {
      "epoch": 1.5757575757575757,
      "grad_norm": 0.08447481691837311,
      "learning_rate": 2e-05,
      "loss": 0.0628,
      "step": 1300
    },
    {
      "epoch": 1.5818181818181818,
      "grad_norm": 0.08566232770681381,
      "learning_rate": 2e-05,
      "loss": 0.0622,
      "step": 1305
    },
    {
      "epoch": 1.587878787878788,
      "grad_norm": 0.09752994030714035,
      "learning_rate": 2e-05,
      "loss": 0.0638,
      "step": 1310
    },
    {
      "epoch": 1.593939393939394,
      "grad_norm": 0.0952834039926529,
      "learning_rate": 2e-05,
      "loss": 0.0642,
      "step": 1315
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.10499224811792374,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 1320
    },
    {
      "epoch": 1.606060606060606,
      "grad_norm": 0.12823627889156342,
      "learning_rate": 2e-05,
      "loss": 0.0623,
      "step": 1325
    },
    {
      "epoch": 1.612121212121212,
      "grad_norm": 0.0972772166132927,
      "learning_rate": 2e-05,
      "loss": 0.064,
      "step": 1330
    },
    {
      "epoch": 1.6181818181818182,
      "grad_norm": 0.08541806042194366,
      "learning_rate": 2e-05,
      "loss": 0.0615,
      "step": 1335
    },
    {
      "epoch": 1.6242424242424243,
      "grad_norm": 0.09506454318761826,
      "learning_rate": 2e-05,
      "loss": 0.0641,
      "step": 1340
    },
    {
      "epoch": 1.6303030303030304,
      "grad_norm": 0.11696548759937286,
      "learning_rate": 2e-05,
      "loss": 0.0674,
      "step": 1345
    },
    {
      "epoch": 1.6363636363636365,
      "grad_norm": 0.11918334662914276,
      "learning_rate": 2e-05,
      "loss": 0.0622,
      "step": 1350
    },
    {
      "epoch": 1.6424242424242423,
      "grad_norm": 0.1053888350725174,
      "learning_rate": 2e-05,
      "loss": 0.0621,
      "step": 1355
    },
    {
      "epoch": 1.6484848484848484,
      "grad_norm": 0.0871436595916748,
      "learning_rate": 2e-05,
      "loss": 0.0622,
      "step": 1360
    },
    {
      "epoch": 1.6545454545454545,
      "grad_norm": 0.10665052384138107,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 1365
    },
    {
      "epoch": 1.6606060606060606,
      "grad_norm": 0.09176793694496155,
      "learning_rate": 2e-05,
      "loss": 0.062,
      "step": 1370
    },
    {
      "epoch": 1.6666666666666667,
      "grad_norm": 0.08321824669837952,
      "learning_rate": 2e-05,
      "loss": 0.0634,
      "step": 1375
    },
    {
      "epoch": 1.6727272727272726,
      "grad_norm": 0.09972991049289703,
      "learning_rate": 2e-05,
      "loss": 0.0635,
      "step": 1380
    },
    {
      "epoch": 1.6787878787878787,
      "grad_norm": 0.15259890258312225,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 1385
    },
    {
      "epoch": 1.6848484848484848,
      "grad_norm": 0.09284790605306625,
      "learning_rate": 2e-05,
      "loss": 0.0599,
      "step": 1390
    },
    {
      "epoch": 1.690909090909091,
      "grad_norm": 0.10297628492116928,
      "learning_rate": 2e-05,
      "loss": 0.0635,
      "step": 1395
    },
    {
      "epoch": 1.696969696969697,
      "grad_norm": 0.0987033024430275,
      "learning_rate": 2e-05,
      "loss": 0.0619,
      "step": 1400
    },
    {
      "epoch": 1.7030303030303031,
      "grad_norm": 0.09456738829612732,
      "learning_rate": 2e-05,
      "loss": 0.0584,
      "step": 1405
    },
    {
      "epoch": 1.709090909090909,
      "grad_norm": 0.09003407508134842,
      "learning_rate": 2e-05,
      "loss": 0.0602,
      "step": 1410
    },
    {
      "epoch": 1.715151515151515,
      "grad_norm": 0.08077117800712585,
      "learning_rate": 2e-05,
      "loss": 0.0594,
      "step": 1415
    },
    {
      "epoch": 1.7212121212121212,
      "grad_norm": 0.0737229585647583,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 1420
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.09734409302473068,
      "learning_rate": 2e-05,
      "loss": 0.0624,
      "step": 1425
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.10400369018316269,
      "learning_rate": 2e-05,
      "loss": 0.0607,
      "step": 1430
    },
    {
      "epoch": 1.7393939393939395,
      "grad_norm": 0.1011214628815651,
      "learning_rate": 2e-05,
      "loss": 0.0621,
      "step": 1435
    },
    {
      "epoch": 1.7454545454545454,
      "grad_norm": 0.09769021719694138,
      "learning_rate": 2e-05,
      "loss": 0.0624,
      "step": 1440
    },
    {
      "epoch": 1.7515151515151515,
      "grad_norm": 0.11158230900764465,
      "learning_rate": 2e-05,
      "loss": 0.0614,
      "step": 1445
    },
    {
      "epoch": 1.7575757575757576,
      "grad_norm": 0.11446844041347504,
      "learning_rate": 2e-05,
      "loss": 0.058,
      "step": 1450
    },
    {
      "epoch": 1.7636363636363637,
      "grad_norm": 0.1096675917506218,
      "learning_rate": 2e-05,
      "loss": 0.0619,
      "step": 1455
    },
    {
      "epoch": 1.7696969696969698,
      "grad_norm": 0.08933830261230469,
      "learning_rate": 2e-05,
      "loss": 0.0621,
      "step": 1460
    },
    {
      "epoch": 1.7757575757575759,
      "grad_norm": 0.09324691444635391,
      "learning_rate": 2e-05,
      "loss": 0.0603,
      "step": 1465
    },
    {
      "epoch": 1.7818181818181817,
      "grad_norm": 0.08502551913261414,
      "learning_rate": 2e-05,
      "loss": 0.0611,
      "step": 1470
    },
    {
      "epoch": 1.7878787878787878,
      "grad_norm": 0.09008023887872696,
      "learning_rate": 2e-05,
      "loss": 0.0575,
      "step": 1475
    },
    {
      "epoch": 1.793939393939394,
      "grad_norm": 0.09035488218069077,
      "learning_rate": 2e-05,
      "loss": 0.059,
      "step": 1480
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.11020947247743607,
      "learning_rate": 2e-05,
      "loss": 0.0582,
      "step": 1485
    },
    {
      "epoch": 1.8060606060606061,
      "grad_norm": 0.09203866124153137,
      "learning_rate": 2e-05,
      "loss": 0.0604,
      "step": 1490
    },
    {
      "epoch": 1.812121212121212,
      "grad_norm": 0.08437725901603699,
      "learning_rate": 2e-05,
      "loss": 0.0629,
      "step": 1495
    },
    {
      "epoch": 1.8181818181818181,
      "grad_norm": 0.11370174586772919,
      "learning_rate": 2e-05,
      "loss": 0.0601,
      "step": 1500
    },
    {
      "epoch": 1.8242424242424242,
      "grad_norm": 0.09544355422258377,
      "learning_rate": 2e-05,
      "loss": 0.0555,
      "step": 1505
    },
    {
      "epoch": 1.8303030303030303,
      "grad_norm": 0.08889370411634445,
      "learning_rate": 2e-05,
      "loss": 0.0603,
      "step": 1510
    },
    {
      "epoch": 1.8363636363636364,
      "grad_norm": 0.10728336870670319,
      "learning_rate": 2e-05,
      "loss": 0.0612,
      "step": 1515
    },
    {
      "epoch": 1.8424242424242425,
      "grad_norm": 0.0853598415851593,
      "learning_rate": 2e-05,
      "loss": 0.0616,
      "step": 1520
    },
    {
      "epoch": 1.8484848484848484,
      "grad_norm": 0.09412270784378052,
      "learning_rate": 2e-05,
      "loss": 0.0611,
      "step": 1525
    },
    {
      "epoch": 1.8545454545454545,
      "grad_norm": 0.11459766328334808,
      "learning_rate": 2e-05,
      "loss": 0.0635,
      "step": 1530
    },
    {
      "epoch": 1.8606060606060606,
      "grad_norm": 0.09458527714014053,
      "learning_rate": 2e-05,
      "loss": 0.0613,
      "step": 1535
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.09504768252372742,
      "learning_rate": 2e-05,
      "loss": 0.0607,
      "step": 1540
    },
    {
      "epoch": 1.8727272727272728,
      "grad_norm": 0.09514124691486359,
      "learning_rate": 2e-05,
      "loss": 0.0598,
      "step": 1545
    },
    {
      "epoch": 1.878787878787879,
      "grad_norm": 0.1047569066286087,
      "learning_rate": 2e-05,
      "loss": 0.0584,
      "step": 1550
    },
    {
      "epoch": 1.8848484848484848,
      "grad_norm": 0.08832447975873947,
      "learning_rate": 2e-05,
      "loss": 0.0615,
      "step": 1555
    },
    {
      "epoch": 1.8909090909090909,
      "grad_norm": 0.0950426384806633,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 1560
    },
    {
      "epoch": 1.896969696969697,
      "grad_norm": 0.12014970928430557,
      "learning_rate": 2e-05,
      "loss": 0.0608,
      "step": 1565
    },
    {
      "epoch": 1.903030303030303,
      "grad_norm": 0.09588909894227982,
      "learning_rate": 2e-05,
      "loss": 0.0613,
      "step": 1570
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 0.09555643051862717,
      "learning_rate": 2e-05,
      "loss": 0.058,
      "step": 1575
    },
    {
      "epoch": 1.915151515151515,
      "grad_norm": 0.08480541408061981,
      "learning_rate": 2e-05,
      "loss": 0.0606,
      "step": 1580
    },
    {
      "epoch": 1.9212121212121211,
      "grad_norm": 0.07799738645553589,
      "learning_rate": 2e-05,
      "loss": 0.0595,
      "step": 1585
    },
    {
      "epoch": 1.9272727272727272,
      "grad_norm": 0.09509621560573578,
      "learning_rate": 2e-05,
      "loss": 0.0611,
      "step": 1590
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.08786647766828537,
      "learning_rate": 2e-05,
      "loss": 0.0623,
      "step": 1595
    },
    {
      "epoch": 1.9393939393939394,
      "grad_norm": 0.12229779362678528,
      "learning_rate": 2e-05,
      "loss": 0.0591,
      "step": 1600
    },
    {
      "epoch": 1.9454545454545455,
      "grad_norm": 0.118565633893013,
      "learning_rate": 2e-05,
      "loss": 0.0589,
      "step": 1605
    },
    {
      "epoch": 1.9515151515151514,
      "grad_norm": 0.12439098209142685,
      "learning_rate": 2e-05,
      "loss": 0.0609,
      "step": 1610
    },
    {
      "epoch": 1.9575757575757575,
      "grad_norm": 0.08985460549592972,
      "learning_rate": 2e-05,
      "loss": 0.0594,
      "step": 1615
    },
    {
      "epoch": 1.9636363636363636,
      "grad_norm": 0.09046149998903275,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 1620
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 0.12047301977872849,
      "learning_rate": 2e-05,
      "loss": 0.0586,
      "step": 1625
    },
    {
      "epoch": 1.9757575757575758,
      "grad_norm": 0.08493439853191376,
      "learning_rate": 2e-05,
      "loss": 0.0584,
      "step": 1630
    },
    {
      "epoch": 1.981818181818182,
      "grad_norm": 0.08824591338634491,
      "learning_rate": 2e-05,
      "loss": 0.0574,
      "step": 1635
    },
    {
      "epoch": 1.9878787878787878,
      "grad_norm": 0.21161822974681854,
      "learning_rate": 2e-05,
      "loss": 0.0617,
      "step": 1640
    },
    {
      "epoch": 1.993939393939394,
      "grad_norm": 0.08468802273273468,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 1645
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.08454450964927673,
      "learning_rate": 2e-05,
      "loss": 0.057,
      "step": 1650
    },
    {
      "epoch": 2.0,
      "eval_average": 0.4018467454451038,
      "eval_crossner_ai": 0.3687635574350459,
      "eval_crossner_literature": 0.5060240963355135,
      "eval_crossner_music": 0.519283746506416,
      "eval_crossner_politics": 0.5292307691808966,
      "eval_crossner_science": 0.5132223309985939,
      "eval_mit-movie": 0.18911174780232348,
      "eval_mit-restaurant": 0.18729096985693672,
      "eval_runtime": 37.345,
      "eval_samples_per_second": 18.744,
      "eval_steps_per_second": 0.187,
      "step": 1650
    },
    {
      "epoch": 2.006060606060606,
      "grad_norm": 0.08293530344963074,
      "learning_rate": 2e-05,
      "loss": 0.052,
      "step": 1655
    },
    {
      "epoch": 2.012121212121212,
      "grad_norm": 0.10082875937223434,
      "learning_rate": 2e-05,
      "loss": 0.0559,
      "step": 1660
    },
    {
      "epoch": 2.018181818181818,
      "grad_norm": 0.08649992942810059,
      "learning_rate": 2e-05,
      "loss": 0.0586,
      "step": 1665
    },
    {
      "epoch": 2.0242424242424244,
      "grad_norm": 0.09639701247215271,
      "learning_rate": 2e-05,
      "loss": 0.0565,
      "step": 1670
    },
    {
      "epoch": 2.0303030303030303,
      "grad_norm": 0.09376944601535797,
      "learning_rate": 2e-05,
      "loss": 0.0578,
      "step": 1675
    },
    {
      "epoch": 2.036363636363636,
      "grad_norm": 0.11191318184137344,
      "learning_rate": 2e-05,
      "loss": 0.0598,
      "step": 1680
    },
    {
      "epoch": 2.0424242424242425,
      "grad_norm": 0.10058274120092392,
      "learning_rate": 2e-05,
      "loss": 0.057,
      "step": 1685
    },
    {
      "epoch": 2.0484848484848484,
      "grad_norm": 0.09023085236549377,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 1690
    },
    {
      "epoch": 2.0545454545454547,
      "grad_norm": 0.07463552057743073,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 1695
    },
    {
      "epoch": 2.0606060606060606,
      "grad_norm": 0.08651968091726303,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 1700
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.0830967053771019,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 1705
    },
    {
      "epoch": 2.0727272727272728,
      "grad_norm": 0.08277024328708649,
      "learning_rate": 2e-05,
      "loss": 0.0592,
      "step": 1710
    },
    {
      "epoch": 2.0787878787878786,
      "grad_norm": 0.1154651939868927,
      "learning_rate": 2e-05,
      "loss": 0.0567,
      "step": 1715
    },
    {
      "epoch": 2.084848484848485,
      "grad_norm": 0.08568475395441055,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 1720
    },
    {
      "epoch": 2.090909090909091,
      "grad_norm": 0.09125778824090958,
      "learning_rate": 2e-05,
      "loss": 0.0572,
      "step": 1725
    },
    {
      "epoch": 2.096969696969697,
      "grad_norm": 0.08005303889513016,
      "learning_rate": 2e-05,
      "loss": 0.0572,
      "step": 1730
    },
    {
      "epoch": 2.103030303030303,
      "grad_norm": 0.11653123050928116,
      "learning_rate": 2e-05,
      "loss": 0.0579,
      "step": 1735
    },
    {
      "epoch": 2.109090909090909,
      "grad_norm": 0.09451491385698318,
      "learning_rate": 2e-05,
      "loss": 0.0586,
      "step": 1740
    },
    {
      "epoch": 2.1151515151515152,
      "grad_norm": 0.07427140325307846,
      "learning_rate": 2e-05,
      "loss": 0.057,
      "step": 1745
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 0.08032707124948502,
      "learning_rate": 2e-05,
      "loss": 0.0528,
      "step": 1750
    },
    {
      "epoch": 2.1272727272727274,
      "grad_norm": 0.09421360492706299,
      "learning_rate": 2e-05,
      "loss": 0.0562,
      "step": 1755
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.0782245621085167,
      "learning_rate": 2e-05,
      "loss": 0.0562,
      "step": 1760
    },
    {
      "epoch": 2.139393939393939,
      "grad_norm": 0.11334434151649475,
      "learning_rate": 2e-05,
      "loss": 0.0577,
      "step": 1765
    },
    {
      "epoch": 2.1454545454545455,
      "grad_norm": 0.14566591382026672,
      "learning_rate": 2e-05,
      "loss": 0.0582,
      "step": 1770
    },
    {
      "epoch": 2.1515151515151514,
      "grad_norm": 0.10331124067306519,
      "learning_rate": 2e-05,
      "loss": 0.0568,
      "step": 1775
    },
    {
      "epoch": 2.1575757575757577,
      "grad_norm": 0.10763604938983917,
      "learning_rate": 2e-05,
      "loss": 0.0597,
      "step": 1780
    },
    {
      "epoch": 2.1636363636363636,
      "grad_norm": 0.08148172497749329,
      "learning_rate": 2e-05,
      "loss": 0.0554,
      "step": 1785
    },
    {
      "epoch": 2.16969696969697,
      "grad_norm": 0.11597803235054016,
      "learning_rate": 2e-05,
      "loss": 0.0622,
      "step": 1790
    },
    {
      "epoch": 2.175757575757576,
      "grad_norm": 0.10236319899559021,
      "learning_rate": 2e-05,
      "loss": 0.0567,
      "step": 1795
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 0.09480895847082138,
      "learning_rate": 2e-05,
      "loss": 0.0554,
      "step": 1800
    },
    {
      "epoch": 2.187878787878788,
      "grad_norm": 0.09424911439418793,
      "learning_rate": 2e-05,
      "loss": 0.0574,
      "step": 1805
    },
    {
      "epoch": 2.193939393939394,
      "grad_norm": 0.11717522144317627,
      "learning_rate": 2e-05,
      "loss": 0.0575,
      "step": 1810
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.08012079447507858,
      "learning_rate": 2e-05,
      "loss": 0.0571,
      "step": 1815
    },
    {
      "epoch": 2.206060606060606,
      "grad_norm": 0.09216365963220596,
      "learning_rate": 2e-05,
      "loss": 0.056,
      "step": 1820
    },
    {
      "epoch": 2.212121212121212,
      "grad_norm": 0.09722694754600525,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 1825
    },
    {
      "epoch": 2.2181818181818183,
      "grad_norm": 0.09954019635915756,
      "learning_rate": 2e-05,
      "loss": 0.0585,
      "step": 1830
    },
    {
      "epoch": 2.224242424242424,
      "grad_norm": 0.07811231911182404,
      "learning_rate": 2e-05,
      "loss": 0.054,
      "step": 1835
    },
    {
      "epoch": 2.2303030303030305,
      "grad_norm": 0.09428803622722626,
      "learning_rate": 2e-05,
      "loss": 0.0564,
      "step": 1840
    },
    {
      "epoch": 2.2363636363636363,
      "grad_norm": 0.1146610900759697,
      "learning_rate": 2e-05,
      "loss": 0.0551,
      "step": 1845
    },
    {
      "epoch": 2.242424242424242,
      "grad_norm": 0.10100534558296204,
      "learning_rate": 2e-05,
      "loss": 0.0587,
      "step": 1850
    },
    {
      "epoch": 2.2484848484848485,
      "grad_norm": 0.08056534081697464,
      "learning_rate": 2e-05,
      "loss": 0.0574,
      "step": 1855
    },
    {
      "epoch": 2.2545454545454544,
      "grad_norm": 0.12085167318582535,
      "learning_rate": 2e-05,
      "loss": 0.0577,
      "step": 1860
    },
    {
      "epoch": 2.2606060606060607,
      "grad_norm": 0.09579804539680481,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 1865
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.09970785677433014,
      "learning_rate": 2e-05,
      "loss": 0.0561,
      "step": 1870
    },
    {
      "epoch": 2.272727272727273,
      "grad_norm": 0.0863180160522461,
      "learning_rate": 2e-05,
      "loss": 0.0556,
      "step": 1875
    },
    {
      "epoch": 2.278787878787879,
      "grad_norm": 0.10155583918094635,
      "learning_rate": 2e-05,
      "loss": 0.0536,
      "step": 1880
    },
    {
      "epoch": 2.2848484848484847,
      "grad_norm": 0.11108866333961487,
      "learning_rate": 2e-05,
      "loss": 0.0557,
      "step": 1885
    },
    {
      "epoch": 2.290909090909091,
      "grad_norm": 0.08725141733884811,
      "learning_rate": 2e-05,
      "loss": 0.0561,
      "step": 1890
    },
    {
      "epoch": 2.296969696969697,
      "grad_norm": 0.08204500377178192,
      "learning_rate": 2e-05,
      "loss": 0.0543,
      "step": 1895
    },
    {
      "epoch": 2.303030303030303,
      "grad_norm": 0.09504231065511703,
      "learning_rate": 2e-05,
      "loss": 0.0554,
      "step": 1900
    },
    {
      "epoch": 2.309090909090909,
      "grad_norm": 0.11862266063690186,
      "learning_rate": 2e-05,
      "loss": 0.06,
      "step": 1905
    },
    {
      "epoch": 2.315151515151515,
      "grad_norm": 0.09070909768342972,
      "learning_rate": 2e-05,
      "loss": 0.0554,
      "step": 1910
    },
    {
      "epoch": 2.3212121212121213,
      "grad_norm": 0.08296236395835876,
      "learning_rate": 2e-05,
      "loss": 0.0547,
      "step": 1915
    },
    {
      "epoch": 2.327272727272727,
      "grad_norm": 0.0831565111875534,
      "learning_rate": 2e-05,
      "loss": 0.0558,
      "step": 1920
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.1003640815615654,
      "learning_rate": 2e-05,
      "loss": 0.0548,
      "step": 1925
    },
    {
      "epoch": 2.3393939393939394,
      "grad_norm": 0.09701624512672424,
      "learning_rate": 2e-05,
      "loss": 0.057,
      "step": 1930
    },
    {
      "epoch": 2.3454545454545452,
      "grad_norm": 0.09111439436674118,
      "learning_rate": 2e-05,
      "loss": 0.0554,
      "step": 1935
    },
    {
      "epoch": 2.3515151515151516,
      "grad_norm": 0.08885496109724045,
      "learning_rate": 2e-05,
      "loss": 0.0544,
      "step": 1940
    },
    {
      "epoch": 2.3575757575757574,
      "grad_norm": 0.08709389716386795,
      "learning_rate": 2e-05,
      "loss": 0.0532,
      "step": 1945
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 0.09056419134140015,
      "learning_rate": 2e-05,
      "loss": 0.0548,
      "step": 1950
    },
    {
      "epoch": 2.3696969696969696,
      "grad_norm": 0.0902629867196083,
      "learning_rate": 2e-05,
      "loss": 0.0559,
      "step": 1955
    },
    {
      "epoch": 2.375757575757576,
      "grad_norm": 0.09819531440734863,
      "learning_rate": 2e-05,
      "loss": 0.0541,
      "step": 1960
    },
    {
      "epoch": 2.381818181818182,
      "grad_norm": 0.09457546472549438,
      "learning_rate": 2e-05,
      "loss": 0.0554,
      "step": 1965
    },
    {
      "epoch": 2.3878787878787877,
      "grad_norm": 0.10259608924388885,
      "learning_rate": 2e-05,
      "loss": 0.053,
      "step": 1970
    },
    {
      "epoch": 2.393939393939394,
      "grad_norm": 0.09202545881271362,
      "learning_rate": 2e-05,
      "loss": 0.0565,
      "step": 1975
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.08592692762613297,
      "learning_rate": 2e-05,
      "loss": 0.0542,
      "step": 1980
    },
    {
      "epoch": 2.4060606060606062,
      "grad_norm": 0.07557180523872375,
      "learning_rate": 2e-05,
      "loss": 0.0548,
      "step": 1985
    },
    {
      "epoch": 2.412121212121212,
      "grad_norm": 0.0878227949142456,
      "learning_rate": 2e-05,
      "loss": 0.0564,
      "step": 1990
    },
    {
      "epoch": 2.418181818181818,
      "grad_norm": 0.11222141981124878,
      "learning_rate": 2e-05,
      "loss": 0.0544,
      "step": 1995
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 0.09251585602760315,
      "learning_rate": 2e-05,
      "loss": 0.0563,
      "step": 2000
    },
    {
      "epoch": 2.43030303030303,
      "grad_norm": 0.08692378550767899,
      "learning_rate": 2e-05,
      "loss": 0.0536,
      "step": 2005
    },
    {
      "epoch": 2.4363636363636365,
      "grad_norm": 0.11542733758687973,
      "learning_rate": 2e-05,
      "loss": 0.0534,
      "step": 2010
    },
    {
      "epoch": 2.4424242424242424,
      "grad_norm": 0.09277421236038208,
      "learning_rate": 2e-05,
      "loss": 0.0535,
      "step": 2015
    },
    {
      "epoch": 2.4484848484848483,
      "grad_norm": 0.07674136012792587,
      "learning_rate": 2e-05,
      "loss": 0.0534,
      "step": 2020
    },
    {
      "epoch": 2.4545454545454546,
      "grad_norm": 0.08172065764665604,
      "learning_rate": 2e-05,
      "loss": 0.0534,
      "step": 2025
    },
    {
      "epoch": 2.4606060606060605,
      "grad_norm": 0.11257901787757874,
      "learning_rate": 2e-05,
      "loss": 0.0539,
      "step": 2030
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.08258523792028427,
      "learning_rate": 2e-05,
      "loss": 0.0546,
      "step": 2035
    },
    {
      "epoch": 2.4727272727272727,
      "grad_norm": 0.07788795232772827,
      "learning_rate": 2e-05,
      "loss": 0.0568,
      "step": 2040
    },
    {
      "epoch": 2.478787878787879,
      "grad_norm": 0.1026751771569252,
      "learning_rate": 2e-05,
      "loss": 0.0573,
      "step": 2045
    },
    {
      "epoch": 2.484848484848485,
      "grad_norm": 0.1028742641210556,
      "learning_rate": 2e-05,
      "loss": 0.0536,
      "step": 2050
    },
    {
      "epoch": 2.4909090909090907,
      "grad_norm": 0.07339055091142654,
      "learning_rate": 2e-05,
      "loss": 0.0541,
      "step": 2055
    },
    {
      "epoch": 2.496969696969697,
      "grad_norm": 0.10682176053524017,
      "learning_rate": 2e-05,
      "loss": 0.0555,
      "step": 2060
    },
    {
      "epoch": 2.4993939393939395,
      "eval_average": 0.42648903419957634,
      "eval_crossner_ai": 0.38992332963354154,
      "eval_crossner_literature": 0.5140291806457461,
      "eval_crossner_music": 0.5378151260003448,
      "eval_crossner_politics": 0.5445935279689375,
      "eval_crossner_science": 0.5438066464760087,
      "eval_mit-movie": 0.2492753622703634,
      "eval_mit-restaurant": 0.2059800664020927,
      "eval_runtime": 36.8875,
      "eval_samples_per_second": 18.977,
      "eval_steps_per_second": 0.19,
      "step": 2062
    },
    {
      "epoch": 2.503030303030303,
      "grad_norm": 0.08328652381896973,
      "learning_rate": 2e-05,
      "loss": 0.0529,
      "step": 2065
    },
    {
      "epoch": 2.5090909090909093,
      "grad_norm": 0.09705033898353577,
      "learning_rate": 2e-05,
      "loss": 0.0517,
      "step": 2070
    },
    {
      "epoch": 2.515151515151515,
      "grad_norm": 0.08473820984363556,
      "learning_rate": 2e-05,
      "loss": 0.0537,
      "step": 2075
    },
    {
      "epoch": 2.521212121212121,
      "grad_norm": 0.09556052088737488,
      "learning_rate": 2e-05,
      "loss": 0.0545,
      "step": 2080
    },
    {
      "epoch": 2.5272727272727273,
      "grad_norm": 0.10133683681488037,
      "learning_rate": 2e-05,
      "loss": 0.0534,
      "step": 2085
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.0928308442234993,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 2090
    },
    {
      "epoch": 2.5393939393939395,
      "grad_norm": 0.08001869171857834,
      "learning_rate": 2e-05,
      "loss": 0.053,
      "step": 2095
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 0.14043131470680237,
      "learning_rate": 2e-05,
      "loss": 0.0538,
      "step": 2100
    },
    {
      "epoch": 2.5515151515151517,
      "grad_norm": 0.10991417616605759,
      "learning_rate": 2e-05,
      "loss": 0.0548,
      "step": 2105
    },
    {
      "epoch": 2.5575757575757576,
      "grad_norm": 0.08969065546989441,
      "learning_rate": 2e-05,
      "loss": 0.054,
      "step": 2110
    },
    {
      "epoch": 2.5636363636363635,
      "grad_norm": 0.0813891589641571,
      "learning_rate": 2e-05,
      "loss": 0.0538,
      "step": 2115
    },
    {
      "epoch": 2.56969696969697,
      "grad_norm": 0.11514724791049957,
      "learning_rate": 2e-05,
      "loss": 0.0554,
      "step": 2120
    },
    {
      "epoch": 2.5757575757575757,
      "grad_norm": 0.0815487951040268,
      "learning_rate": 2e-05,
      "loss": 0.0559,
      "step": 2125
    },
    {
      "epoch": 2.581818181818182,
      "grad_norm": 0.09173119813203812,
      "learning_rate": 2e-05,
      "loss": 0.0521,
      "step": 2130
    },
    {
      "epoch": 2.587878787878788,
      "grad_norm": 0.08805537223815918,
      "learning_rate": 2e-05,
      "loss": 0.0536,
      "step": 2135
    },
    {
      "epoch": 2.5939393939393938,
      "grad_norm": 0.08567826449871063,
      "learning_rate": 2e-05,
      "loss": 0.0546,
      "step": 2140
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.08710130304098129,
      "learning_rate": 2e-05,
      "loss": 0.052,
      "step": 2145
    },
    {
      "epoch": 2.606060606060606,
      "grad_norm": 0.08095793426036835,
      "learning_rate": 2e-05,
      "loss": 0.0539,
      "step": 2150
    },
    {
      "epoch": 2.6121212121212123,
      "grad_norm": 0.099518321454525,
      "learning_rate": 2e-05,
      "loss": 0.0546,
      "step": 2155
    },
    {
      "epoch": 2.618181818181818,
      "grad_norm": 0.095518559217453,
      "learning_rate": 2e-05,
      "loss": 0.0528,
      "step": 2160
    },
    {
      "epoch": 2.624242424242424,
      "grad_norm": 0.07177802175283432,
      "learning_rate": 2e-05,
      "loss": 0.0546,
      "step": 2165
    },
    {
      "epoch": 2.6303030303030304,
      "grad_norm": 0.0785207748413086,
      "learning_rate": 2e-05,
      "loss": 0.0574,
      "step": 2170
    },
    {
      "epoch": 2.6363636363636362,
      "grad_norm": 0.08196365833282471,
      "learning_rate": 2e-05,
      "loss": 0.0513,
      "step": 2175
    },
    {
      "epoch": 2.6424242424242426,
      "grad_norm": 0.10643240064382553,
      "learning_rate": 2e-05,
      "loss": 0.0547,
      "step": 2180
    },
    {
      "epoch": 2.6484848484848484,
      "grad_norm": 0.07184486836194992,
      "learning_rate": 2e-05,
      "loss": 0.0533,
      "step": 2185
    },
    {
      "epoch": 2.6545454545454548,
      "grad_norm": 0.07186758518218994,
      "learning_rate": 2e-05,
      "loss": 0.0542,
      "step": 2190
    },
    {
      "epoch": 2.6606060606060606,
      "grad_norm": 0.09532799571752548,
      "learning_rate": 2e-05,
      "loss": 0.052,
      "step": 2195
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.09583264589309692,
      "learning_rate": 2e-05,
      "loss": 0.0559,
      "step": 2200
    },
    {
      "epoch": 2.672727272727273,
      "grad_norm": 0.0916813313961029,
      "learning_rate": 2e-05,
      "loss": 0.0514,
      "step": 2205
    },
    {
      "epoch": 2.6787878787878787,
      "grad_norm": 0.09480542689561844,
      "learning_rate": 2e-05,
      "loss": 0.0533,
      "step": 2210
    },
    {
      "epoch": 2.684848484848485,
      "grad_norm": 0.14924627542495728,
      "learning_rate": 2e-05,
      "loss": 0.0555,
      "step": 2215
    },
    {
      "epoch": 2.690909090909091,
      "grad_norm": 0.08633684366941452,
      "learning_rate": 2e-05,
      "loss": 0.0562,
      "step": 2220
    },
    {
      "epoch": 2.696969696969697,
      "grad_norm": 0.11001294106245041,
      "learning_rate": 2e-05,
      "loss": 0.0535,
      "step": 2225
    },
    {
      "epoch": 2.703030303030303,
      "grad_norm": 0.0840749442577362,
      "learning_rate": 2e-05,
      "loss": 0.0565,
      "step": 2230
    },
    {
      "epoch": 2.709090909090909,
      "grad_norm": 0.0826306939125061,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 2235
    },
    {
      "epoch": 2.7151515151515153,
      "grad_norm": 0.08084285259246826,
      "learning_rate": 2e-05,
      "loss": 0.0532,
      "step": 2240
    },
    {
      "epoch": 2.721212121212121,
      "grad_norm": 0.07185543328523636,
      "learning_rate": 2e-05,
      "loss": 0.0547,
      "step": 2245
    },
    {
      "epoch": 2.727272727272727,
      "grad_norm": 0.09753673523664474,
      "learning_rate": 2e-05,
      "loss": 0.053,
      "step": 2250
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.07325294613838196,
      "learning_rate": 2e-05,
      "loss": 0.0558,
      "step": 2255
    },
    {
      "epoch": 2.7393939393939393,
      "grad_norm": 0.09294768422842026,
      "learning_rate": 2e-05,
      "loss": 0.0541,
      "step": 2260
    },
    {
      "epoch": 2.7454545454545456,
      "grad_norm": 0.09603574126958847,
      "learning_rate": 2e-05,
      "loss": 0.0507,
      "step": 2265
    },
    {
      "epoch": 2.7515151515151515,
      "grad_norm": 0.0822620764374733,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 2270
    },
    {
      "epoch": 2.757575757575758,
      "grad_norm": 0.07940545678138733,
      "learning_rate": 2e-05,
      "loss": 0.0529,
      "step": 2275
    },
    {
      "epoch": 2.7636363636363637,
      "grad_norm": 0.10696929693222046,
      "learning_rate": 2e-05,
      "loss": 0.0553,
      "step": 2280
    },
    {
      "epoch": 2.7696969696969695,
      "grad_norm": 0.07762196660041809,
      "learning_rate": 2e-05,
      "loss": 0.0535,
      "step": 2285
    },
    {
      "epoch": 2.775757575757576,
      "grad_norm": 0.08690202236175537,
      "learning_rate": 2e-05,
      "loss": 0.0528,
      "step": 2290
    },
    {
      "epoch": 2.7818181818181817,
      "grad_norm": 0.11286535114049911,
      "learning_rate": 2e-05,
      "loss": 0.054,
      "step": 2295
    },
    {
      "epoch": 2.787878787878788,
      "grad_norm": 0.09217704832553864,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 2300
    },
    {
      "epoch": 2.793939393939394,
      "grad_norm": 0.08927052468061447,
      "learning_rate": 2e-05,
      "loss": 0.0539,
      "step": 2305
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.0882687196135521,
      "learning_rate": 2e-05,
      "loss": 0.0527,
      "step": 2310
    },
    {
      "epoch": 2.806060606060606,
      "grad_norm": 0.08721008896827698,
      "learning_rate": 2e-05,
      "loss": 0.0537,
      "step": 2315
    },
    {
      "epoch": 2.812121212121212,
      "grad_norm": 0.07122723013162613,
      "learning_rate": 2e-05,
      "loss": 0.0534,
      "step": 2320
    },
    {
      "epoch": 2.8181818181818183,
      "grad_norm": 0.0903463214635849,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 2325
    },
    {
      "epoch": 2.824242424242424,
      "grad_norm": 0.06891690194606781,
      "learning_rate": 2e-05,
      "loss": 0.0523,
      "step": 2330
    },
    {
      "epoch": 2.83030303030303,
      "grad_norm": 0.0785510241985321,
      "learning_rate": 2e-05,
      "loss": 0.0524,
      "step": 2335
    },
    {
      "epoch": 2.8363636363636364,
      "grad_norm": 0.07461301237344742,
      "learning_rate": 2e-05,
      "loss": 0.0511,
      "step": 2340
    },
    {
      "epoch": 2.8424242424242423,
      "grad_norm": 0.0739818587899208,
      "learning_rate": 2e-05,
      "loss": 0.0539,
      "step": 2345
    },
    {
      "epoch": 2.8484848484848486,
      "grad_norm": 0.09911369532346725,
      "learning_rate": 2e-05,
      "loss": 0.0528,
      "step": 2350
    },
    {
      "epoch": 2.8545454545454545,
      "grad_norm": 0.08169437944889069,
      "learning_rate": 2e-05,
      "loss": 0.0515,
      "step": 2355
    },
    {
      "epoch": 2.860606060606061,
      "grad_norm": 0.07299330085515976,
      "learning_rate": 2e-05,
      "loss": 0.0527,
      "step": 2360
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.07333392649888992,
      "learning_rate": 2e-05,
      "loss": 0.0497,
      "step": 2365
    },
    {
      "epoch": 2.8727272727272726,
      "grad_norm": 0.08029394596815109,
      "learning_rate": 2e-05,
      "loss": 0.0521,
      "step": 2370
    },
    {
      "epoch": 2.878787878787879,
      "grad_norm": 0.08430328220129013,
      "learning_rate": 2e-05,
      "loss": 0.0519,
      "step": 2375
    },
    {
      "epoch": 2.8848484848484848,
      "grad_norm": 0.07373260706663132,
      "learning_rate": 2e-05,
      "loss": 0.0521,
      "step": 2380
    },
    {
      "epoch": 2.890909090909091,
      "grad_norm": 0.08018258959054947,
      "learning_rate": 2e-05,
      "loss": 0.0508,
      "step": 2385
    },
    {
      "epoch": 2.896969696969697,
      "grad_norm": 0.07129046320915222,
      "learning_rate": 2e-05,
      "loss": 0.0544,
      "step": 2390
    },
    {
      "epoch": 2.903030303030303,
      "grad_norm": 0.08294586092233658,
      "learning_rate": 2e-05,
      "loss": 0.0545,
      "step": 2395
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 0.07333393394947052,
      "learning_rate": 2e-05,
      "loss": 0.0515,
      "step": 2400
    },
    {
      "epoch": 2.915151515151515,
      "grad_norm": 0.07823723554611206,
      "learning_rate": 2e-05,
      "loss": 0.0533,
      "step": 2405
    },
    {
      "epoch": 2.9212121212121214,
      "grad_norm": 0.09782014787197113,
      "learning_rate": 2e-05,
      "loss": 0.0515,
      "step": 2410
    },
    {
      "epoch": 2.9272727272727272,
      "grad_norm": 0.09487218409776688,
      "learning_rate": 2e-05,
      "loss": 0.0544,
      "step": 2415
    },
    {
      "epoch": 2.933333333333333,
      "grad_norm": 0.07786460965871811,
      "learning_rate": 2e-05,
      "loss": 0.0514,
      "step": 2420
    },
    {
      "epoch": 2.9393939393939394,
      "grad_norm": 0.1301707923412323,
      "learning_rate": 2e-05,
      "loss": 0.0523,
      "step": 2425
    },
    {
      "epoch": 2.9454545454545453,
      "grad_norm": 0.09946519136428833,
      "learning_rate": 2e-05,
      "loss": 0.0511,
      "step": 2430
    },
    {
      "epoch": 2.9515151515151516,
      "grad_norm": 0.09072303771972656,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 2435
    },
    {
      "epoch": 2.9575757575757575,
      "grad_norm": 0.08280112594366074,
      "learning_rate": 2e-05,
      "loss": 0.0529,
      "step": 2440
    },
    {
      "epoch": 2.963636363636364,
      "grad_norm": 0.07518143206834793,
      "learning_rate": 2e-05,
      "loss": 0.0492,
      "step": 2445
    },
    {
      "epoch": 2.9696969696969697,
      "grad_norm": 0.09459201991558075,
      "learning_rate": 2e-05,
      "loss": 0.0531,
      "step": 2450
    },
    {
      "epoch": 2.9757575757575756,
      "grad_norm": 0.08602622151374817,
      "learning_rate": 2e-05,
      "loss": 0.0523,
      "step": 2455
    },
    {
      "epoch": 2.981818181818182,
      "grad_norm": 0.10319439321756363,
      "learning_rate": 2e-05,
      "loss": 0.0513,
      "step": 2460
    },
    {
      "epoch": 2.987878787878788,
      "grad_norm": 0.10870449990034103,
      "learning_rate": 2e-05,
      "loss": 0.0542,
      "step": 2465
    },
    {
      "epoch": 2.993939393939394,
      "grad_norm": 0.09876380115747452,
      "learning_rate": 2e-05,
      "loss": 0.0501,
      "step": 2470
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.08492707461118698,
      "learning_rate": 2e-05,
      "loss": 0.0525,
      "step": 2475
    },
    {
      "epoch": 3.0,
      "eval_average": 0.436919654065071,
      "eval_crossner_ai": 0.41612200430854274,
      "eval_crossner_literature": 0.5269058295463106,
      "eval_crossner_music": 0.5606694560168682,
      "eval_crossner_politics": 0.5315457412749214,
      "eval_crossner_science": 0.5636910731700191,
      "eval_mit-movie": 0.2556818181329384,
      "eval_mit-restaurant": 0.2038216560058968,
      "eval_runtime": 37.028,
      "eval_samples_per_second": 18.905,
      "eval_steps_per_second": 0.189,
      "step": 2475
    },
    {
      "epoch": 3.006060606060606,
      "grad_norm": 0.08126520365476608,
      "learning_rate": 2e-05,
      "loss": 0.0455,
      "step": 2480
    },
    {
      "epoch": 3.012121212121212,
      "grad_norm": 0.11523045599460602,
      "learning_rate": 2e-05,
      "loss": 0.0479,
      "step": 2485
    },
    {
      "epoch": 3.018181818181818,
      "grad_norm": 0.08021163195371628,
      "learning_rate": 2e-05,
      "loss": 0.0505,
      "step": 2490
    },
    {
      "epoch": 3.0242424242424244,
      "grad_norm": 0.10933960974216461,
      "learning_rate": 2e-05,
      "loss": 0.0527,
      "step": 2495
    },
    {
      "epoch": 3.0303030303030303,
      "grad_norm": 0.0824558213353157,
      "learning_rate": 2e-05,
      "loss": 0.0512,
      "step": 2500
    },
    {
      "epoch": 3.036363636363636,
      "grad_norm": 0.07768621295690536,
      "learning_rate": 2e-05,
      "loss": 0.0526,
      "step": 2505
    },
    {
      "epoch": 3.0424242424242425,
      "grad_norm": 0.08578856289386749,
      "learning_rate": 2e-05,
      "loss": 0.052,
      "step": 2510
    },
    {
      "epoch": 3.0484848484848484,
      "grad_norm": 0.10224081575870514,
      "learning_rate": 2e-05,
      "loss": 0.0512,
      "step": 2515
    },
    {
      "epoch": 3.0545454545454547,
      "grad_norm": 0.08893104642629623,
      "learning_rate": 2e-05,
      "loss": 0.0507,
      "step": 2520
    },
    {
      "epoch": 3.0606060606060606,
      "grad_norm": 0.07990828901529312,
      "learning_rate": 2e-05,
      "loss": 0.0521,
      "step": 2525
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.07502003759145737,
      "learning_rate": 2e-05,
      "loss": 0.0514,
      "step": 2530
    },
    {
      "epoch": 3.0727272727272728,
      "grad_norm": 0.07419338822364807,
      "learning_rate": 2e-05,
      "loss": 0.0512,
      "step": 2535
    },
    {
      "epoch": 3.0787878787878786,
      "grad_norm": 0.09514818340539932,
      "learning_rate": 2e-05,
      "loss": 0.0498,
      "step": 2540
    },
    {
      "epoch": 3.084848484848485,
      "grad_norm": 0.06699466705322266,
      "learning_rate": 2e-05,
      "loss": 0.0471,
      "step": 2545
    },
    {
      "epoch": 3.090909090909091,
      "grad_norm": 0.1037176102399826,
      "learning_rate": 2e-05,
      "loss": 0.0482,
      "step": 2550
    },
    {
      "epoch": 3.096969696969697,
      "grad_norm": 0.07632723450660706,
      "learning_rate": 2e-05,
      "loss": 0.0496,
      "step": 2555
    },
    {
      "epoch": 3.103030303030303,
      "grad_norm": 0.08387600630521774,
      "learning_rate": 2e-05,
      "loss": 0.0542,
      "step": 2560
    },
    {
      "epoch": 3.109090909090909,
      "grad_norm": 0.07948878407478333,
      "learning_rate": 2e-05,
      "loss": 0.0501,
      "step": 2565
    },
    {
      "epoch": 3.1151515151515152,
      "grad_norm": 0.10482034832239151,
      "learning_rate": 2e-05,
      "loss": 0.0505,
      "step": 2570
    },
    {
      "epoch": 3.121212121212121,
      "grad_norm": 0.0903296247124672,
      "learning_rate": 2e-05,
      "loss": 0.0491,
      "step": 2575
    },
    {
      "epoch": 3.1272727272727274,
      "grad_norm": 0.07336815446615219,
      "learning_rate": 2e-05,
      "loss": 0.0502,
      "step": 2580
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.08560603857040405,
      "learning_rate": 2e-05,
      "loss": 0.052,
      "step": 2585
    },
    {
      "epoch": 3.139393939393939,
      "grad_norm": 0.1134142279624939,
      "learning_rate": 2e-05,
      "loss": 0.0503,
      "step": 2590
    },
    {
      "epoch": 3.1454545454545455,
      "grad_norm": 0.08839640021324158,
      "learning_rate": 2e-05,
      "loss": 0.0508,
      "step": 2595
    },
    {
      "epoch": 3.1515151515151514,
      "grad_norm": 0.08056219667196274,
      "learning_rate": 2e-05,
      "loss": 0.0499,
      "step": 2600
    },
    {
      "epoch": 3.1575757575757577,
      "grad_norm": 0.09858381003141403,
      "learning_rate": 2e-05,
      "loss": 0.0503,
      "step": 2605
    },
    {
      "epoch": 3.1636363636363636,
      "grad_norm": 0.1036360040307045,
      "learning_rate": 2e-05,
      "loss": 0.048,
      "step": 2610
    },
    {
      "epoch": 3.16969696969697,
      "grad_norm": 0.07284624129533768,
      "learning_rate": 2e-05,
      "loss": 0.0488,
      "step": 2615
    },
    {
      "epoch": 3.175757575757576,
      "grad_norm": 0.07893965393304825,
      "learning_rate": 2e-05,
      "loss": 0.0495,
      "step": 2620
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 0.06299871951341629,
      "learning_rate": 2e-05,
      "loss": 0.0492,
      "step": 2625
    },
    {
      "epoch": 3.187878787878788,
      "grad_norm": 0.08479047566652298,
      "learning_rate": 2e-05,
      "loss": 0.0515,
      "step": 2630
    },
    {
      "epoch": 3.193939393939394,
      "grad_norm": 0.07494788616895676,
      "learning_rate": 2e-05,
      "loss": 0.0485,
      "step": 2635
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.09878699481487274,
      "learning_rate": 2e-05,
      "loss": 0.0504,
      "step": 2640
    },
    {
      "epoch": 3.206060606060606,
      "grad_norm": 0.07650727778673172,
      "learning_rate": 2e-05,
      "loss": 0.0515,
      "step": 2645
    },
    {
      "epoch": 3.212121212121212,
      "grad_norm": 0.07814870774745941,
      "learning_rate": 2e-05,
      "loss": 0.0497,
      "step": 2650
    },
    {
      "epoch": 3.2181818181818183,
      "grad_norm": 0.07646813988685608,
      "learning_rate": 2e-05,
      "loss": 0.0529,
      "step": 2655
    },
    {
      "epoch": 3.224242424242424,
      "grad_norm": 0.07032671570777893,
      "learning_rate": 2e-05,
      "loss": 0.05,
      "step": 2660
    },
    {
      "epoch": 3.2303030303030305,
      "grad_norm": 0.1017293855547905,
      "learning_rate": 2e-05,
      "loss": 0.0503,
      "step": 2665
    },
    {
      "epoch": 3.2363636363636363,
      "grad_norm": 0.08346594125032425,
      "learning_rate": 2e-05,
      "loss": 0.0512,
      "step": 2670
    },
    {
      "epoch": 3.242424242424242,
      "grad_norm": 0.08396374434232712,
      "learning_rate": 2e-05,
      "loss": 0.0502,
      "step": 2675
    },
    {
      "epoch": 3.2484848484848485,
      "grad_norm": 0.08745280653238297,
      "learning_rate": 2e-05,
      "loss": 0.048,
      "step": 2680
    },
    {
      "epoch": 3.2545454545454544,
      "grad_norm": 0.09214481711387634,
      "learning_rate": 2e-05,
      "loss": 0.0495,
      "step": 2685
    },
    {
      "epoch": 3.2606060606060607,
      "grad_norm": 0.07967057079076767,
      "learning_rate": 2e-05,
      "loss": 0.0513,
      "step": 2690
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.08613846451044083,
      "learning_rate": 2e-05,
      "loss": 0.0502,
      "step": 2695
    },
    {
      "epoch": 3.272727272727273,
      "grad_norm": 0.0827888548374176,
      "learning_rate": 2e-05,
      "loss": 0.0509,
      "step": 2700
    },
    {
      "epoch": 3.278787878787879,
      "grad_norm": 0.08517400175333023,
      "learning_rate": 2e-05,
      "loss": 0.0514,
      "step": 2705
    },
    {
      "epoch": 3.2848484848484847,
      "grad_norm": 0.0982867032289505,
      "learning_rate": 2e-05,
      "loss": 0.0497,
      "step": 2710
    },
    {
      "epoch": 3.290909090909091,
      "grad_norm": 0.09097997099161148,
      "learning_rate": 2e-05,
      "loss": 0.0489,
      "step": 2715
    },
    {
      "epoch": 3.296969696969697,
      "grad_norm": 0.07994979619979858,
      "learning_rate": 2e-05,
      "loss": 0.0503,
      "step": 2720
    },
    {
      "epoch": 3.303030303030303,
      "grad_norm": 0.08794131129980087,
      "learning_rate": 2e-05,
      "loss": 0.0488,
      "step": 2725
    },
    {
      "epoch": 3.309090909090909,
      "grad_norm": 0.06289218366146088,
      "learning_rate": 2e-05,
      "loss": 0.0493,
      "step": 2730
    },
    {
      "epoch": 3.315151515151515,
      "grad_norm": 0.09704709053039551,
      "learning_rate": 2e-05,
      "loss": 0.0525,
      "step": 2735
    },
    {
      "epoch": 3.3212121212121213,
      "grad_norm": 0.08160286396741867,
      "learning_rate": 2e-05,
      "loss": 0.0479,
      "step": 2740
    },
    {
      "epoch": 3.327272727272727,
      "grad_norm": 0.08084530383348465,
      "learning_rate": 2e-05,
      "loss": 0.0497,
      "step": 2745
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.09469226747751236,
      "learning_rate": 2e-05,
      "loss": 0.0495,
      "step": 2750
    },
    {
      "epoch": 3.3393939393939394,
      "grad_norm": 0.08212893456220627,
      "learning_rate": 2e-05,
      "loss": 0.0488,
      "step": 2755
    },
    {
      "epoch": 3.3454545454545452,
      "grad_norm": 0.09544996172189713,
      "learning_rate": 2e-05,
      "loss": 0.0496,
      "step": 2760
    },
    {
      "epoch": 3.3515151515151516,
      "grad_norm": 0.08618342876434326,
      "learning_rate": 2e-05,
      "loss": 0.0495,
      "step": 2765
    },
    {
      "epoch": 3.3575757575757574,
      "grad_norm": 0.07632188498973846,
      "learning_rate": 2e-05,
      "loss": 0.049,
      "step": 2770
    },
    {
      "epoch": 3.3636363636363638,
      "grad_norm": 0.08443369716405869,
      "learning_rate": 2e-05,
      "loss": 0.0512,
      "step": 2775
    },
    {
      "epoch": 3.3696969696969696,
      "grad_norm": 0.06697536259889603,
      "learning_rate": 2e-05,
      "loss": 0.0507,
      "step": 2780
    },
    {
      "epoch": 3.375757575757576,
      "grad_norm": 0.06862537562847137,
      "learning_rate": 2e-05,
      "loss": 0.0465,
      "step": 2785
    },
    {
      "epoch": 3.381818181818182,
      "grad_norm": 0.0811719000339508,
      "learning_rate": 2e-05,
      "loss": 0.0502,
      "step": 2790
    },
    {
      "epoch": 3.3878787878787877,
      "grad_norm": 0.08253072947263718,
      "learning_rate": 2e-05,
      "loss": 0.0475,
      "step": 2795
    },
    {
      "epoch": 3.393939393939394,
      "grad_norm": 0.09789919853210449,
      "learning_rate": 2e-05,
      "loss": 0.048,
      "step": 2800
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.09134412556886673,
      "learning_rate": 2e-05,
      "loss": 0.0505,
      "step": 2805
    },
    {
      "epoch": 3.4060606060606062,
      "grad_norm": 0.07500903308391571,
      "learning_rate": 2e-05,
      "loss": 0.0479,
      "step": 2810
    },
    {
      "epoch": 3.412121212121212,
      "grad_norm": 0.0761306881904602,
      "learning_rate": 2e-05,
      "loss": 0.0461,
      "step": 2815
    },
    {
      "epoch": 3.418181818181818,
      "grad_norm": 0.07203389704227448,
      "learning_rate": 2e-05,
      "loss": 0.0501,
      "step": 2820
    },
    {
      "epoch": 3.4242424242424243,
      "grad_norm": 0.08914759010076523,
      "learning_rate": 2e-05,
      "loss": 0.0497,
      "step": 2825
    },
    {
      "epoch": 3.43030303030303,
      "grad_norm": 0.07551413029432297,
      "learning_rate": 2e-05,
      "loss": 0.0492,
      "step": 2830
    },
    {
      "epoch": 3.4363636363636365,
      "grad_norm": 0.08405609428882599,
      "learning_rate": 2e-05,
      "loss": 0.0501,
      "step": 2835
    },
    {
      "epoch": 3.4424242424242424,
      "grad_norm": 0.08056672662496567,
      "learning_rate": 2e-05,
      "loss": 0.0493,
      "step": 2840
    },
    {
      "epoch": 3.4484848484848483,
      "grad_norm": 0.10566660761833191,
      "learning_rate": 2e-05,
      "loss": 0.0501,
      "step": 2845
    },
    {
      "epoch": 3.4545454545454546,
      "grad_norm": 0.08070467412471771,
      "learning_rate": 2e-05,
      "loss": 0.0499,
      "step": 2850
    },
    {
      "epoch": 3.4606060606060605,
      "grad_norm": 0.08191893994808197,
      "learning_rate": 2e-05,
      "loss": 0.0487,
      "step": 2855
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.07883719354867935,
      "learning_rate": 2e-05,
      "loss": 0.0475,
      "step": 2860
    },
    {
      "epoch": 3.4727272727272727,
      "grad_norm": 0.09503255784511566,
      "learning_rate": 2e-05,
      "loss": 0.0516,
      "step": 2865
    },
    {
      "epoch": 3.478787878787879,
      "grad_norm": 0.08640462905168533,
      "learning_rate": 2e-05,
      "loss": 0.0493,
      "step": 2870
    },
    {
      "epoch": 3.484848484848485,
      "grad_norm": 0.08768320828676224,
      "learning_rate": 2e-05,
      "loss": 0.047,
      "step": 2875
    },
    {
      "epoch": 3.4909090909090907,
      "grad_norm": 0.10089338570833206,
      "learning_rate": 2e-05,
      "loss": 0.0474,
      "step": 2880
    },
    {
      "epoch": 3.496969696969697,
      "grad_norm": 0.08559415489435196,
      "learning_rate": 2e-05,
      "loss": 0.052,
      "step": 2885
    },
    {
      "epoch": 3.5006060606060605,
      "eval_average": 0.44226920802400566,
      "eval_crossner_ai": 0.41850220259426824,
      "eval_crossner_literature": 0.5223367697093311,
      "eval_crossner_music": 0.5708304256303117,
      "eval_crossner_politics": 0.5087719297745261,
      "eval_crossner_science": 0.5617283950118871,
      "eval_mit-movie": 0.306818181769273,
      "eval_mit-restaurant": 0.20689655167844265,
      "eval_runtime": 36.0045,
      "eval_samples_per_second": 19.442,
      "eval_steps_per_second": 0.194,
      "step": 2888
    },
    {
      "epoch": 3.503030303030303,
      "grad_norm": 0.07158413529396057,
      "learning_rate": 2e-05,
      "loss": 0.0478,
      "step": 2890
    },
    {
      "epoch": 3.5090909090909093,
      "grad_norm": 0.07699014246463776,
      "learning_rate": 2e-05,
      "loss": 0.0494,
      "step": 2895
    },
    {
      "epoch": 3.515151515151515,
      "grad_norm": 0.07872941344976425,
      "learning_rate": 2e-05,
      "loss": 0.0496,
      "step": 2900
    },
    {
      "epoch": 3.521212121212121,
      "grad_norm": 0.10374293476343155,
      "learning_rate": 2e-05,
      "loss": 0.0494,
      "step": 2905
    },
    {
      "epoch": 3.5272727272727273,
      "grad_norm": 0.09289398789405823,
      "learning_rate": 2e-05,
      "loss": 0.0499,
      "step": 2910
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.09216403216123581,
      "learning_rate": 2e-05,
      "loss": 0.0501,
      "step": 2915
    },
    {
      "epoch": 3.5393939393939395,
      "grad_norm": 0.0962405577301979,
      "learning_rate": 2e-05,
      "loss": 0.0475,
      "step": 2920
    },
    {
      "epoch": 3.5454545454545454,
      "grad_norm": 0.09235166013240814,
      "learning_rate": 2e-05,
      "loss": 0.049,
      "step": 2925
    },
    {
      "epoch": 3.5515151515151517,
      "grad_norm": 0.07869108021259308,
      "learning_rate": 2e-05,
      "loss": 0.047,
      "step": 2930
    },
    {
      "epoch": 3.5575757575757576,
      "grad_norm": 0.06957069784402847,
      "learning_rate": 2e-05,
      "loss": 0.0522,
      "step": 2935
    },
    {
      "epoch": 3.5636363636363635,
      "grad_norm": 0.08449220657348633,
      "learning_rate": 2e-05,
      "loss": 0.0462,
      "step": 2940
    },
    {
      "epoch": 3.56969696969697,
      "grad_norm": 0.07405271381139755,
      "learning_rate": 2e-05,
      "loss": 0.0504,
      "step": 2945
    },
    {
      "epoch": 3.5757575757575757,
      "grad_norm": 0.08655665069818497,
      "learning_rate": 2e-05,
      "loss": 0.0494,
      "step": 2950
    },
    {
      "epoch": 3.581818181818182,
      "grad_norm": 0.08181513845920563,
      "learning_rate": 2e-05,
      "loss": 0.0489,
      "step": 2955
    },
    {
      "epoch": 3.587878787878788,
      "grad_norm": 0.07775123417377472,
      "learning_rate": 2e-05,
      "loss": 0.0502,
      "step": 2960
    },
    {
      "epoch": 3.5939393939393938,
      "grad_norm": 0.08087930828332901,
      "learning_rate": 2e-05,
      "loss": 0.0483,
      "step": 2965
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.07853736728429794,
      "learning_rate": 2e-05,
      "loss": 0.0495,
      "step": 2970
    },
    {
      "epoch": 3.606060606060606,
      "grad_norm": 0.07979531586170197,
      "learning_rate": 2e-05,
      "loss": 0.046,
      "step": 2975
    },
    {
      "epoch": 3.6121212121212123,
      "grad_norm": 0.08854591101408005,
      "learning_rate": 2e-05,
      "loss": 0.0517,
      "step": 2980
    },
    {
      "epoch": 3.618181818181818,
      "grad_norm": 0.0764528140425682,
      "learning_rate": 2e-05,
      "loss": 0.0479,
      "step": 2985
    },
    {
      "epoch": 3.624242424242424,
      "grad_norm": 0.08396422117948532,
      "learning_rate": 2e-05,
      "loss": 0.0477,
      "step": 2990
    },
    {
      "epoch": 3.6303030303030304,
      "grad_norm": 0.0939362421631813,
      "learning_rate": 2e-05,
      "loss": 0.0462,
      "step": 2995
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 0.08253025263547897,
      "learning_rate": 2e-05,
      "loss": 0.0494,
      "step": 3000
    },
    {
      "epoch": 3.6424242424242426,
      "grad_norm": 0.08833007514476776,
      "learning_rate": 2e-05,
      "loss": 0.0501,
      "step": 3005
    },
    {
      "epoch": 3.6484848484848484,
      "grad_norm": 0.08269928395748138,
      "learning_rate": 2e-05,
      "loss": 0.0479,
      "step": 3010
    },
    {
      "epoch": 3.6545454545454548,
      "grad_norm": 0.08779532462358475,
      "learning_rate": 2e-05,
      "loss": 0.0479,
      "step": 3015
    },
    {
      "epoch": 3.6606060606060606,
      "grad_norm": 0.08229665458202362,
      "learning_rate": 2e-05,
      "loss": 0.0481,
      "step": 3020
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.1047871857881546,
      "learning_rate": 2e-05,
      "loss": 0.0494,
      "step": 3025
    },
    {
      "epoch": 3.672727272727273,
      "grad_norm": 0.09364937990903854,
      "learning_rate": 2e-05,
      "loss": 0.051,
      "step": 3030
    },
    {
      "epoch": 3.6787878787878787,
      "grad_norm": 0.06863924860954285,
      "learning_rate": 2e-05,
      "loss": 0.0501,
      "step": 3035
    },
    {
      "epoch": 3.684848484848485,
      "grad_norm": 0.10138887166976929,
      "learning_rate": 2e-05,
      "loss": 0.0466,
      "step": 3040
    },
    {
      "epoch": 3.690909090909091,
      "grad_norm": 0.07104488462209702,
      "learning_rate": 2e-05,
      "loss": 0.05,
      "step": 3045
    },
    {
      "epoch": 3.696969696969697,
      "grad_norm": 0.07732344418764114,
      "learning_rate": 2e-05,
      "loss": 0.0492,
      "step": 3050
    },
    {
      "epoch": 3.703030303030303,
      "grad_norm": 0.0693678930401802,
      "learning_rate": 2e-05,
      "loss": 0.0481,
      "step": 3055
    },
    {
      "epoch": 3.709090909090909,
      "grad_norm": 0.071347177028656,
      "learning_rate": 2e-05,
      "loss": 0.0489,
      "step": 3060
    },
    {
      "epoch": 3.7151515151515153,
      "grad_norm": 0.07742372900247574,
      "learning_rate": 2e-05,
      "loss": 0.0478,
      "step": 3065
    },
    {
      "epoch": 3.721212121212121,
      "grad_norm": 0.08291993290185928,
      "learning_rate": 2e-05,
      "loss": 0.0467,
      "step": 3070
    },
    {
      "epoch": 3.727272727272727,
      "grad_norm": 0.11181927472352982,
      "learning_rate": 2e-05,
      "loss": 0.05,
      "step": 3075
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.0843551754951477,
      "learning_rate": 2e-05,
      "loss": 0.0509,
      "step": 3080
    },
    {
      "epoch": 3.7393939393939393,
      "grad_norm": 0.07416243106126785,
      "learning_rate": 2e-05,
      "loss": 0.0485,
      "step": 3085
    },
    {
      "epoch": 3.7454545454545456,
      "grad_norm": 0.07896336168050766,
      "learning_rate": 2e-05,
      "loss": 0.049,
      "step": 3090
    },
    {
      "epoch": 3.7515151515151515,
      "grad_norm": 0.08228787779808044,
      "learning_rate": 2e-05,
      "loss": 0.049,
      "step": 3095
    },
    {
      "epoch": 3.757575757575758,
      "grad_norm": 0.0934787169098854,
      "learning_rate": 2e-05,
      "loss": 0.0485,
      "step": 3100
    },
    {
      "epoch": 3.7636363636363637,
      "grad_norm": 0.11529288440942764,
      "learning_rate": 2e-05,
      "loss": 0.047,
      "step": 3105
    },
    {
      "epoch": 3.7696969696969695,
      "grad_norm": 0.10103463381528854,
      "learning_rate": 2e-05,
      "loss": 0.0507,
      "step": 3110
    },
    {
      "epoch": 3.775757575757576,
      "grad_norm": 0.12039438635110855,
      "learning_rate": 2e-05,
      "loss": 0.0476,
      "step": 3115
    },
    {
      "epoch": 3.7818181818181817,
      "grad_norm": 0.06677257269620895,
      "learning_rate": 2e-05,
      "loss": 0.0486,
      "step": 3120
    },
    {
      "epoch": 3.787878787878788,
      "grad_norm": 0.08078735321760178,
      "learning_rate": 2e-05,
      "loss": 0.0481,
      "step": 3125
    },
    {
      "epoch": 3.793939393939394,
      "grad_norm": 0.09211715310811996,
      "learning_rate": 2e-05,
      "loss": 0.0494,
      "step": 3130
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.08111614733934402,
      "learning_rate": 2e-05,
      "loss": 0.0483,
      "step": 3135
    },
    {
      "epoch": 3.806060606060606,
      "grad_norm": 0.08206365257501602,
      "learning_rate": 2e-05,
      "loss": 0.0479,
      "step": 3140
    },
    {
      "epoch": 3.812121212121212,
      "grad_norm": 0.07582933455705643,
      "learning_rate": 2e-05,
      "loss": 0.047,
      "step": 3145
    },
    {
      "epoch": 3.8181818181818183,
      "grad_norm": 0.10525751113891602,
      "learning_rate": 2e-05,
      "loss": 0.0466,
      "step": 3150
    },
    {
      "epoch": 3.824242424242424,
      "grad_norm": 0.0807139128446579,
      "learning_rate": 2e-05,
      "loss": 0.0481,
      "step": 3155
    },
    {
      "epoch": 3.83030303030303,
      "grad_norm": 0.06644953042268753,
      "learning_rate": 2e-05,
      "loss": 0.0473,
      "step": 3160
    },
    {
      "epoch": 3.8363636363636364,
      "grad_norm": 0.07880116254091263,
      "learning_rate": 2e-05,
      "loss": 0.0508,
      "step": 3165
    },
    {
      "epoch": 3.8424242424242423,
      "grad_norm": 0.1067301332950592,
      "learning_rate": 2e-05,
      "loss": 0.0481,
      "step": 3170
    },
    {
      "epoch": 3.8484848484848486,
      "grad_norm": 0.13313250243663788,
      "learning_rate": 2e-05,
      "loss": 0.0494,
      "step": 3175
    },
    {
      "epoch": 3.8545454545454545,
      "grad_norm": 0.08866290003061295,
      "learning_rate": 2e-05,
      "loss": 0.0485,
      "step": 3180
    },
    {
      "epoch": 3.860606060606061,
      "grad_norm": 0.06840592622756958,
      "learning_rate": 2e-05,
      "loss": 0.0467,
      "step": 3185
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.07395321875810623,
      "learning_rate": 2e-05,
      "loss": 0.0492,
      "step": 3190
    },
    {
      "epoch": 3.8727272727272726,
      "grad_norm": 0.08455193787813187,
      "learning_rate": 2e-05,
      "loss": 0.0484,
      "step": 3195
    },
    {
      "epoch": 3.878787878787879,
      "grad_norm": 0.10224758833646774,
      "learning_rate": 2e-05,
      "loss": 0.0505,
      "step": 3200
    },
    {
      "epoch": 3.8848484848484848,
      "grad_norm": 0.07259096950292587,
      "learning_rate": 2e-05,
      "loss": 0.0503,
      "step": 3205
    },
    {
      "epoch": 3.890909090909091,
      "grad_norm": 0.10439657419919968,
      "learning_rate": 2e-05,
      "loss": 0.0473,
      "step": 3210
    },
    {
      "epoch": 3.896969696969697,
      "grad_norm": 0.0777759850025177,
      "learning_rate": 2e-05,
      "loss": 0.0516,
      "step": 3215
    },
    {
      "epoch": 3.903030303030303,
      "grad_norm": 0.07737286388874054,
      "learning_rate": 2e-05,
      "loss": 0.048,
      "step": 3220
    },
    {
      "epoch": 3.909090909090909,
      "grad_norm": 0.08297255635261536,
      "learning_rate": 2e-05,
      "loss": 0.0464,
      "step": 3225
    },
    {
      "epoch": 3.915151515151515,
      "grad_norm": 0.07381986081600189,
      "learning_rate": 2e-05,
      "loss": 0.0462,
      "step": 3230
    },
    {
      "epoch": 3.9212121212121214,
      "grad_norm": 0.07922395318746567,
      "learning_rate": 2e-05,
      "loss": 0.0461,
      "step": 3235
    },
    {
      "epoch": 3.9272727272727272,
      "grad_norm": 0.09830028563737869,
      "learning_rate": 2e-05,
      "loss": 0.0483,
      "step": 3240
    },
    {
      "epoch": 3.933333333333333,
      "grad_norm": 0.07537075132131577,
      "learning_rate": 2e-05,
      "loss": 0.0475,
      "step": 3245
    },
    {
      "epoch": 3.9393939393939394,
      "grad_norm": 0.08197235316038132,
      "learning_rate": 2e-05,
      "loss": 0.0463,
      "step": 3250
    },
    {
      "epoch": 3.9454545454545453,
      "grad_norm": 0.0781550258398056,
      "learning_rate": 2e-05,
      "loss": 0.0486,
      "step": 3255
    },
    {
      "epoch": 3.9515151515151516,
      "grad_norm": 0.08050331473350525,
      "learning_rate": 2e-05,
      "loss": 0.0476,
      "step": 3260
    },
    {
      "epoch": 3.9575757575757575,
      "grad_norm": 0.06438468396663666,
      "learning_rate": 2e-05,
      "loss": 0.0475,
      "step": 3265
    },
    {
      "epoch": 3.963636363636364,
      "grad_norm": 0.0999208390712738,
      "learning_rate": 2e-05,
      "loss": 0.0478,
      "step": 3270
    },
    {
      "epoch": 3.9696969696969697,
      "grad_norm": 0.07363041490316391,
      "learning_rate": 2e-05,
      "loss": 0.0467,
      "step": 3275
    },
    {
      "epoch": 3.9757575757575756,
      "grad_norm": 0.07046134769916534,
      "learning_rate": 2e-05,
      "loss": 0.0473,
      "step": 3280
    },
    {
      "epoch": 3.981818181818182,
      "grad_norm": 0.07522117346525192,
      "learning_rate": 2e-05,
      "loss": 0.0466,
      "step": 3285
    },
    {
      "epoch": 3.987878787878788,
      "grad_norm": 0.08812962472438812,
      "learning_rate": 2e-05,
      "loss": 0.0469,
      "step": 3290
    },
    {
      "epoch": 3.993939393939394,
      "grad_norm": 0.0763944461941719,
      "learning_rate": 2e-05,
      "loss": 0.0501,
      "step": 3295
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.0690586045384407,
      "learning_rate": 2e-05,
      "loss": 0.0468,
      "step": 3300
    },
    {
      "epoch": 4.0,
      "eval_average": 0.4646680254629328,
      "eval_crossner_ai": 0.43777777772875603,
      "eval_crossner_literature": 0.5467128027180453,
      "eval_crossner_music": 0.6116165149954008,
      "eval_crossner_politics": 0.5511684125204391,
      "eval_crossner_science": 0.5632065775452324,
      "eval_mit-movie": 0.3333333332843085,
      "eval_mit-restaurant": 0.20886075944834764,
      "eval_runtime": 36.3148,
      "eval_samples_per_second": 19.276,
      "eval_steps_per_second": 0.193,
      "step": 3300
    },
    {
      "epoch": 4.006060606060606,
      "grad_norm": 0.0724700540304184,
      "learning_rate": 2e-05,
      "loss": 0.0396,
      "step": 3305
    },
    {
      "epoch": 4.012121212121212,
      "grad_norm": 0.0799122080206871,
      "learning_rate": 2e-05,
      "loss": 0.0443,
      "step": 3310
    },
    {
      "epoch": 4.0181818181818185,
      "grad_norm": 0.08381908386945724,
      "learning_rate": 2e-05,
      "loss": 0.0475,
      "step": 3315
    },
    {
      "epoch": 4.024242424242424,
      "grad_norm": 0.10882685333490372,
      "learning_rate": 2e-05,
      "loss": 0.0479,
      "step": 3320
    },
    {
      "epoch": 4.03030303030303,
      "grad_norm": 0.1004105880856514,
      "learning_rate": 2e-05,
      "loss": 0.0468,
      "step": 3325
    },
    {
      "epoch": 4.036363636363636,
      "grad_norm": 0.09909500926733017,
      "learning_rate": 2e-05,
      "loss": 0.0481,
      "step": 3330
    },
    {
      "epoch": 4.042424242424242,
      "grad_norm": 0.08123811334371567,
      "learning_rate": 2e-05,
      "loss": 0.0458,
      "step": 3335
    },
    {
      "epoch": 4.048484848484849,
      "grad_norm": 0.08097165822982788,
      "learning_rate": 2e-05,
      "loss": 0.0481,
      "step": 3340
    },
    {
      "epoch": 4.054545454545455,
      "grad_norm": 0.0677558183670044,
      "learning_rate": 2e-05,
      "loss": 0.0492,
      "step": 3345
    },
    {
      "epoch": 4.0606060606060606,
      "grad_norm": 0.08239343762397766,
      "learning_rate": 2e-05,
      "loss": 0.0475,
      "step": 3350
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 0.0906524658203125,
      "learning_rate": 2e-05,
      "loss": 0.0467,
      "step": 3355
    },
    {
      "epoch": 4.072727272727272,
      "grad_norm": 0.08142191916704178,
      "learning_rate": 2e-05,
      "loss": 0.0444,
      "step": 3360
    },
    {
      "epoch": 4.078787878787879,
      "grad_norm": 0.08429724723100662,
      "learning_rate": 2e-05,
      "loss": 0.0477,
      "step": 3365
    },
    {
      "epoch": 4.084848484848485,
      "grad_norm": 0.07937217503786087,
      "learning_rate": 2e-05,
      "loss": 0.0448,
      "step": 3370
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 0.07627823948860168,
      "learning_rate": 2e-05,
      "loss": 0.046,
      "step": 3375
    },
    {
      "epoch": 4.096969696969697,
      "grad_norm": 0.06994131207466125,
      "learning_rate": 2e-05,
      "loss": 0.0474,
      "step": 3380
    },
    {
      "epoch": 4.1030303030303035,
      "grad_norm": 0.07673875987529755,
      "learning_rate": 2e-05,
      "loss": 0.0466,
      "step": 3385
    },
    {
      "epoch": 4.109090909090909,
      "grad_norm": 0.07390077412128448,
      "learning_rate": 2e-05,
      "loss": 0.0463,
      "step": 3390
    },
    {
      "epoch": 4.115151515151515,
      "grad_norm": 0.05786740779876709,
      "learning_rate": 2e-05,
      "loss": 0.0454,
      "step": 3395
    },
    {
      "epoch": 4.121212121212121,
      "grad_norm": 0.08634071797132492,
      "learning_rate": 2e-05,
      "loss": 0.0467,
      "step": 3400
    },
    {
      "epoch": 4.127272727272727,
      "grad_norm": 0.08071518689393997,
      "learning_rate": 2e-05,
      "loss": 0.0457,
      "step": 3405
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 0.09778688848018646,
      "learning_rate": 2e-05,
      "loss": 0.0471,
      "step": 3410
    },
    {
      "epoch": 4.13939393939394,
      "grad_norm": 0.10564505308866501,
      "learning_rate": 2e-05,
      "loss": 0.0485,
      "step": 3415
    },
    {
      "epoch": 4.1454545454545455,
      "grad_norm": 0.08134020119905472,
      "learning_rate": 2e-05,
      "loss": 0.0455,
      "step": 3420
    },
    {
      "epoch": 4.151515151515151,
      "grad_norm": 0.06427061557769775,
      "learning_rate": 2e-05,
      "loss": 0.0462,
      "step": 3425
    },
    {
      "epoch": 4.157575757575757,
      "grad_norm": 0.09235706180334091,
      "learning_rate": 2e-05,
      "loss": 0.0465,
      "step": 3430
    },
    {
      "epoch": 4.163636363636364,
      "grad_norm": 0.12693141400814056,
      "learning_rate": 2e-05,
      "loss": 0.0455,
      "step": 3435
    },
    {
      "epoch": 4.16969696969697,
      "grad_norm": 0.0752534493803978,
      "learning_rate": 2e-05,
      "loss": 0.0472,
      "step": 3440
    },
    {
      "epoch": 4.175757575757576,
      "grad_norm": 0.08060523122549057,
      "learning_rate": 2e-05,
      "loss": 0.0468,
      "step": 3445
    },
    {
      "epoch": 4.181818181818182,
      "grad_norm": 0.08303526788949966,
      "learning_rate": 2e-05,
      "loss": 0.0456,
      "step": 3450
    },
    {
      "epoch": 4.1878787878787875,
      "grad_norm": 0.0722622498869896,
      "learning_rate": 2e-05,
      "loss": 0.0444,
      "step": 3455
    },
    {
      "epoch": 4.193939393939394,
      "grad_norm": 0.0734100341796875,
      "learning_rate": 2e-05,
      "loss": 0.0483,
      "step": 3460
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.0738818421959877,
      "learning_rate": 2e-05,
      "loss": 0.0452,
      "step": 3465
    },
    {
      "epoch": 4.206060606060606,
      "grad_norm": 0.05851128324866295,
      "learning_rate": 2e-05,
      "loss": 0.0465,
      "step": 3470
    },
    {
      "epoch": 4.212121212121212,
      "grad_norm": 0.06442583352327347,
      "learning_rate": 2e-05,
      "loss": 0.0502,
      "step": 3475
    },
    {
      "epoch": 4.218181818181818,
      "grad_norm": 0.07720508426427841,
      "learning_rate": 2e-05,
      "loss": 0.0504,
      "step": 3480
    },
    {
      "epoch": 4.224242424242425,
      "grad_norm": 0.07996121048927307,
      "learning_rate": 2e-05,
      "loss": 0.044,
      "step": 3485
    },
    {
      "epoch": 4.2303030303030305,
      "grad_norm": 0.08518671244382858,
      "learning_rate": 2e-05,
      "loss": 0.0452,
      "step": 3490
    },
    {
      "epoch": 4.236363636363636,
      "grad_norm": 0.07923700660467148,
      "learning_rate": 2e-05,
      "loss": 0.0463,
      "step": 3495
    },
    {
      "epoch": 4.242424242424242,
      "grad_norm": 0.07327400147914886,
      "learning_rate": 2e-05,
      "loss": 0.0449,
      "step": 3500
    },
    {
      "epoch": 4.248484848484848,
      "grad_norm": 0.1271580457687378,
      "learning_rate": 2e-05,
      "loss": 0.045,
      "step": 3505
    },
    {
      "epoch": 4.254545454545455,
      "grad_norm": 0.10979580879211426,
      "learning_rate": 2e-05,
      "loss": 0.0467,
      "step": 3510
    },
    {
      "epoch": 4.260606060606061,
      "grad_norm": 0.0916297510266304,
      "learning_rate": 2e-05,
      "loss": 0.0472,
      "step": 3515
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.0653260201215744,
      "learning_rate": 2e-05,
      "loss": 0.0439,
      "step": 3520
    },
    {
      "epoch": 4.2727272727272725,
      "grad_norm": 0.06492875516414642,
      "learning_rate": 2e-05,
      "loss": 0.0456,
      "step": 3525
    },
    {
      "epoch": 4.278787878787878,
      "grad_norm": 0.07824713736772537,
      "learning_rate": 2e-05,
      "loss": 0.0464,
      "step": 3530
    },
    {
      "epoch": 4.284848484848485,
      "grad_norm": 0.06946180760860443,
      "learning_rate": 2e-05,
      "loss": 0.0461,
      "step": 3535
    },
    {
      "epoch": 4.290909090909091,
      "grad_norm": 0.08410577476024628,
      "learning_rate": 2e-05,
      "loss": 0.0447,
      "step": 3540
    },
    {
      "epoch": 4.296969696969697,
      "grad_norm": 0.09412223845720291,
      "learning_rate": 2e-05,
      "loss": 0.0472,
      "step": 3545
    },
    {
      "epoch": 4.303030303030303,
      "grad_norm": 0.06824109703302383,
      "learning_rate": 2e-05,
      "loss": 0.0447,
      "step": 3550
    },
    {
      "epoch": 4.3090909090909095,
      "grad_norm": 0.08673971146345139,
      "learning_rate": 2e-05,
      "loss": 0.0448,
      "step": 3555
    },
    {
      "epoch": 4.315151515151515,
      "grad_norm": 0.07689904421567917,
      "learning_rate": 2e-05,
      "loss": 0.0483,
      "step": 3560
    },
    {
      "epoch": 4.321212121212121,
      "grad_norm": 0.07512082904577255,
      "learning_rate": 2e-05,
      "loss": 0.0461,
      "step": 3565
    },
    {
      "epoch": 4.327272727272727,
      "grad_norm": 0.12600477039813995,
      "learning_rate": 2e-05,
      "loss": 0.043,
      "step": 3570
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.07519534975290298,
      "learning_rate": 2e-05,
      "loss": 0.0463,
      "step": 3575
    },
    {
      "epoch": 4.33939393939394,
      "grad_norm": 0.09329197555780411,
      "learning_rate": 2e-05,
      "loss": 0.0472,
      "step": 3580
    },
    {
      "epoch": 4.345454545454546,
      "grad_norm": 0.08193442225456238,
      "learning_rate": 2e-05,
      "loss": 0.0463,
      "step": 3585
    },
    {
      "epoch": 4.351515151515152,
      "grad_norm": 0.07059337198734283,
      "learning_rate": 2e-05,
      "loss": 0.0443,
      "step": 3590
    },
    {
      "epoch": 4.357575757575757,
      "grad_norm": 0.08575456589460373,
      "learning_rate": 2e-05,
      "loss": 0.0474,
      "step": 3595
    },
    {
      "epoch": 4.363636363636363,
      "grad_norm": 0.08754952251911163,
      "learning_rate": 2e-05,
      "loss": 0.0471,
      "step": 3600
    },
    {
      "epoch": 4.36969696969697,
      "grad_norm": 0.08199142664670944,
      "learning_rate": 2e-05,
      "loss": 0.045,
      "step": 3605
    },
    {
      "epoch": 4.375757575757576,
      "grad_norm": 0.09185004979372025,
      "learning_rate": 2e-05,
      "loss": 0.0449,
      "step": 3610
    },
    {
      "epoch": 4.381818181818182,
      "grad_norm": 0.09693408012390137,
      "learning_rate": 2e-05,
      "loss": 0.0462,
      "step": 3615
    },
    {
      "epoch": 4.387878787878788,
      "grad_norm": 0.08284078538417816,
      "learning_rate": 2e-05,
      "loss": 0.0457,
      "step": 3620
    },
    {
      "epoch": 4.393939393939394,
      "grad_norm": 0.06654756516218185,
      "learning_rate": 2e-05,
      "loss": 0.0471,
      "step": 3625
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.09044944494962692,
      "learning_rate": 2e-05,
      "loss": 0.0461,
      "step": 3630
    },
    {
      "epoch": 4.406060606060606,
      "grad_norm": 0.08633841574192047,
      "learning_rate": 2e-05,
      "loss": 0.0446,
      "step": 3635
    },
    {
      "epoch": 4.412121212121212,
      "grad_norm": 0.11560870707035065,
      "learning_rate": 2e-05,
      "loss": 0.0459,
      "step": 3640
    },
    {
      "epoch": 4.418181818181818,
      "grad_norm": 0.07650332897901535,
      "learning_rate": 2e-05,
      "loss": 0.0433,
      "step": 3645
    },
    {
      "epoch": 4.424242424242424,
      "grad_norm": 0.0811874270439148,
      "learning_rate": 2e-05,
      "loss": 0.0447,
      "step": 3650
    },
    {
      "epoch": 4.430303030303031,
      "grad_norm": 0.07512960582971573,
      "learning_rate": 2e-05,
      "loss": 0.0448,
      "step": 3655
    },
    {
      "epoch": 4.4363636363636365,
      "grad_norm": 0.07004439830780029,
      "learning_rate": 2e-05,
      "loss": 0.045,
      "step": 3660
    },
    {
      "epoch": 4.442424242424242,
      "grad_norm": 0.06619708240032196,
      "learning_rate": 2e-05,
      "loss": 0.0469,
      "step": 3665
    },
    {
      "epoch": 4.448484848484848,
      "grad_norm": 0.07404869794845581,
      "learning_rate": 2e-05,
      "loss": 0.0474,
      "step": 3670
    },
    {
      "epoch": 4.454545454545454,
      "grad_norm": 0.06692071259021759,
      "learning_rate": 2e-05,
      "loss": 0.0418,
      "step": 3675
    },
    {
      "epoch": 4.460606060606061,
      "grad_norm": 0.08609607070684433,
      "learning_rate": 2e-05,
      "loss": 0.0456,
      "step": 3680
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 0.06324666738510132,
      "learning_rate": 2e-05,
      "loss": 0.0447,
      "step": 3685
    },
    {
      "epoch": 4.472727272727273,
      "grad_norm": 0.08086559921503067,
      "learning_rate": 2e-05,
      "loss": 0.0456,
      "step": 3690
    },
    {
      "epoch": 4.4787878787878785,
      "grad_norm": 0.09036116302013397,
      "learning_rate": 2e-05,
      "loss": 0.0465,
      "step": 3695
    },
    {
      "epoch": 4.484848484848484,
      "grad_norm": 0.07502574473619461,
      "learning_rate": 2e-05,
      "loss": 0.0458,
      "step": 3700
    },
    {
      "epoch": 4.490909090909091,
      "grad_norm": 0.07950283586978912,
      "learning_rate": 2e-05,
      "loss": 0.0461,
      "step": 3705
    },
    {
      "epoch": 4.496969696969697,
      "grad_norm": 0.10214585065841675,
      "learning_rate": 2e-05,
      "loss": 0.0463,
      "step": 3710
    },
    {
      "epoch": 4.499393939393939,
      "eval_average": 0.480907984824292,
      "eval_crossner_ai": 0.44198895022729123,
      "eval_crossner_literature": 0.552511415474988,
      "eval_crossner_music": 0.607242339782786,
      "eval_crossner_politics": 0.5254777069563363,
      "eval_crossner_science": 0.5924412665487745,
      "eval_mit-movie": 0.3966942148265829,
      "eval_mit-restaurant": 0.24999999995328492,
      "eval_runtime": 36.0478,
      "eval_samples_per_second": 19.419,
      "eval_steps_per_second": 0.194,
      "step": 3712
    },
    {
      "epoch": 4.503030303030303,
      "grad_norm": 0.0971822664141655,
      "learning_rate": 2e-05,
      "loss": 0.0431,
      "step": 3715
    },
    {
      "epoch": 4.509090909090909,
      "grad_norm": 0.0869428887963295,
      "learning_rate": 2e-05,
      "loss": 0.0457,
      "step": 3720
    },
    {
      "epoch": 4.515151515151516,
      "grad_norm": 0.08284063637256622,
      "learning_rate": 2e-05,
      "loss": 0.0454,
      "step": 3725
    },
    {
      "epoch": 4.5212121212121215,
      "grad_norm": 0.08091150224208832,
      "learning_rate": 2e-05,
      "loss": 0.0462,
      "step": 3730
    },
    {
      "epoch": 4.527272727272727,
      "grad_norm": 0.10945332795381546,
      "learning_rate": 2e-05,
      "loss": 0.0434,
      "step": 3735
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.09133491665124893,
      "learning_rate": 2e-05,
      "loss": 0.0456,
      "step": 3740
    },
    {
      "epoch": 4.539393939393939,
      "grad_norm": 0.08159004896879196,
      "learning_rate": 2e-05,
      "loss": 0.0464,
      "step": 3745
    },
    {
      "epoch": 4.545454545454546,
      "grad_norm": 0.06154409050941467,
      "learning_rate": 2e-05,
      "loss": 0.0431,
      "step": 3750
    },
    {
      "epoch": 4.551515151515152,
      "grad_norm": 0.07543080300092697,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 3755
    },
    {
      "epoch": 4.557575757575758,
      "grad_norm": 0.0826060026884079,
      "learning_rate": 2e-05,
      "loss": 0.0451,
      "step": 3760
    },
    {
      "epoch": 4.5636363636363635,
      "grad_norm": 0.06397371739149094,
      "learning_rate": 2e-05,
      "loss": 0.0455,
      "step": 3765
    },
    {
      "epoch": 4.569696969696969,
      "grad_norm": 0.0692860558629036,
      "learning_rate": 2e-05,
      "loss": 0.0456,
      "step": 3770
    },
    {
      "epoch": 4.575757575757576,
      "grad_norm": 0.06842461228370667,
      "learning_rate": 2e-05,
      "loss": 0.0436,
      "step": 3775
    },
    {
      "epoch": 4.581818181818182,
      "grad_norm": 0.07655875384807587,
      "learning_rate": 2e-05,
      "loss": 0.0458,
      "step": 3780
    },
    {
      "epoch": 4.587878787878788,
      "grad_norm": 0.06504153460264206,
      "learning_rate": 2e-05,
      "loss": 0.0454,
      "step": 3785
    },
    {
      "epoch": 4.593939393939394,
      "grad_norm": 0.08506596833467484,
      "learning_rate": 2e-05,
      "loss": 0.0461,
      "step": 3790
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.14419548213481903,
      "learning_rate": 2e-05,
      "loss": 0.0466,
      "step": 3795
    },
    {
      "epoch": 4.606060606060606,
      "grad_norm": 0.09294909238815308,
      "learning_rate": 2e-05,
      "loss": 0.0438,
      "step": 3800
    },
    {
      "epoch": 4.612121212121212,
      "grad_norm": 0.07213472574949265,
      "learning_rate": 2e-05,
      "loss": 0.0456,
      "step": 3805
    },
    {
      "epoch": 4.618181818181818,
      "grad_norm": 0.10139923542737961,
      "learning_rate": 2e-05,
      "loss": 0.045,
      "step": 3810
    },
    {
      "epoch": 4.624242424242424,
      "grad_norm": 0.06570515036582947,
      "learning_rate": 2e-05,
      "loss": 0.0455,
      "step": 3815
    },
    {
      "epoch": 4.63030303030303,
      "grad_norm": 0.0753818079829216,
      "learning_rate": 2e-05,
      "loss": 0.0452,
      "step": 3820
    },
    {
      "epoch": 4.636363636363637,
      "grad_norm": 0.07444991916418076,
      "learning_rate": 2e-05,
      "loss": 0.0451,
      "step": 3825
    },
    {
      "epoch": 4.642424242424243,
      "grad_norm": 0.06732432544231415,
      "learning_rate": 2e-05,
      "loss": 0.0449,
      "step": 3830
    },
    {
      "epoch": 4.648484848484848,
      "grad_norm": 0.0820240005850792,
      "learning_rate": 2e-05,
      "loss": 0.0473,
      "step": 3835
    },
    {
      "epoch": 4.654545454545454,
      "grad_norm": 0.07585174590349197,
      "learning_rate": 2e-05,
      "loss": 0.0438,
      "step": 3840
    },
    {
      "epoch": 4.66060606060606,
      "grad_norm": 0.07231801748275757,
      "learning_rate": 2e-05,
      "loss": 0.0442,
      "step": 3845
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.09444718062877655,
      "learning_rate": 2e-05,
      "loss": 0.0444,
      "step": 3850
    },
    {
      "epoch": 4.672727272727273,
      "grad_norm": 0.09351451694965363,
      "learning_rate": 2e-05,
      "loss": 0.0436,
      "step": 3855
    },
    {
      "epoch": 4.678787878787879,
      "grad_norm": 0.05958640202879906,
      "learning_rate": 2e-05,
      "loss": 0.0459,
      "step": 3860
    },
    {
      "epoch": 4.684848484848485,
      "grad_norm": 0.0738256499171257,
      "learning_rate": 2e-05,
      "loss": 0.0444,
      "step": 3865
    },
    {
      "epoch": 4.6909090909090905,
      "grad_norm": 0.08637506514787674,
      "learning_rate": 2e-05,
      "loss": 0.0437,
      "step": 3870
    },
    {
      "epoch": 4.696969696969697,
      "grad_norm": 0.10186227411031723,
      "learning_rate": 2e-05,
      "loss": 0.0453,
      "step": 3875
    },
    {
      "epoch": 4.703030303030303,
      "grad_norm": 0.11437743902206421,
      "learning_rate": 2e-05,
      "loss": 0.0463,
      "step": 3880
    },
    {
      "epoch": 4.709090909090909,
      "grad_norm": 0.06535643339157104,
      "learning_rate": 2e-05,
      "loss": 0.0446,
      "step": 3885
    },
    {
      "epoch": 4.715151515151515,
      "grad_norm": 0.0808338075876236,
      "learning_rate": 2e-05,
      "loss": 0.0454,
      "step": 3890
    },
    {
      "epoch": 4.721212121212122,
      "grad_norm": 0.10203619301319122,
      "learning_rate": 2e-05,
      "loss": 0.0433,
      "step": 3895
    },
    {
      "epoch": 4.7272727272727275,
      "grad_norm": 0.07289847731590271,
      "learning_rate": 2e-05,
      "loss": 0.0446,
      "step": 3900
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 0.06605684757232666,
      "learning_rate": 2e-05,
      "loss": 0.0425,
      "step": 3905
    },
    {
      "epoch": 4.739393939393939,
      "grad_norm": 0.07102695107460022,
      "learning_rate": 2e-05,
      "loss": 0.0453,
      "step": 3910
    },
    {
      "epoch": 4.745454545454545,
      "grad_norm": 0.0676354244351387,
      "learning_rate": 2e-05,
      "loss": 0.0455,
      "step": 3915
    },
    {
      "epoch": 4.751515151515152,
      "grad_norm": 0.0786483883857727,
      "learning_rate": 2e-05,
      "loss": 0.0462,
      "step": 3920
    },
    {
      "epoch": 4.757575757575758,
      "grad_norm": 0.07867243140935898,
      "learning_rate": 2e-05,
      "loss": 0.046,
      "step": 3925
    },
    {
      "epoch": 4.763636363636364,
      "grad_norm": 0.05716538801789284,
      "learning_rate": 2e-05,
      "loss": 0.0452,
      "step": 3930
    },
    {
      "epoch": 4.7696969696969695,
      "grad_norm": 0.05262979492545128,
      "learning_rate": 2e-05,
      "loss": 0.043,
      "step": 3935
    },
    {
      "epoch": 4.775757575757575,
      "grad_norm": 0.0714258998632431,
      "learning_rate": 2e-05,
      "loss": 0.0449,
      "step": 3940
    },
    {
      "epoch": 4.781818181818182,
      "grad_norm": 0.06983095407485962,
      "learning_rate": 2e-05,
      "loss": 0.0436,
      "step": 3945
    },
    {
      "epoch": 4.787878787878788,
      "grad_norm": 0.07955582439899445,
      "learning_rate": 2e-05,
      "loss": 0.0456,
      "step": 3950
    },
    {
      "epoch": 4.793939393939394,
      "grad_norm": 0.100638248026371,
      "learning_rate": 2e-05,
      "loss": 0.0427,
      "step": 3955
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.08117347955703735,
      "learning_rate": 2e-05,
      "loss": 0.044,
      "step": 3960
    },
    {
      "epoch": 4.806060606060606,
      "grad_norm": 0.10048218071460724,
      "learning_rate": 2e-05,
      "loss": 0.0459,
      "step": 3965
    },
    {
      "epoch": 4.8121212121212125,
      "grad_norm": 0.05806518346071243,
      "learning_rate": 2e-05,
      "loss": 0.0453,
      "step": 3970
    },
    {
      "epoch": 4.818181818181818,
      "grad_norm": 0.079307921230793,
      "learning_rate": 2e-05,
      "loss": 0.0453,
      "step": 3975
    },
    {
      "epoch": 4.824242424242424,
      "grad_norm": 0.0865771546959877,
      "learning_rate": 2e-05,
      "loss": 0.0444,
      "step": 3980
    },
    {
      "epoch": 4.83030303030303,
      "grad_norm": 0.0726611316204071,
      "learning_rate": 2e-05,
      "loss": 0.0444,
      "step": 3985
    },
    {
      "epoch": 4.836363636363636,
      "grad_norm": 0.10857509076595306,
      "learning_rate": 2e-05,
      "loss": 0.0444,
      "step": 3990
    },
    {
      "epoch": 4.842424242424243,
      "grad_norm": 0.08460558950901031,
      "learning_rate": 2e-05,
      "loss": 0.0457,
      "step": 3995
    },
    {
      "epoch": 4.848484848484849,
      "grad_norm": 0.07552092522382736,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 4000
    },
    {
      "epoch": 4.8545454545454545,
      "grad_norm": 0.06862561404705048,
      "learning_rate": 2e-05,
      "loss": 0.0434,
      "step": 4005
    },
    {
      "epoch": 4.86060606060606,
      "grad_norm": 0.0664498507976532,
      "learning_rate": 2e-05,
      "loss": 0.0456,
      "step": 4010
    },
    {
      "epoch": 4.866666666666666,
      "grad_norm": 0.07826050370931625,
      "learning_rate": 2e-05,
      "loss": 0.0417,
      "step": 4015
    },
    {
      "epoch": 4.872727272727273,
      "grad_norm": 0.07846997678279877,
      "learning_rate": 2e-05,
      "loss": 0.0449,
      "step": 4020
    },
    {
      "epoch": 4.878787878787879,
      "grad_norm": 0.07405579090118408,
      "learning_rate": 2e-05,
      "loss": 0.044,
      "step": 4025
    },
    {
      "epoch": 4.884848484848485,
      "grad_norm": 0.06333499401807785,
      "learning_rate": 2e-05,
      "loss": 0.0457,
      "step": 4030
    },
    {
      "epoch": 4.890909090909091,
      "grad_norm": 0.08257922530174255,
      "learning_rate": 2e-05,
      "loss": 0.0431,
      "step": 4035
    },
    {
      "epoch": 4.8969696969696965,
      "grad_norm": 0.08381843566894531,
      "learning_rate": 2e-05,
      "loss": 0.0439,
      "step": 4040
    },
    {
      "epoch": 4.903030303030303,
      "grad_norm": 0.07910404354333878,
      "learning_rate": 2e-05,
      "loss": 0.0463,
      "step": 4045
    },
    {
      "epoch": 4.909090909090909,
      "grad_norm": 0.08005142956972122,
      "learning_rate": 2e-05,
      "loss": 0.0443,
      "step": 4050
    },
    {
      "epoch": 4.915151515151515,
      "grad_norm": 0.08261856436729431,
      "learning_rate": 2e-05,
      "loss": 0.0472,
      "step": 4055
    },
    {
      "epoch": 4.921212121212121,
      "grad_norm": 0.06599439680576324,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 4060
    },
    {
      "epoch": 4.927272727272728,
      "grad_norm": 0.089924156665802,
      "learning_rate": 2e-05,
      "loss": 0.0429,
      "step": 4065
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 0.0649292916059494,
      "learning_rate": 2e-05,
      "loss": 0.0443,
      "step": 4070
    },
    {
      "epoch": 4.9393939393939394,
      "grad_norm": 0.0891399160027504,
      "learning_rate": 2e-05,
      "loss": 0.0459,
      "step": 4075
    },
    {
      "epoch": 4.945454545454545,
      "grad_norm": 0.07314775139093399,
      "learning_rate": 2e-05,
      "loss": 0.0441,
      "step": 4080
    },
    {
      "epoch": 4.951515151515151,
      "grad_norm": 0.0791521966457367,
      "learning_rate": 2e-05,
      "loss": 0.0436,
      "step": 4085
    },
    {
      "epoch": 4.957575757575758,
      "grad_norm": 0.11315857619047165,
      "learning_rate": 2e-05,
      "loss": 0.0444,
      "step": 4090
    },
    {
      "epoch": 4.963636363636364,
      "grad_norm": 0.06539865583181381,
      "learning_rate": 2e-05,
      "loss": 0.0446,
      "step": 4095
    },
    {
      "epoch": 4.96969696969697,
      "grad_norm": 0.08987577259540558,
      "learning_rate": 2e-05,
      "loss": 0.0436,
      "step": 4100
    },
    {
      "epoch": 4.975757575757576,
      "grad_norm": 0.09876946359872818,
      "learning_rate": 2e-05,
      "loss": 0.0445,
      "step": 4105
    },
    {
      "epoch": 4.9818181818181815,
      "grad_norm": 0.07390950620174408,
      "learning_rate": 2e-05,
      "loss": 0.0457,
      "step": 4110
    },
    {
      "epoch": 4.987878787878788,
      "grad_norm": 0.07600279152393341,
      "learning_rate": 2e-05,
      "loss": 0.0453,
      "step": 4115
    },
    {
      "epoch": 4.993939393939394,
      "grad_norm": 0.09627139568328857,
      "learning_rate": 2e-05,
      "loss": 0.045,
      "step": 4120
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.06506019085645676,
      "learning_rate": 2e-05,
      "loss": 0.0485,
      "step": 4125
    },
    {
      "epoch": 5.0,
      "eval_average": 0.49493951158183186,
      "eval_crossner_ai": 0.47005649712590897,
      "eval_crossner_literature": 0.5757225433024761,
      "eval_crossner_music": 0.6287300485272936,
      "eval_crossner_politics": 0.54370489169012,
      "eval_crossner_science": 0.6099585061741603,
      "eval_mit-movie": 0.380952380903188,
      "eval_mit-restaurant": 0.2554517133496763,
      "eval_runtime": 36.1789,
      "eval_samples_per_second": 19.348,
      "eval_steps_per_second": 0.193,
      "step": 4125
    },
    {
      "epoch": 5.006060606060606,
      "grad_norm": 0.10341857373714447,
      "learning_rate": 2e-05,
      "loss": 0.0413,
      "step": 4130
    },
    {
      "epoch": 5.012121212121212,
      "grad_norm": 0.06759698688983917,
      "learning_rate": 2e-05,
      "loss": 0.0441,
      "step": 4135
    },
    {
      "epoch": 5.0181818181818185,
      "grad_norm": 0.09344696998596191,
      "learning_rate": 2e-05,
      "loss": 0.0418,
      "step": 4140
    },
    {
      "epoch": 5.024242424242424,
      "grad_norm": 0.08327244222164154,
      "learning_rate": 2e-05,
      "loss": 0.0445,
      "step": 4145
    },
    {
      "epoch": 5.03030303030303,
      "grad_norm": 0.07104939967393875,
      "learning_rate": 2e-05,
      "loss": 0.0431,
      "step": 4150
    },
    {
      "epoch": 5.036363636363636,
      "grad_norm": 0.0900537446141243,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 4155
    },
    {
      "epoch": 5.042424242424242,
      "grad_norm": 0.11579758673906326,
      "learning_rate": 2e-05,
      "loss": 0.0439,
      "step": 4160
    },
    {
      "epoch": 5.048484848484849,
      "grad_norm": 0.06914803385734558,
      "learning_rate": 2e-05,
      "loss": 0.0447,
      "step": 4165
    },
    {
      "epoch": 5.054545454545455,
      "grad_norm": 0.07682763785123825,
      "learning_rate": 2e-05,
      "loss": 0.0419,
      "step": 4170
    },
    {
      "epoch": 5.0606060606060606,
      "grad_norm": 0.09263328462839127,
      "learning_rate": 2e-05,
      "loss": 0.0431,
      "step": 4175
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 0.07812339067459106,
      "learning_rate": 2e-05,
      "loss": 0.0434,
      "step": 4180
    },
    {
      "epoch": 5.072727272727272,
      "grad_norm": 0.057678911834955215,
      "learning_rate": 2e-05,
      "loss": 0.0438,
      "step": 4185
    },
    {
      "epoch": 5.078787878787879,
      "grad_norm": 0.06788688153028488,
      "learning_rate": 2e-05,
      "loss": 0.045,
      "step": 4190
    },
    {
      "epoch": 5.084848484848485,
      "grad_norm": 0.08875338733196259,
      "learning_rate": 2e-05,
      "loss": 0.0428,
      "step": 4195
    },
    {
      "epoch": 5.090909090909091,
      "grad_norm": 0.06960246711969376,
      "learning_rate": 2e-05,
      "loss": 0.0421,
      "step": 4200
    },
    {
      "epoch": 5.096969696969697,
      "grad_norm": 0.08333971351385117,
      "learning_rate": 2e-05,
      "loss": 0.0454,
      "step": 4205
    },
    {
      "epoch": 5.1030303030303035,
      "grad_norm": 0.071959488093853,
      "learning_rate": 2e-05,
      "loss": 0.0445,
      "step": 4210
    },
    {
      "epoch": 5.109090909090909,
      "grad_norm": 0.07212013006210327,
      "learning_rate": 2e-05,
      "loss": 0.0424,
      "step": 4215
    },
    {
      "epoch": 5.115151515151515,
      "grad_norm": 0.07415523380041122,
      "learning_rate": 2e-05,
      "loss": 0.0442,
      "step": 4220
    },
    {
      "epoch": 5.121212121212121,
      "grad_norm": 0.09013757854700089,
      "learning_rate": 2e-05,
      "loss": 0.044,
      "step": 4225
    },
    {
      "epoch": 5.127272727272727,
      "grad_norm": 0.058147531002759933,
      "learning_rate": 2e-05,
      "loss": 0.0418,
      "step": 4230
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 0.08847303688526154,
      "learning_rate": 2e-05,
      "loss": 0.0442,
      "step": 4235
    },
    {
      "epoch": 5.13939393939394,
      "grad_norm": 0.0676228255033493,
      "learning_rate": 2e-05,
      "loss": 0.0452,
      "step": 4240
    },
    {
      "epoch": 5.1454545454545455,
      "grad_norm": 0.09673496335744858,
      "learning_rate": 2e-05,
      "loss": 0.0451,
      "step": 4245
    },
    {
      "epoch": 5.151515151515151,
      "grad_norm": 0.0877143070101738,
      "learning_rate": 2e-05,
      "loss": 0.0428,
      "step": 4250
    },
    {
      "epoch": 5.157575757575757,
      "grad_norm": 0.09608054906129837,
      "learning_rate": 2e-05,
      "loss": 0.0438,
      "step": 4255
    },
    {
      "epoch": 5.163636363636364,
      "grad_norm": 0.08229297399520874,
      "learning_rate": 2e-05,
      "loss": 0.0474,
      "step": 4260
    },
    {
      "epoch": 5.16969696969697,
      "grad_norm": 0.07223395258188248,
      "learning_rate": 2e-05,
      "loss": 0.0444,
      "step": 4265
    },
    {
      "epoch": 5.175757575757576,
      "grad_norm": 0.06197703629732132,
      "learning_rate": 2e-05,
      "loss": 0.0432,
      "step": 4270
    },
    {
      "epoch": 5.181818181818182,
      "grad_norm": 0.07061632722616196,
      "learning_rate": 2e-05,
      "loss": 0.042,
      "step": 4275
    },
    {
      "epoch": 5.1878787878787875,
      "grad_norm": 0.1224561408162117,
      "learning_rate": 2e-05,
      "loss": 0.0443,
      "step": 4280
    },
    {
      "epoch": 5.193939393939394,
      "grad_norm": 0.0892423614859581,
      "learning_rate": 2e-05,
      "loss": 0.0433,
      "step": 4285
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.09262067079544067,
      "learning_rate": 2e-05,
      "loss": 0.0446,
      "step": 4290
    },
    {
      "epoch": 5.206060606060606,
      "grad_norm": 0.06779827177524567,
      "learning_rate": 2e-05,
      "loss": 0.0428,
      "step": 4295
    },
    {
      "epoch": 5.212121212121212,
      "grad_norm": 0.07353706657886505,
      "learning_rate": 2e-05,
      "loss": 0.042,
      "step": 4300
    },
    {
      "epoch": 5.218181818181818,
      "grad_norm": 0.05926135927438736,
      "learning_rate": 2e-05,
      "loss": 0.044,
      "step": 4305
    },
    {
      "epoch": 5.224242424242425,
      "grad_norm": 0.08019654452800751,
      "learning_rate": 2e-05,
      "loss": 0.043,
      "step": 4310
    },
    {
      "epoch": 5.2303030303030305,
      "grad_norm": 0.05243084952235222,
      "learning_rate": 2e-05,
      "loss": 0.0448,
      "step": 4315
    },
    {
      "epoch": 5.236363636363636,
      "grad_norm": 0.12163501232862473,
      "learning_rate": 2e-05,
      "loss": 0.0447,
      "step": 4320
    },
    {
      "epoch": 5.242424242424242,
      "grad_norm": 0.08718914538621902,
      "learning_rate": 2e-05,
      "loss": 0.0414,
      "step": 4325
    },
    {
      "epoch": 5.248484848484848,
      "grad_norm": 0.07116523385047913,
      "learning_rate": 2e-05,
      "loss": 0.0416,
      "step": 4330
    },
    {
      "epoch": 5.254545454545455,
      "grad_norm": 0.060914527624845505,
      "learning_rate": 2e-05,
      "loss": 0.0442,
      "step": 4335
    },
    {
      "epoch": 5.260606060606061,
      "grad_norm": 0.07220016419887543,
      "learning_rate": 2e-05,
      "loss": 0.0419,
      "step": 4340
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 0.07080906629562378,
      "learning_rate": 2e-05,
      "loss": 0.0438,
      "step": 4345
    },
    {
      "epoch": 5.2727272727272725,
      "grad_norm": 0.11158453673124313,
      "learning_rate": 2e-05,
      "loss": 0.044,
      "step": 4350
    },
    {
      "epoch": 5.278787878787878,
      "grad_norm": 0.07493796944618225,
      "learning_rate": 2e-05,
      "loss": 0.0441,
      "step": 4355
    },
    {
      "epoch": 5.284848484848485,
      "grad_norm": 0.06711626052856445,
      "learning_rate": 2e-05,
      "loss": 0.0422,
      "step": 4360
    },
    {
      "epoch": 5.290909090909091,
      "grad_norm": 0.08476518839597702,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 4365
    },
    {
      "epoch": 5.296969696969697,
      "grad_norm": 0.0963478684425354,
      "learning_rate": 2e-05,
      "loss": 0.0437,
      "step": 4370
    },
    {
      "epoch": 5.303030303030303,
      "grad_norm": 0.05718943849205971,
      "learning_rate": 2e-05,
      "loss": 0.0435,
      "step": 4375
    },
    {
      "epoch": 5.3090909090909095,
      "grad_norm": 0.07062268257141113,
      "learning_rate": 2e-05,
      "loss": 0.0435,
      "step": 4380
    },
    {
      "epoch": 5.315151515151515,
      "grad_norm": 0.08366136997938156,
      "learning_rate": 2e-05,
      "loss": 0.0425,
      "step": 4385
    },
    {
      "epoch": 5.321212121212121,
      "grad_norm": 0.0704469382762909,
      "learning_rate": 2e-05,
      "loss": 0.044,
      "step": 4390
    },
    {
      "epoch": 5.327272727272727,
      "grad_norm": 0.05899306386709213,
      "learning_rate": 2e-05,
      "loss": 0.0444,
      "step": 4395
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.08520129323005676,
      "learning_rate": 2e-05,
      "loss": 0.0428,
      "step": 4400
    },
    {
      "epoch": 5.33939393939394,
      "grad_norm": 0.06486666202545166,
      "learning_rate": 2e-05,
      "loss": 0.0429,
      "step": 4405
    },
    {
      "epoch": 5.345454545454546,
      "grad_norm": 0.07942592352628708,
      "learning_rate": 2e-05,
      "loss": 0.0433,
      "step": 4410
    },
    {
      "epoch": 5.351515151515152,
      "grad_norm": 0.07229137420654297,
      "learning_rate": 2e-05,
      "loss": 0.0411,
      "step": 4415
    },
    {
      "epoch": 5.357575757575757,
      "grad_norm": 0.0730251669883728,
      "learning_rate": 2e-05,
      "loss": 0.0437,
      "step": 4420
    },
    {
      "epoch": 5.363636363636363,
      "grad_norm": 0.06622860580682755,
      "learning_rate": 2e-05,
      "loss": 0.0454,
      "step": 4425
    },
    {
      "epoch": 5.36969696969697,
      "grad_norm": 0.08182500302791595,
      "learning_rate": 2e-05,
      "loss": 0.0428,
      "step": 4430
    },
    {
      "epoch": 5.375757575757576,
      "grad_norm": 0.07177002727985382,
      "learning_rate": 2e-05,
      "loss": 0.0444,
      "step": 4435
    },
    {
      "epoch": 5.381818181818182,
      "grad_norm": 0.08157717436552048,
      "learning_rate": 2e-05,
      "loss": 0.043,
      "step": 4440
    },
    {
      "epoch": 5.387878787878788,
      "grad_norm": 0.0703938901424408,
      "learning_rate": 2e-05,
      "loss": 0.0432,
      "step": 4445
    },
    {
      "epoch": 5.393939393939394,
      "grad_norm": 0.08380775153636932,
      "learning_rate": 2e-05,
      "loss": 0.0449,
      "step": 4450
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.0647616982460022,
      "learning_rate": 2e-05,
      "loss": 0.0435,
      "step": 4455
    },
    {
      "epoch": 5.406060606060606,
      "grad_norm": 0.09030988067388535,
      "learning_rate": 2e-05,
      "loss": 0.0432,
      "step": 4460
    },
    {
      "epoch": 5.412121212121212,
      "grad_norm": 0.07017971575260162,
      "learning_rate": 2e-05,
      "loss": 0.043,
      "step": 4465
    },
    {
      "epoch": 5.418181818181818,
      "grad_norm": 0.07042255997657776,
      "learning_rate": 2e-05,
      "loss": 0.0423,
      "step": 4470
    },
    {
      "epoch": 5.424242424242424,
      "grad_norm": 0.1008792445063591,
      "learning_rate": 2e-05,
      "loss": 0.0441,
      "step": 4475
    },
    {
      "epoch": 5.430303030303031,
      "grad_norm": 0.07300961017608643,
      "learning_rate": 2e-05,
      "loss": 0.04,
      "step": 4480
    },
    {
      "epoch": 5.4363636363636365,
      "grad_norm": 0.06424854695796967,
      "learning_rate": 2e-05,
      "loss": 0.0434,
      "step": 4485
    },
    {
      "epoch": 5.442424242424242,
      "grad_norm": 0.08124735951423645,
      "learning_rate": 2e-05,
      "loss": 0.0432,
      "step": 4490
    },
    {
      "epoch": 5.448484848484848,
      "grad_norm": 0.21625709533691406,
      "learning_rate": 2e-05,
      "loss": 0.0428,
      "step": 4495
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 0.07035007327795029,
      "learning_rate": 2e-05,
      "loss": 0.0419,
      "step": 4500
    },
    {
      "epoch": 5.460606060606061,
      "grad_norm": 0.10823701322078705,
      "learning_rate": 2e-05,
      "loss": 0.0451,
      "step": 4505
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 0.07443373650312424,
      "learning_rate": 2e-05,
      "loss": 0.0445,
      "step": 4510
    },
    {
      "epoch": 5.472727272727273,
      "grad_norm": 0.09014886617660522,
      "learning_rate": 2e-05,
      "loss": 0.0435,
      "step": 4515
    },
    {
      "epoch": 5.4787878787878785,
      "grad_norm": 0.0998278334736824,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 4520
    },
    {
      "epoch": 5.484848484848484,
      "grad_norm": 0.0659908801317215,
      "learning_rate": 2e-05,
      "loss": 0.0422,
      "step": 4525
    },
    {
      "epoch": 5.490909090909091,
      "grad_norm": 0.07750387489795685,
      "learning_rate": 2e-05,
      "loss": 0.0431,
      "step": 4530
    },
    {
      "epoch": 5.496969696969697,
      "grad_norm": 0.07897263765335083,
      "learning_rate": 2e-05,
      "loss": 0.0406,
      "step": 4535
    },
    {
      "epoch": 5.500606060606061,
      "eval_average": 0.48082046215074337,
      "eval_crossner_ai": 0.45051194534318634,
      "eval_crossner_literature": 0.5448275861567737,
      "eval_crossner_music": 0.6111111110610297,
      "eval_crossner_politics": 0.5297906601753793,
      "eval_crossner_science": 0.5882352940676726,
      "eval_mit-movie": 0.3920454544964973,
      "eval_mit-restaurant": 0.24922118375466462,
      "eval_runtime": 36.1164,
      "eval_samples_per_second": 19.382,
      "eval_steps_per_second": 0.194,
      "step": 4538
    },
    {
      "epoch": 5.503030303030303,
      "grad_norm": 0.08450009673833847,
      "learning_rate": 2e-05,
      "loss": 0.0414,
      "step": 4540
    },
    {
      "epoch": 5.509090909090909,
      "grad_norm": 0.08544646203517914,
      "learning_rate": 2e-05,
      "loss": 0.0458,
      "step": 4545
    },
    {
      "epoch": 5.515151515151516,
      "grad_norm": 0.09475193172693253,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 4550
    },
    {
      "epoch": 5.5212121212121215,
      "grad_norm": 0.12338010221719742,
      "learning_rate": 2e-05,
      "loss": 0.0424,
      "step": 4555
    },
    {
      "epoch": 5.527272727272727,
      "grad_norm": 0.07761914283037186,
      "learning_rate": 2e-05,
      "loss": 0.0439,
      "step": 4560
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 0.0789954662322998,
      "learning_rate": 2e-05,
      "loss": 0.0425,
      "step": 4565
    },
    {
      "epoch": 5.539393939393939,
      "grad_norm": 0.0767124593257904,
      "learning_rate": 2e-05,
      "loss": 0.0437,
      "step": 4570
    },
    {
      "epoch": 5.545454545454546,
      "grad_norm": 0.06898193061351776,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 4575
    },
    {
      "epoch": 5.551515151515152,
      "grad_norm": 0.07978130877017975,
      "learning_rate": 2e-05,
      "loss": 0.0429,
      "step": 4580
    },
    {
      "epoch": 5.557575757575758,
      "grad_norm": 0.06858648359775543,
      "learning_rate": 2e-05,
      "loss": 0.0407,
      "step": 4585
    },
    {
      "epoch": 5.5636363636363635,
      "grad_norm": 0.07469204813241959,
      "learning_rate": 2e-05,
      "loss": 0.0432,
      "step": 4590
    },
    {
      "epoch": 5.569696969696969,
      "grad_norm": 0.07631050795316696,
      "learning_rate": 2e-05,
      "loss": 0.0418,
      "step": 4595
    },
    {
      "epoch": 5.575757575757576,
      "grad_norm": 0.07841050624847412,
      "learning_rate": 2e-05,
      "loss": 0.0415,
      "step": 4600
    },
    {
      "epoch": 5.581818181818182,
      "grad_norm": 0.07698159664869308,
      "learning_rate": 2e-05,
      "loss": 0.0438,
      "step": 4605
    },
    {
      "epoch": 5.587878787878788,
      "grad_norm": 0.0861409455537796,
      "learning_rate": 2e-05,
      "loss": 0.0414,
      "step": 4610
    },
    {
      "epoch": 5.593939393939394,
      "grad_norm": 0.0759510025382042,
      "learning_rate": 2e-05,
      "loss": 0.0419,
      "step": 4615
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.08423160016536713,
      "learning_rate": 2e-05,
      "loss": 0.0434,
      "step": 4620
    },
    {
      "epoch": 5.606060606060606,
      "grad_norm": 0.10125710070133209,
      "learning_rate": 2e-05,
      "loss": 0.0454,
      "step": 4625
    },
    {
      "epoch": 5.612121212121212,
      "grad_norm": 0.06610909104347229,
      "learning_rate": 2e-05,
      "loss": 0.0406,
      "step": 4630
    },
    {
      "epoch": 5.618181818181818,
      "grad_norm": 0.08079002052545547,
      "learning_rate": 2e-05,
      "loss": 0.0405,
      "step": 4635
    },
    {
      "epoch": 5.624242424242424,
      "grad_norm": 0.08213961869478226,
      "learning_rate": 2e-05,
      "loss": 0.0454,
      "step": 4640
    },
    {
      "epoch": 5.63030303030303,
      "grad_norm": 0.0706721842288971,
      "learning_rate": 2e-05,
      "loss": 0.0398,
      "step": 4645
    },
    {
      "epoch": 5.636363636363637,
      "grad_norm": 0.06696821004152298,
      "learning_rate": 2e-05,
      "loss": 0.0412,
      "step": 4650
    },
    {
      "epoch": 5.642424242424243,
      "grad_norm": 0.0944649949669838,
      "learning_rate": 2e-05,
      "loss": 0.0429,
      "step": 4655
    },
    {
      "epoch": 5.648484848484848,
      "grad_norm": 0.08666690438985825,
      "learning_rate": 2e-05,
      "loss": 0.043,
      "step": 4660
    },
    {
      "epoch": 5.654545454545454,
      "grad_norm": 0.06336940079927444,
      "learning_rate": 2e-05,
      "loss": 0.0415,
      "step": 4665
    },
    {
      "epoch": 5.66060606060606,
      "grad_norm": 0.08990988880395889,
      "learning_rate": 2e-05,
      "loss": 0.0429,
      "step": 4670
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 0.07311377674341202,
      "learning_rate": 2e-05,
      "loss": 0.0443,
      "step": 4675
    },
    {
      "epoch": 5.672727272727273,
      "grad_norm": 0.09186342358589172,
      "learning_rate": 2e-05,
      "loss": 0.0428,
      "step": 4680
    },
    {
      "epoch": 5.678787878787879,
      "grad_norm": 0.07964355498552322,
      "learning_rate": 2e-05,
      "loss": 0.0451,
      "step": 4685
    },
    {
      "epoch": 5.684848484848485,
      "grad_norm": 0.08067834377288818,
      "learning_rate": 2e-05,
      "loss": 0.0413,
      "step": 4690
    },
    {
      "epoch": 5.6909090909090905,
      "grad_norm": 0.07228002697229385,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 4695
    },
    {
      "epoch": 5.696969696969697,
      "grad_norm": 0.09389597922563553,
      "learning_rate": 2e-05,
      "loss": 0.0438,
      "step": 4700
    },
    {
      "epoch": 5.703030303030303,
      "grad_norm": 0.08724804222583771,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 4705
    },
    {
      "epoch": 5.709090909090909,
      "grad_norm": 0.05854690447449684,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 4710
    },
    {
      "epoch": 5.715151515151515,
      "grad_norm": 0.07744122296571732,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 4715
    },
    {
      "epoch": 5.721212121212122,
      "grad_norm": 0.07611364126205444,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 4720
    },
    {
      "epoch": 5.7272727272727275,
      "grad_norm": 0.06687074899673462,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 4725
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 0.06434157490730286,
      "learning_rate": 2e-05,
      "loss": 0.0436,
      "step": 4730
    },
    {
      "epoch": 5.739393939393939,
      "grad_norm": 0.06627151370048523,
      "learning_rate": 2e-05,
      "loss": 0.0408,
      "step": 4735
    },
    {
      "epoch": 5.745454545454545,
      "grad_norm": 0.07946962118148804,
      "learning_rate": 2e-05,
      "loss": 0.0422,
      "step": 4740
    },
    {
      "epoch": 5.751515151515152,
      "grad_norm": 0.060792069882154465,
      "learning_rate": 2e-05,
      "loss": 0.041,
      "step": 4745
    },
    {
      "epoch": 5.757575757575758,
      "grad_norm": 0.06196894869208336,
      "learning_rate": 2e-05,
      "loss": 0.041,
      "step": 4750
    },
    {
      "epoch": 5.763636363636364,
      "grad_norm": 0.08266350626945496,
      "learning_rate": 2e-05,
      "loss": 0.0401,
      "step": 4755
    },
    {
      "epoch": 5.7696969696969695,
      "grad_norm": 0.05743752419948578,
      "learning_rate": 2e-05,
      "loss": 0.0406,
      "step": 4760
    },
    {
      "epoch": 5.775757575757575,
      "grad_norm": 0.0753088891506195,
      "learning_rate": 2e-05,
      "loss": 0.0433,
      "step": 4765
    },
    {
      "epoch": 5.781818181818182,
      "grad_norm": 0.062378350645303726,
      "learning_rate": 2e-05,
      "loss": 0.0414,
      "step": 4770
    },
    {
      "epoch": 5.787878787878788,
      "grad_norm": 0.08140760660171509,
      "learning_rate": 2e-05,
      "loss": 0.0444,
      "step": 4775
    },
    {
      "epoch": 5.793939393939394,
      "grad_norm": 0.08012127876281738,
      "learning_rate": 2e-05,
      "loss": 0.0416,
      "step": 4780
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.08338282257318497,
      "learning_rate": 2e-05,
      "loss": 0.0421,
      "step": 4785
    },
    {
      "epoch": 5.806060606060606,
      "grad_norm": 0.0687524825334549,
      "learning_rate": 2e-05,
      "loss": 0.0427,
      "step": 4790
    },
    {
      "epoch": 5.8121212121212125,
      "grad_norm": 0.058591846376657486,
      "learning_rate": 2e-05,
      "loss": 0.0435,
      "step": 4795
    },
    {
      "epoch": 5.818181818181818,
      "grad_norm": 0.07041959464550018,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 4800
    },
    {
      "epoch": 5.824242424242424,
      "grad_norm": 0.07730993628501892,
      "learning_rate": 2e-05,
      "loss": 0.0413,
      "step": 4805
    },
    {
      "epoch": 5.83030303030303,
      "grad_norm": 0.057786330580711365,
      "learning_rate": 2e-05,
      "loss": 0.0438,
      "step": 4810
    },
    {
      "epoch": 5.836363636363636,
      "grad_norm": 0.07009665668010712,
      "learning_rate": 2e-05,
      "loss": 0.044,
      "step": 4815
    },
    {
      "epoch": 5.842424242424243,
      "grad_norm": 0.07959314435720444,
      "learning_rate": 2e-05,
      "loss": 0.0442,
      "step": 4820
    },
    {
      "epoch": 5.848484848484849,
      "grad_norm": 0.07046230137348175,
      "learning_rate": 2e-05,
      "loss": 0.0398,
      "step": 4825
    },
    {
      "epoch": 5.8545454545454545,
      "grad_norm": 0.10337064415216446,
      "learning_rate": 2e-05,
      "loss": 0.0417,
      "step": 4830
    },
    {
      "epoch": 5.86060606060606,
      "grad_norm": 0.07449709624052048,
      "learning_rate": 2e-05,
      "loss": 0.0433,
      "step": 4835
    },
    {
      "epoch": 5.866666666666666,
      "grad_norm": 0.0879519060254097,
      "learning_rate": 2e-05,
      "loss": 0.0434,
      "step": 4840
    },
    {
      "epoch": 5.872727272727273,
      "grad_norm": 0.0753110870718956,
      "learning_rate": 2e-05,
      "loss": 0.0401,
      "step": 4845
    },
    {
      "epoch": 5.878787878787879,
      "grad_norm": 0.11175484955310822,
      "learning_rate": 2e-05,
      "loss": 0.0407,
      "step": 4850
    },
    {
      "epoch": 5.884848484848485,
      "grad_norm": 0.07490792870521545,
      "learning_rate": 2e-05,
      "loss": 0.0409,
      "step": 4855
    },
    {
      "epoch": 5.890909090909091,
      "grad_norm": 0.07865083962678909,
      "learning_rate": 2e-05,
      "loss": 0.0429,
      "step": 4860
    },
    {
      "epoch": 5.8969696969696965,
      "grad_norm": 0.06292015314102173,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 4865
    },
    {
      "epoch": 5.903030303030303,
      "grad_norm": 0.06139948591589928,
      "learning_rate": 2e-05,
      "loss": 0.0417,
      "step": 4870
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 0.07410220056772232,
      "learning_rate": 2e-05,
      "loss": 0.0437,
      "step": 4875
    },
    {
      "epoch": 5.915151515151515,
      "grad_norm": 0.09692002087831497,
      "learning_rate": 2e-05,
      "loss": 0.0439,
      "step": 4880
    },
    {
      "epoch": 5.921212121212121,
      "grad_norm": 0.07234317064285278,
      "learning_rate": 2e-05,
      "loss": 0.0418,
      "step": 4885
    },
    {
      "epoch": 5.927272727272728,
      "grad_norm": 0.08439754694700241,
      "learning_rate": 2e-05,
      "loss": 0.0409,
      "step": 4890
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 0.0881393551826477,
      "learning_rate": 2e-05,
      "loss": 0.0416,
      "step": 4895
    },
    {
      "epoch": 5.9393939393939394,
      "grad_norm": 0.07994222640991211,
      "learning_rate": 2e-05,
      "loss": 0.0442,
      "step": 4900
    },
    {
      "epoch": 5.945454545454545,
      "grad_norm": 0.08812013268470764,
      "learning_rate": 2e-05,
      "loss": 0.0417,
      "step": 4905
    },
    {
      "epoch": 5.951515151515151,
      "grad_norm": 0.08185288310050964,
      "learning_rate": 2e-05,
      "loss": 0.0398,
      "step": 4910
    },
    {
      "epoch": 5.957575757575758,
      "grad_norm": 0.0731198713183403,
      "learning_rate": 2e-05,
      "loss": 0.043,
      "step": 4915
    },
    {
      "epoch": 5.963636363636364,
      "grad_norm": 0.06946530193090439,
      "learning_rate": 2e-05,
      "loss": 0.0428,
      "step": 4920
    },
    {
      "epoch": 5.96969696969697,
      "grad_norm": 0.0555591806769371,
      "learning_rate": 2e-05,
      "loss": 0.0395,
      "step": 4925
    },
    {
      "epoch": 5.975757575757576,
      "grad_norm": 0.07903490960597992,
      "learning_rate": 2e-05,
      "loss": 0.0424,
      "step": 4930
    },
    {
      "epoch": 5.9818181818181815,
      "grad_norm": 0.10276588052511215,
      "learning_rate": 2e-05,
      "loss": 0.0401,
      "step": 4935
    },
    {
      "epoch": 5.987878787878788,
      "grad_norm": 2.9159977436065674,
      "learning_rate": 2e-05,
      "loss": 0.0401,
      "step": 4940
    },
    {
      "epoch": 5.993939393939394,
      "grad_norm": 0.05997609719634056,
      "learning_rate": 2e-05,
      "loss": 0.0422,
      "step": 4945
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.06632907688617706,
      "learning_rate": 2e-05,
      "loss": 0.0414,
      "step": 4950
    },
    {
      "epoch": 6.0,
      "eval_average": 0.4854822980612581,
      "eval_crossner_ai": 0.4461538461049656,
      "eval_crossner_literature": 0.5308924484624646,
      "eval_crossner_music": 0.6236263735763063,
      "eval_crossner_politics": 0.5258964142925939,
      "eval_crossner_science": 0.6020618556202394,
      "eval_mit-movie": 0.39010989006040636,
      "eval_mit-restaurant": 0.27963525831183006,
      "eval_runtime": 36.1925,
      "eval_samples_per_second": 19.341,
      "eval_steps_per_second": 0.193,
      "step": 4950
    },
    {
      "epoch": 6.006060606060606,
      "grad_norm": 0.08394065499305725,
      "learning_rate": 2e-05,
      "loss": 0.0415,
      "step": 4955
    },
    {
      "epoch": 6.012121212121212,
      "grad_norm": 0.15092122554779053,
      "learning_rate": 2e-05,
      "loss": 0.0367,
      "step": 4960
    },
    {
      "epoch": 6.0181818181818185,
      "grad_norm": 0.06953209638595581,
      "learning_rate": 2e-05,
      "loss": 0.0423,
      "step": 4965
    },
    {
      "epoch": 6.024242424242424,
      "grad_norm": 0.07229292392730713,
      "learning_rate": 2e-05,
      "loss": 0.0428,
      "step": 4970
    },
    {
      "epoch": 6.03030303030303,
      "grad_norm": 0.14147181808948517,
      "learning_rate": 2e-05,
      "loss": 0.0402,
      "step": 4975
    },
    {
      "epoch": 6.036363636363636,
      "grad_norm": 0.06296221166849136,
      "learning_rate": 2e-05,
      "loss": 0.042,
      "step": 4980
    },
    {
      "epoch": 6.042424242424242,
      "grad_norm": 0.08000855892896652,
      "learning_rate": 2e-05,
      "loss": 0.0418,
      "step": 4985
    },
    {
      "epoch": 6.048484848484849,
      "grad_norm": 0.0793956071138382,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 4990
    },
    {
      "epoch": 6.054545454545455,
      "grad_norm": 0.06094950810074806,
      "learning_rate": 2e-05,
      "loss": 0.0409,
      "step": 4995
    },
    {
      "epoch": 6.0606060606060606,
      "grad_norm": 0.06423007696866989,
      "learning_rate": 2e-05,
      "loss": 0.041,
      "step": 5000
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 0.06115322560071945,
      "learning_rate": 2e-05,
      "loss": 0.04,
      "step": 5005
    },
    {
      "epoch": 6.072727272727272,
      "grad_norm": 0.07532776147127151,
      "learning_rate": 2e-05,
      "loss": 0.0393,
      "step": 5010
    },
    {
      "epoch": 6.078787878787879,
      "grad_norm": 0.0831807553768158,
      "learning_rate": 2e-05,
      "loss": 0.0406,
      "step": 5015
    },
    {
      "epoch": 6.084848484848485,
      "grad_norm": 0.11045572906732559,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 5020
    },
    {
      "epoch": 6.090909090909091,
      "grad_norm": 0.08781079202890396,
      "learning_rate": 2e-05,
      "loss": 0.0414,
      "step": 5025
    },
    {
      "epoch": 6.096969696969697,
      "grad_norm": 0.16602319478988647,
      "learning_rate": 2e-05,
      "loss": 0.0421,
      "step": 5030
    },
    {
      "epoch": 6.1030303030303035,
      "grad_norm": 0.08982957899570465,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 5035
    },
    {
      "epoch": 6.109090909090909,
      "grad_norm": 0.08111178129911423,
      "learning_rate": 2e-05,
      "loss": 0.0421,
      "step": 5040
    },
    {
      "epoch": 6.115151515151515,
      "grad_norm": 0.07297017425298691,
      "learning_rate": 2e-05,
      "loss": 0.0415,
      "step": 5045
    },
    {
      "epoch": 6.121212121212121,
      "grad_norm": 0.07348763942718506,
      "learning_rate": 2e-05,
      "loss": 0.0427,
      "step": 5050
    },
    {
      "epoch": 6.127272727272727,
      "grad_norm": 0.06717216223478317,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 5055
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 0.06489155441522598,
      "learning_rate": 2e-05,
      "loss": 0.0408,
      "step": 5060
    },
    {
      "epoch": 6.13939393939394,
      "grad_norm": 0.07312159985303879,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 5065
    },
    {
      "epoch": 6.1454545454545455,
      "grad_norm": 0.10169971734285355,
      "learning_rate": 2e-05,
      "loss": 0.042,
      "step": 5070
    },
    {
      "epoch": 6.151515151515151,
      "grad_norm": 0.07139234989881516,
      "learning_rate": 2e-05,
      "loss": 0.0415,
      "step": 5075
    },
    {
      "epoch": 6.157575757575757,
      "grad_norm": 0.06719270348548889,
      "learning_rate": 2e-05,
      "loss": 0.0417,
      "step": 5080
    },
    {
      "epoch": 6.163636363636364,
      "grad_norm": 0.07140057533979416,
      "learning_rate": 2e-05,
      "loss": 0.0389,
      "step": 5085
    },
    {
      "epoch": 6.16969696969697,
      "grad_norm": 0.05831874534487724,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 5090
    },
    {
      "epoch": 6.175757575757576,
      "grad_norm": 0.07334849238395691,
      "learning_rate": 2e-05,
      "loss": 0.0395,
      "step": 5095
    },
    {
      "epoch": 6.181818181818182,
      "grad_norm": 0.08657391369342804,
      "learning_rate": 2e-05,
      "loss": 0.0421,
      "step": 5100
    },
    {
      "epoch": 6.1878787878787875,
      "grad_norm": 0.0637081116437912,
      "learning_rate": 2e-05,
      "loss": 0.0414,
      "step": 5105
    },
    {
      "epoch": 6.193939393939394,
      "grad_norm": 0.06279345601797104,
      "learning_rate": 2e-05,
      "loss": 0.0416,
      "step": 5110
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.057572800666093826,
      "learning_rate": 2e-05,
      "loss": 0.0417,
      "step": 5115
    },
    {
      "epoch": 6.206060606060606,
      "grad_norm": 0.08833107352256775,
      "learning_rate": 2e-05,
      "loss": 0.0451,
      "step": 5120
    },
    {
      "epoch": 6.212121212121212,
      "grad_norm": 0.07619896531105042,
      "learning_rate": 2e-05,
      "loss": 0.0408,
      "step": 5125
    },
    {
      "epoch": 6.218181818181818,
      "grad_norm": 0.07163996249437332,
      "learning_rate": 2e-05,
      "loss": 0.0416,
      "step": 5130
    },
    {
      "epoch": 6.224242424242425,
      "grad_norm": 0.0697825476527214,
      "learning_rate": 2e-05,
      "loss": 0.0408,
      "step": 5135
    },
    {
      "epoch": 6.2303030303030305,
      "grad_norm": 0.07235340774059296,
      "learning_rate": 2e-05,
      "loss": 0.0394,
      "step": 5140
    },
    {
      "epoch": 6.236363636363636,
      "grad_norm": 0.06420307606458664,
      "learning_rate": 2e-05,
      "loss": 0.0406,
      "step": 5145
    },
    {
      "epoch": 6.242424242424242,
      "grad_norm": 0.06255548447370529,
      "learning_rate": 2e-05,
      "loss": 0.041,
      "step": 5150
    },
    {
      "epoch": 6.248484848484848,
      "grad_norm": 0.0820978581905365,
      "learning_rate": 2e-05,
      "loss": 0.0438,
      "step": 5155
    },
    {
      "epoch": 6.254545454545455,
      "grad_norm": 0.07958497107028961,
      "learning_rate": 2e-05,
      "loss": 0.0427,
      "step": 5160
    },
    {
      "epoch": 6.260606060606061,
      "grad_norm": 0.0726412907242775,
      "learning_rate": 2e-05,
      "loss": 0.0414,
      "step": 5165
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 0.07039907574653625,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 5170
    },
    {
      "epoch": 6.2727272727272725,
      "grad_norm": 0.06058211624622345,
      "learning_rate": 2e-05,
      "loss": 0.0415,
      "step": 5175
    },
    {
      "epoch": 6.278787878787878,
      "grad_norm": 0.09030341356992722,
      "learning_rate": 2e-05,
      "loss": 0.0423,
      "step": 5180
    },
    {
      "epoch": 6.284848484848485,
      "grad_norm": 0.05400341749191284,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 5185
    },
    {
      "epoch": 6.290909090909091,
      "grad_norm": 0.07209938019514084,
      "learning_rate": 2e-05,
      "loss": 0.0405,
      "step": 5190
    },
    {
      "epoch": 6.296969696969697,
      "grad_norm": 0.07721499353647232,
      "learning_rate": 2e-05,
      "loss": 0.0406,
      "step": 5195
    },
    {
      "epoch": 6.303030303030303,
      "grad_norm": 0.06673335283994675,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 5200
    },
    {
      "epoch": 6.3090909090909095,
      "grad_norm": 0.06764581799507141,
      "learning_rate": 2e-05,
      "loss": 0.0415,
      "step": 5205
    },
    {
      "epoch": 6.315151515151515,
      "grad_norm": 0.07721278071403503,
      "learning_rate": 2e-05,
      "loss": 0.0397,
      "step": 5210
    },
    {
      "epoch": 6.321212121212121,
      "grad_norm": 0.07298830896615982,
      "learning_rate": 2e-05,
      "loss": 0.0409,
      "step": 5215
    },
    {
      "epoch": 6.327272727272727,
      "grad_norm": 0.05945977941155434,
      "learning_rate": 2e-05,
      "loss": 0.0412,
      "step": 5220
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 0.07485030591487885,
      "learning_rate": 2e-05,
      "loss": 0.0408,
      "step": 5225
    },
    {
      "epoch": 6.33939393939394,
      "grad_norm": 0.0671764686703682,
      "learning_rate": 2e-05,
      "loss": 0.0388,
      "step": 5230
    },
    {
      "epoch": 6.345454545454546,
      "grad_norm": 0.07450016587972641,
      "learning_rate": 2e-05,
      "loss": 0.0411,
      "step": 5235
    },
    {
      "epoch": 6.351515151515152,
      "grad_norm": 0.05870736017823219,
      "learning_rate": 2e-05,
      "loss": 0.0415,
      "step": 5240
    },
    {
      "epoch": 6.357575757575757,
      "grad_norm": 0.06928116828203201,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 5245
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 0.06716646254062653,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 5250
    },
    {
      "epoch": 6.36969696969697,
      "grad_norm": 0.09705913811922073,
      "learning_rate": 2e-05,
      "loss": 0.0427,
      "step": 5255
    },
    {
      "epoch": 6.375757575757576,
      "grad_norm": 0.0712868720293045,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 5260
    },
    {
      "epoch": 6.381818181818182,
      "grad_norm": 0.06570251286029816,
      "learning_rate": 2e-05,
      "loss": 0.0406,
      "step": 5265
    },
    {
      "epoch": 6.387878787878788,
      "grad_norm": 0.09032589942216873,
      "learning_rate": 2e-05,
      "loss": 0.0408,
      "step": 5270
    },
    {
      "epoch": 6.393939393939394,
      "grad_norm": 0.0966697707772255,
      "learning_rate": 2e-05,
      "loss": 0.0412,
      "step": 5275
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.05482909083366394,
      "learning_rate": 2e-05,
      "loss": 0.0411,
      "step": 5280
    },
    {
      "epoch": 6.406060606060606,
      "grad_norm": 0.06453830003738403,
      "learning_rate": 2e-05,
      "loss": 0.043,
      "step": 5285
    },
    {
      "epoch": 6.412121212121212,
      "grad_norm": 0.07580024749040604,
      "learning_rate": 2e-05,
      "loss": 0.0417,
      "step": 5290
    },
    {
      "epoch": 6.418181818181818,
      "grad_norm": 0.05825069174170494,
      "learning_rate": 2e-05,
      "loss": 0.0395,
      "step": 5295
    },
    {
      "epoch": 6.424242424242424,
      "grad_norm": 0.06796596199274063,
      "learning_rate": 2e-05,
      "loss": 0.0416,
      "step": 5300
    },
    {
      "epoch": 6.430303030303031,
      "grad_norm": 0.07377056032419205,
      "learning_rate": 2e-05,
      "loss": 0.0406,
      "step": 5305
    },
    {
      "epoch": 6.4363636363636365,
      "grad_norm": 0.09313192963600159,
      "learning_rate": 2e-05,
      "loss": 0.0408,
      "step": 5310
    },
    {
      "epoch": 6.442424242424242,
      "grad_norm": 0.06870434433221817,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 5315
    },
    {
      "epoch": 6.448484848484848,
      "grad_norm": 0.08838781714439392,
      "learning_rate": 2e-05,
      "loss": 0.0384,
      "step": 5320
    },
    {
      "epoch": 6.454545454545454,
      "grad_norm": 0.066415935754776,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 5325
    },
    {
      "epoch": 6.460606060606061,
      "grad_norm": 0.07343672960996628,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 5330
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 0.05383593216538429,
      "learning_rate": 2e-05,
      "loss": 0.0394,
      "step": 5335
    },
    {
      "epoch": 6.472727272727273,
      "grad_norm": 0.0756969228386879,
      "learning_rate": 2e-05,
      "loss": 0.0412,
      "step": 5340
    },
    {
      "epoch": 6.4787878787878785,
      "grad_norm": 0.07780711352825165,
      "learning_rate": 2e-05,
      "loss": 0.0383,
      "step": 5345
    },
    {
      "epoch": 6.484848484848484,
      "grad_norm": 0.07364028692245483,
      "learning_rate": 2e-05,
      "loss": 0.0438,
      "step": 5350
    },
    {
      "epoch": 6.490909090909091,
      "grad_norm": 0.0760655626654625,
      "learning_rate": 2e-05,
      "loss": 0.0415,
      "step": 5355
    },
    {
      "epoch": 6.496969696969697,
      "grad_norm": 0.05819961056113243,
      "learning_rate": 2e-05,
      "loss": 0.0415,
      "step": 5360
    },
    {
      "epoch": 6.499393939393939,
      "eval_average": 0.5196723249949516,
      "eval_crossner_ai": 0.46575342460818786,
      "eval_crossner_literature": 0.5710982658458297,
      "eval_crossner_music": 0.6490618484558195,
      "eval_crossner_politics": 0.5548387096273485,
      "eval_crossner_science": 0.6018808776929983,
      "eval_mit-movie": 0.47091412737440935,
      "eval_mit-restaurant": 0.32415902136006886,
      "eval_runtime": 35.9908,
      "eval_samples_per_second": 19.449,
      "eval_steps_per_second": 0.194,
      "step": 5362
    },
    {
      "epoch": 6.503030303030303,
      "grad_norm": 0.06150834634900093,
      "learning_rate": 2e-05,
      "loss": 0.0412,
      "step": 5365
    },
    {
      "epoch": 6.509090909090909,
      "grad_norm": 0.06951841711997986,
      "learning_rate": 2e-05,
      "loss": 0.0401,
      "step": 5370
    },
    {
      "epoch": 6.515151515151516,
      "grad_norm": 0.08917361497879028,
      "learning_rate": 2e-05,
      "loss": 0.0407,
      "step": 5375
    },
    {
      "epoch": 6.5212121212121215,
      "grad_norm": 0.058698758482933044,
      "learning_rate": 2e-05,
      "loss": 0.0391,
      "step": 5380
    },
    {
      "epoch": 6.527272727272727,
      "grad_norm": 0.07251175493001938,
      "learning_rate": 2e-05,
      "loss": 0.0387,
      "step": 5385
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 0.0766206756234169,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 5390
    },
    {
      "epoch": 6.539393939393939,
      "grad_norm": 0.09000357985496521,
      "learning_rate": 2e-05,
      "loss": 0.0411,
      "step": 5395
    },
    {
      "epoch": 6.545454545454546,
      "grad_norm": 0.06370560824871063,
      "learning_rate": 2e-05,
      "loss": 0.0409,
      "step": 5400
    },
    {
      "epoch": 6.551515151515152,
      "grad_norm": 0.08343702554702759,
      "learning_rate": 2e-05,
      "loss": 0.039,
      "step": 5405
    },
    {
      "epoch": 6.557575757575758,
      "grad_norm": 0.06449458748102188,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 5410
    },
    {
      "epoch": 6.5636363636363635,
      "grad_norm": 0.07805731147527695,
      "learning_rate": 2e-05,
      "loss": 0.039,
      "step": 5415
    },
    {
      "epoch": 6.569696969696969,
      "grad_norm": 0.07924548536539078,
      "learning_rate": 2e-05,
      "loss": 0.0399,
      "step": 5420
    },
    {
      "epoch": 6.575757575757576,
      "grad_norm": 0.08095242083072662,
      "learning_rate": 2e-05,
      "loss": 0.0414,
      "step": 5425
    },
    {
      "epoch": 6.581818181818182,
      "grad_norm": 0.08553362637758255,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 5430
    },
    {
      "epoch": 6.587878787878788,
      "grad_norm": 0.06828118860721588,
      "learning_rate": 2e-05,
      "loss": 0.0397,
      "step": 5435
    },
    {
      "epoch": 6.593939393939394,
      "grad_norm": 0.06660578399896622,
      "learning_rate": 2e-05,
      "loss": 0.039,
      "step": 5440
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.07342511415481567,
      "learning_rate": 2e-05,
      "loss": 0.0388,
      "step": 5445
    },
    {
      "epoch": 6.606060606060606,
      "grad_norm": 0.06591589748859406,
      "learning_rate": 2e-05,
      "loss": 0.0415,
      "step": 5450
    },
    {
      "epoch": 6.612121212121212,
      "grad_norm": 0.09295936673879623,
      "learning_rate": 2e-05,
      "loss": 0.0428,
      "step": 5455
    },
    {
      "epoch": 6.618181818181818,
      "grad_norm": 0.05786015838384628,
      "learning_rate": 2e-05,
      "loss": 0.0431,
      "step": 5460
    },
    {
      "epoch": 6.624242424242424,
      "grad_norm": 0.1022174209356308,
      "learning_rate": 2e-05,
      "loss": 0.04,
      "step": 5465
    },
    {
      "epoch": 6.63030303030303,
      "grad_norm": 0.06828048825263977,
      "learning_rate": 2e-05,
      "loss": 0.0418,
      "step": 5470
    },
    {
      "epoch": 6.636363636363637,
      "grad_norm": 0.06547033041715622,
      "learning_rate": 2e-05,
      "loss": 0.0398,
      "step": 5475
    },
    {
      "epoch": 6.642424242424243,
      "grad_norm": 0.07316282391548157,
      "learning_rate": 2e-05,
      "loss": 0.0391,
      "step": 5480
    },
    {
      "epoch": 6.648484848484848,
      "grad_norm": 0.061354756355285645,
      "learning_rate": 2e-05,
      "loss": 0.0419,
      "step": 5485
    },
    {
      "epoch": 6.654545454545454,
      "grad_norm": 0.06156715750694275,
      "learning_rate": 2e-05,
      "loss": 0.0396,
      "step": 5490
    },
    {
      "epoch": 6.66060606060606,
      "grad_norm": 0.09124669432640076,
      "learning_rate": 2e-05,
      "loss": 0.0415,
      "step": 5495
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.10473627597093582,
      "learning_rate": 2e-05,
      "loss": 0.0423,
      "step": 5500
    },
    {
      "epoch": 6.672727272727273,
      "grad_norm": 0.05324726179242134,
      "learning_rate": 2e-05,
      "loss": 0.038,
      "step": 5505
    },
    {
      "epoch": 6.678787878787879,
      "grad_norm": 0.09560446441173553,
      "learning_rate": 2e-05,
      "loss": 0.0406,
      "step": 5510
    },
    {
      "epoch": 6.684848484848485,
      "grad_norm": 0.07538275420665741,
      "learning_rate": 2e-05,
      "loss": 0.0426,
      "step": 5515
    },
    {
      "epoch": 6.6909090909090905,
      "grad_norm": 0.05963309481739998,
      "learning_rate": 2e-05,
      "loss": 0.0407,
      "step": 5520
    },
    {
      "epoch": 6.696969696969697,
      "grad_norm": 0.09345043450593948,
      "learning_rate": 2e-05,
      "loss": 0.0412,
      "step": 5525
    },
    {
      "epoch": 6.703030303030303,
      "grad_norm": 0.0928206741809845,
      "learning_rate": 2e-05,
      "loss": 0.0413,
      "step": 5530
    },
    {
      "epoch": 6.709090909090909,
      "grad_norm": 0.062487538903951645,
      "learning_rate": 2e-05,
      "loss": 0.0397,
      "step": 5535
    },
    {
      "epoch": 6.715151515151515,
      "grad_norm": 0.07775480300188065,
      "learning_rate": 2e-05,
      "loss": 0.0393,
      "step": 5540
    },
    {
      "epoch": 6.721212121212122,
      "grad_norm": 0.07076092064380646,
      "learning_rate": 2e-05,
      "loss": 0.0396,
      "step": 5545
    },
    {
      "epoch": 6.7272727272727275,
      "grad_norm": 0.06830567866563797,
      "learning_rate": 2e-05,
      "loss": 0.0395,
      "step": 5550
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 0.06782740354537964,
      "learning_rate": 2e-05,
      "loss": 0.0394,
      "step": 5555
    },
    {
      "epoch": 6.739393939393939,
      "grad_norm": 0.06709828972816467,
      "learning_rate": 2e-05,
      "loss": 0.0398,
      "step": 5560
    },
    {
      "epoch": 6.745454545454545,
      "grad_norm": 0.06735154986381531,
      "learning_rate": 2e-05,
      "loss": 0.0405,
      "step": 5565
    },
    {
      "epoch": 6.751515151515152,
      "grad_norm": 0.07820669561624527,
      "learning_rate": 2e-05,
      "loss": 0.0427,
      "step": 5570
    },
    {
      "epoch": 6.757575757575758,
      "grad_norm": 0.07317856699228287,
      "learning_rate": 2e-05,
      "loss": 0.0412,
      "step": 5575
    },
    {
      "epoch": 6.763636363636364,
      "grad_norm": 0.08467010408639908,
      "learning_rate": 2e-05,
      "loss": 0.0429,
      "step": 5580
    },
    {
      "epoch": 6.7696969696969695,
      "grad_norm": 0.08615102618932724,
      "learning_rate": 2e-05,
      "loss": 0.0395,
      "step": 5585
    },
    {
      "epoch": 6.775757575757575,
      "grad_norm": 0.06572358310222626,
      "learning_rate": 2e-05,
      "loss": 0.0409,
      "step": 5590
    },
    {
      "epoch": 6.781818181818182,
      "grad_norm": 0.06004192307591438,
      "learning_rate": 2e-05,
      "loss": 0.0402,
      "step": 5595
    },
    {
      "epoch": 6.787878787878788,
      "grad_norm": 0.07058217376470566,
      "learning_rate": 2e-05,
      "loss": 0.0394,
      "step": 5600
    },
    {
      "epoch": 6.793939393939394,
      "grad_norm": 0.061502061784267426,
      "learning_rate": 2e-05,
      "loss": 0.0401,
      "step": 5605
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.08318015187978745,
      "learning_rate": 2e-05,
      "loss": 0.0387,
      "step": 5610
    },
    {
      "epoch": 6.806060606060606,
      "grad_norm": 0.061963602900505066,
      "learning_rate": 2e-05,
      "loss": 0.0411,
      "step": 5615
    },
    {
      "epoch": 6.8121212121212125,
      "grad_norm": 0.10277850180864334,
      "learning_rate": 2e-05,
      "loss": 0.04,
      "step": 5620
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 0.06448952108621597,
      "learning_rate": 2e-05,
      "loss": 0.038,
      "step": 5625
    },
    {
      "epoch": 6.824242424242424,
      "grad_norm": 0.07338221371173859,
      "learning_rate": 2e-05,
      "loss": 0.0397,
      "step": 5630
    },
    {
      "epoch": 6.83030303030303,
      "grad_norm": 0.13947150111198425,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 5635
    },
    {
      "epoch": 6.836363636363636,
      "grad_norm": 0.05080742761492729,
      "learning_rate": 2e-05,
      "loss": 0.043,
      "step": 5640
    },
    {
      "epoch": 6.842424242424243,
      "grad_norm": 0.0879233106970787,
      "learning_rate": 2e-05,
      "loss": 0.0439,
      "step": 5645
    },
    {
      "epoch": 6.848484848484849,
      "grad_norm": 0.07489263266324997,
      "learning_rate": 2e-05,
      "loss": 0.041,
      "step": 5650
    },
    {
      "epoch": 6.8545454545454545,
      "grad_norm": 0.07050609588623047,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 5655
    },
    {
      "epoch": 6.86060606060606,
      "grad_norm": 0.06412232667207718,
      "learning_rate": 2e-05,
      "loss": 0.0393,
      "step": 5660
    },
    {
      "epoch": 6.866666666666666,
      "grad_norm": 0.08352609723806381,
      "learning_rate": 2e-05,
      "loss": 0.04,
      "step": 5665
    },
    {
      "epoch": 6.872727272727273,
      "grad_norm": 0.08662665635347366,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 5670
    },
    {
      "epoch": 6.878787878787879,
      "grad_norm": 0.06821351498365402,
      "learning_rate": 2e-05,
      "loss": 0.0409,
      "step": 5675
    },
    {
      "epoch": 6.884848484848485,
      "grad_norm": 0.06963231414556503,
      "learning_rate": 2e-05,
      "loss": 0.0405,
      "step": 5680
    },
    {
      "epoch": 6.890909090909091,
      "grad_norm": 0.0893261730670929,
      "learning_rate": 2e-05,
      "loss": 0.0411,
      "step": 5685
    },
    {
      "epoch": 6.8969696969696965,
      "grad_norm": 0.06874647736549377,
      "learning_rate": 2e-05,
      "loss": 0.0385,
      "step": 5690
    },
    {
      "epoch": 6.903030303030303,
      "grad_norm": 0.07700566947460175,
      "learning_rate": 2e-05,
      "loss": 0.0382,
      "step": 5695
    },
    {
      "epoch": 6.909090909090909,
      "grad_norm": 0.06318265944719315,
      "learning_rate": 2e-05,
      "loss": 0.0409,
      "step": 5700
    },
    {
      "epoch": 6.915151515151515,
      "grad_norm": 0.06541935354471207,
      "learning_rate": 2e-05,
      "loss": 0.0406,
      "step": 5705
    },
    {
      "epoch": 6.921212121212121,
      "grad_norm": 0.0752682238817215,
      "learning_rate": 2e-05,
      "loss": 0.0398,
      "step": 5710
    },
    {
      "epoch": 6.927272727272728,
      "grad_norm": 0.09620620310306549,
      "learning_rate": 2e-05,
      "loss": 0.0398,
      "step": 5715
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 0.06225026771426201,
      "learning_rate": 2e-05,
      "loss": 0.0391,
      "step": 5720
    },
    {
      "epoch": 6.9393939393939394,
      "grad_norm": 0.055637408047914505,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 5725
    },
    {
      "epoch": 6.945454545454545,
      "grad_norm": 0.07575652003288269,
      "learning_rate": 2e-05,
      "loss": 0.0408,
      "step": 5730
    },
    {
      "epoch": 6.951515151515151,
      "grad_norm": 0.06349407881498337,
      "learning_rate": 2e-05,
      "loss": 0.0382,
      "step": 5735
    },
    {
      "epoch": 6.957575757575758,
      "grad_norm": 0.05845385044813156,
      "learning_rate": 2e-05,
      "loss": 0.0406,
      "step": 5740
    },
    {
      "epoch": 6.963636363636364,
      "grad_norm": 0.060694221407175064,
      "learning_rate": 2e-05,
      "loss": 0.0413,
      "step": 5745
    },
    {
      "epoch": 6.96969696969697,
      "grad_norm": 0.062381718307733536,
      "learning_rate": 2e-05,
      "loss": 0.0385,
      "step": 5750
    },
    {
      "epoch": 6.975757575757576,
      "grad_norm": 0.06992113590240479,
      "learning_rate": 2e-05,
      "loss": 0.0389,
      "step": 5755
    },
    {
      "epoch": 6.9818181818181815,
      "grad_norm": 0.05453137680888176,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 5760
    },
    {
      "epoch": 6.987878787878788,
      "grad_norm": 0.06465044617652893,
      "learning_rate": 2e-05,
      "loss": 0.04,
      "step": 5765
    },
    {
      "epoch": 6.993939393939394,
      "grad_norm": 0.0678420290350914,
      "learning_rate": 2e-05,
      "loss": 0.0395,
      "step": 5770
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.07181114703416824,
      "learning_rate": 2e-05,
      "loss": 0.0386,
      "step": 5775
    },
    {
      "epoch": 7.0,
      "eval_average": 0.5112947006511471,
      "eval_crossner_ai": 0.48409090904160895,
      "eval_crossner_literature": 0.5631517960101358,
      "eval_crossner_music": 0.6390041493275127,
      "eval_crossner_politics": 0.529270248546578,
      "eval_crossner_science": 0.6054279748978648,
      "eval_mit-movie": 0.44808743164440273,
      "eval_mit-restaurant": 0.3100303950899271,
      "eval_runtime": 35.7321,
      "eval_samples_per_second": 19.59,
      "eval_steps_per_second": 0.196,
      "step": 5775
    },
    {
      "epoch": 7.006060606060606,
      "grad_norm": 0.06128166988492012,
      "learning_rate": 2e-05,
      "loss": 0.0397,
      "step": 5780
    },
    {
      "epoch": 7.012121212121212,
      "grad_norm": 0.09408824145793915,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 5785
    },
    {
      "epoch": 7.0181818181818185,
      "grad_norm": 0.09201506525278091,
      "learning_rate": 2e-05,
      "loss": 0.04,
      "step": 5790
    },
    {
      "epoch": 7.024242424242424,
      "grad_norm": 0.06678246706724167,
      "learning_rate": 2e-05,
      "loss": 0.0419,
      "step": 5795
    },
    {
      "epoch": 7.03030303030303,
      "grad_norm": 0.06613035500049591,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 5800
    },
    {
      "epoch": 7.036363636363636,
      "grad_norm": 0.057129569351673126,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 5805
    },
    {
      "epoch": 7.042424242424242,
      "grad_norm": 0.10309311002492905,
      "learning_rate": 2e-05,
      "loss": 0.0402,
      "step": 5810
    },
    {
      "epoch": 7.048484848484849,
      "grad_norm": 0.08738923072814941,
      "learning_rate": 2e-05,
      "loss": 0.0385,
      "step": 5815
    },
    {
      "epoch": 7.054545454545455,
      "grad_norm": 0.08808532357215881,
      "learning_rate": 2e-05,
      "loss": 0.0389,
      "step": 5820
    },
    {
      "epoch": 7.0606060606060606,
      "grad_norm": 0.07452018558979034,
      "learning_rate": 2e-05,
      "loss": 0.0412,
      "step": 5825
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 0.05256218835711479,
      "learning_rate": 2e-05,
      "loss": 0.0382,
      "step": 5830
    },
    {
      "epoch": 7.072727272727272,
      "grad_norm": 0.06169696897268295,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 5835
    },
    {
      "epoch": 7.078787878787879,
      "grad_norm": 0.06720468401908875,
      "learning_rate": 2e-05,
      "loss": 0.0397,
      "step": 5840
    },
    {
      "epoch": 7.084848484848485,
      "grad_norm": 0.0681387260556221,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 5845
    },
    {
      "epoch": 7.090909090909091,
      "grad_norm": 0.09265822172164917,
      "learning_rate": 2e-05,
      "loss": 0.0398,
      "step": 5850
    },
    {
      "epoch": 7.096969696969697,
      "grad_norm": 0.09379707276821136,
      "learning_rate": 2e-05,
      "loss": 0.039,
      "step": 5855
    },
    {
      "epoch": 7.1030303030303035,
      "grad_norm": 0.05936212092638016,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 5860
    },
    {
      "epoch": 7.109090909090909,
      "grad_norm": 0.08715690672397614,
      "learning_rate": 2e-05,
      "loss": 0.0384,
      "step": 5865
    },
    {
      "epoch": 7.115151515151515,
      "grad_norm": 0.09594336897134781,
      "learning_rate": 2e-05,
      "loss": 0.0388,
      "step": 5870
    },
    {
      "epoch": 7.121212121212121,
      "grad_norm": 0.06346134096384048,
      "learning_rate": 2e-05,
      "loss": 0.0377,
      "step": 5875
    },
    {
      "epoch": 7.127272727272727,
      "grad_norm": 0.10621039569377899,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 5880
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 0.07186494767665863,
      "learning_rate": 2e-05,
      "loss": 0.0389,
      "step": 5885
    },
    {
      "epoch": 7.13939393939394,
      "grad_norm": 0.07518164068460464,
      "learning_rate": 2e-05,
      "loss": 0.0408,
      "step": 5890
    },
    {
      "epoch": 7.1454545454545455,
      "grad_norm": 0.1274561882019043,
      "learning_rate": 2e-05,
      "loss": 0.0414,
      "step": 5895
    },
    {
      "epoch": 7.151515151515151,
      "grad_norm": 0.06791380792856216,
      "learning_rate": 2e-05,
      "loss": 0.0386,
      "step": 5900
    },
    {
      "epoch": 7.157575757575757,
      "grad_norm": 0.06837326288223267,
      "learning_rate": 2e-05,
      "loss": 0.0394,
      "step": 5905
    },
    {
      "epoch": 7.163636363636364,
      "grad_norm": 0.0694851502776146,
      "learning_rate": 2e-05,
      "loss": 0.041,
      "step": 5910
    },
    {
      "epoch": 7.16969696969697,
      "grad_norm": 0.06420861929655075,
      "learning_rate": 2e-05,
      "loss": 0.0383,
      "step": 5915
    },
    {
      "epoch": 7.175757575757576,
      "grad_norm": 0.07728226482868195,
      "learning_rate": 2e-05,
      "loss": 0.04,
      "step": 5920
    },
    {
      "epoch": 7.181818181818182,
      "grad_norm": 0.06526961922645569,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 5925
    },
    {
      "epoch": 7.1878787878787875,
      "grad_norm": 0.08290830254554749,
      "learning_rate": 2e-05,
      "loss": 0.0399,
      "step": 5930
    },
    {
      "epoch": 7.193939393939394,
      "grad_norm": 0.07548999041318893,
      "learning_rate": 2e-05,
      "loss": 0.0378,
      "step": 5935
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.07296163588762283,
      "learning_rate": 2e-05,
      "loss": 0.0395,
      "step": 5940
    },
    {
      "epoch": 7.206060606060606,
      "grad_norm": 0.062457434833049774,
      "learning_rate": 2e-05,
      "loss": 0.0391,
      "step": 5945
    },
    {
      "epoch": 7.212121212121212,
      "grad_norm": 0.0858798548579216,
      "learning_rate": 2e-05,
      "loss": 0.0405,
      "step": 5950
    },
    {
      "epoch": 7.218181818181818,
      "grad_norm": 0.05714133009314537,
      "learning_rate": 2e-05,
      "loss": 0.0385,
      "step": 5955
    },
    {
      "epoch": 7.224242424242425,
      "grad_norm": 0.06821078807115555,
      "learning_rate": 2e-05,
      "loss": 0.0377,
      "step": 5960
    },
    {
      "epoch": 7.2303030303030305,
      "grad_norm": 0.0631827637553215,
      "learning_rate": 2e-05,
      "loss": 0.039,
      "step": 5965
    },
    {
      "epoch": 7.236363636363636,
      "grad_norm": 0.05360237881541252,
      "learning_rate": 2e-05,
      "loss": 0.0384,
      "step": 5970
    },
    {
      "epoch": 7.242424242424242,
      "grad_norm": 0.06479132175445557,
      "learning_rate": 2e-05,
      "loss": 0.0399,
      "step": 5975
    },
    {
      "epoch": 7.248484848484848,
      "grad_norm": 0.0755973607301712,
      "learning_rate": 2e-05,
      "loss": 0.0399,
      "step": 5980
    },
    {
      "epoch": 7.254545454545455,
      "grad_norm": 0.07112964987754822,
      "learning_rate": 2e-05,
      "loss": 0.0383,
      "step": 5985
    },
    {
      "epoch": 7.260606060606061,
      "grad_norm": 0.0575018934905529,
      "learning_rate": 2e-05,
      "loss": 0.0401,
      "step": 5990
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 0.07884504646062851,
      "learning_rate": 2e-05,
      "loss": 0.0402,
      "step": 5995
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 0.07884568721055984,
      "learning_rate": 2e-05,
      "loss": 0.0413,
      "step": 6000
    },
    {
      "epoch": 7.278787878787878,
      "grad_norm": 0.06682366132736206,
      "learning_rate": 2e-05,
      "loss": 0.0378,
      "step": 6005
    },
    {
      "epoch": 7.284848484848485,
      "grad_norm": 0.055077049881219864,
      "learning_rate": 2e-05,
      "loss": 0.0375,
      "step": 6010
    },
    {
      "epoch": 7.290909090909091,
      "grad_norm": 0.053198203444480896,
      "learning_rate": 2e-05,
      "loss": 0.0397,
      "step": 6015
    },
    {
      "epoch": 7.296969696969697,
      "grad_norm": 0.10345204174518585,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 6020
    },
    {
      "epoch": 7.303030303030303,
      "grad_norm": 0.12481752038002014,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 6025
    },
    {
      "epoch": 7.3090909090909095,
      "grad_norm": 0.06808049231767654,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 6030
    },
    {
      "epoch": 7.315151515151515,
      "grad_norm": 0.06301016360521317,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 6035
    },
    {
      "epoch": 7.321212121212121,
      "grad_norm": 0.09896549582481384,
      "learning_rate": 2e-05,
      "loss": 0.0373,
      "step": 6040
    },
    {
      "epoch": 7.327272727272727,
      "grad_norm": 0.09582474082708359,
      "learning_rate": 2e-05,
      "loss": 0.0384,
      "step": 6045
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 0.09620226919651031,
      "learning_rate": 2e-05,
      "loss": 0.039,
      "step": 6050
    },
    {
      "epoch": 7.33939393939394,
      "grad_norm": 0.07941470295190811,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 6055
    },
    {
      "epoch": 7.345454545454546,
      "grad_norm": 0.0665370300412178,
      "learning_rate": 2e-05,
      "loss": 0.0383,
      "step": 6060
    },
    {
      "epoch": 7.351515151515152,
      "grad_norm": 0.08083336055278778,
      "learning_rate": 2e-05,
      "loss": 0.0401,
      "step": 6065
    },
    {
      "epoch": 7.357575757575757,
      "grad_norm": 0.06350915879011154,
      "learning_rate": 2e-05,
      "loss": 0.0396,
      "step": 6070
    },
    {
      "epoch": 7.363636363636363,
      "grad_norm": 0.062388572841882706,
      "learning_rate": 2e-05,
      "loss": 0.0393,
      "step": 6075
    },
    {
      "epoch": 7.36969696969697,
      "grad_norm": 0.08925025165081024,
      "learning_rate": 2e-05,
      "loss": 0.0383,
      "step": 6080
    },
    {
      "epoch": 7.375757575757576,
      "grad_norm": 0.061143964529037476,
      "learning_rate": 2e-05,
      "loss": 0.0406,
      "step": 6085
    },
    {
      "epoch": 7.381818181818182,
      "grad_norm": 0.05992402508854866,
      "learning_rate": 2e-05,
      "loss": 0.0377,
      "step": 6090
    },
    {
      "epoch": 7.387878787878788,
      "grad_norm": 0.0543084479868412,
      "learning_rate": 2e-05,
      "loss": 0.0391,
      "step": 6095
    },
    {
      "epoch": 7.393939393939394,
      "grad_norm": 0.053756702691316605,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 6100
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.06574589014053345,
      "learning_rate": 2e-05,
      "loss": 0.039,
      "step": 6105
    },
    {
      "epoch": 7.406060606060606,
      "grad_norm": 0.07298533618450165,
      "learning_rate": 2e-05,
      "loss": 0.0405,
      "step": 6110
    },
    {
      "epoch": 7.412121212121212,
      "grad_norm": 0.09279947727918625,
      "learning_rate": 2e-05,
      "loss": 0.0384,
      "step": 6115
    },
    {
      "epoch": 7.418181818181818,
      "grad_norm": 0.08367535471916199,
      "learning_rate": 2e-05,
      "loss": 0.0391,
      "step": 6120
    },
    {
      "epoch": 7.424242424242424,
      "grad_norm": 0.08404272049665451,
      "learning_rate": 2e-05,
      "loss": 0.0398,
      "step": 6125
    },
    {
      "epoch": 7.430303030303031,
      "grad_norm": 0.06869124621152878,
      "learning_rate": 2e-05,
      "loss": 0.0398,
      "step": 6130
    },
    {
      "epoch": 7.4363636363636365,
      "grad_norm": 0.08936352282762527,
      "learning_rate": 2e-05,
      "loss": 0.0371,
      "step": 6135
    },
    {
      "epoch": 7.442424242424242,
      "grad_norm": 0.09485947340726852,
      "learning_rate": 2e-05,
      "loss": 0.0388,
      "step": 6140
    },
    {
      "epoch": 7.448484848484848,
      "grad_norm": 0.06208851933479309,
      "learning_rate": 2e-05,
      "loss": 0.0376,
      "step": 6145
    },
    {
      "epoch": 7.454545454545454,
      "grad_norm": 0.08890949189662933,
      "learning_rate": 2e-05,
      "loss": 0.0386,
      "step": 6150
    },
    {
      "epoch": 7.460606060606061,
      "grad_norm": 0.0791144147515297,
      "learning_rate": 2e-05,
      "loss": 0.0395,
      "step": 6155
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 0.07723279297351837,
      "learning_rate": 2e-05,
      "loss": 0.04,
      "step": 6160
    },
    {
      "epoch": 7.472727272727273,
      "grad_norm": 0.0816197469830513,
      "learning_rate": 2e-05,
      "loss": 0.0405,
      "step": 6165
    },
    {
      "epoch": 7.4787878787878785,
      "grad_norm": 0.08968781679868698,
      "learning_rate": 2e-05,
      "loss": 0.0393,
      "step": 6170
    },
    {
      "epoch": 7.484848484848484,
      "grad_norm": 0.06398498266935349,
      "learning_rate": 2e-05,
      "loss": 0.039,
      "step": 6175
    },
    {
      "epoch": 7.490909090909091,
      "grad_norm": 0.05861736834049225,
      "learning_rate": 2e-05,
      "loss": 0.041,
      "step": 6180
    },
    {
      "epoch": 7.496969696969697,
      "grad_norm": 0.05159022659063339,
      "learning_rate": 2e-05,
      "loss": 0.0399,
      "step": 6185
    },
    {
      "epoch": 7.500606060606061,
      "eval_average": 0.5283167620573748,
      "eval_crossner_ai": 0.47491638791079444,
      "eval_crossner_literature": 0.5740318906104618,
      "eval_crossner_music": 0.6777316735322099,
      "eval_crossner_politics": 0.5416666666166128,
      "eval_crossner_science": 0.6113989636806701,
      "eval_mit-movie": 0.48913043473293366,
      "eval_mit-restaurant": 0.32934131731794075,
      "eval_runtime": 36.3768,
      "eval_samples_per_second": 19.243,
      "eval_steps_per_second": 0.192,
      "step": 6188
    },
    {
      "epoch": 7.503030303030303,
      "grad_norm": 0.05865045264363289,
      "learning_rate": 2e-05,
      "loss": 0.0388,
      "step": 6190
    },
    {
      "epoch": 7.509090909090909,
      "grad_norm": 0.11502144485712051,
      "learning_rate": 2e-05,
      "loss": 0.0405,
      "step": 6195
    },
    {
      "epoch": 7.515151515151516,
      "grad_norm": 0.06965146958827972,
      "learning_rate": 2e-05,
      "loss": 0.0391,
      "step": 6200
    },
    {
      "epoch": 7.5212121212121215,
      "grad_norm": 0.07216881960630417,
      "learning_rate": 2e-05,
      "loss": 0.0386,
      "step": 6205
    },
    {
      "epoch": 7.527272727272727,
      "grad_norm": 0.09712067991495132,
      "learning_rate": 2e-05,
      "loss": 0.04,
      "step": 6210
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 0.05618784576654434,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 6215
    },
    {
      "epoch": 7.539393939393939,
      "grad_norm": 0.05862788110971451,
      "learning_rate": 2e-05,
      "loss": 0.0393,
      "step": 6220
    },
    {
      "epoch": 7.545454545454546,
      "grad_norm": 0.059271130710840225,
      "learning_rate": 2e-05,
      "loss": 0.0389,
      "step": 6225
    },
    {
      "epoch": 7.551515151515152,
      "grad_norm": 0.06580176204442978,
      "learning_rate": 2e-05,
      "loss": 0.0412,
      "step": 6230
    },
    {
      "epoch": 7.557575757575758,
      "grad_norm": 0.08062112331390381,
      "learning_rate": 2e-05,
      "loss": 0.0388,
      "step": 6235
    },
    {
      "epoch": 7.5636363636363635,
      "grad_norm": 0.08144274353981018,
      "learning_rate": 2e-05,
      "loss": 0.0397,
      "step": 6240
    },
    {
      "epoch": 7.569696969696969,
      "grad_norm": 0.07983450591564178,
      "learning_rate": 2e-05,
      "loss": 0.0402,
      "step": 6245
    },
    {
      "epoch": 7.575757575757576,
      "grad_norm": 0.05845827981829643,
      "learning_rate": 2e-05,
      "loss": 0.0398,
      "step": 6250
    },
    {
      "epoch": 7.581818181818182,
      "grad_norm": 0.07081902772188187,
      "learning_rate": 2e-05,
      "loss": 0.0396,
      "step": 6255
    },
    {
      "epoch": 7.587878787878788,
      "grad_norm": 0.056943852454423904,
      "learning_rate": 2e-05,
      "loss": 0.0386,
      "step": 6260
    },
    {
      "epoch": 7.593939393939394,
      "grad_norm": 0.05731816962361336,
      "learning_rate": 2e-05,
      "loss": 0.0386,
      "step": 6265
    },
    {
      "epoch": 7.6,
      "grad_norm": 0.05039210990071297,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 6270
    },
    {
      "epoch": 7.606060606060606,
      "grad_norm": 0.06677911430597305,
      "learning_rate": 2e-05,
      "loss": 0.0409,
      "step": 6275
    },
    {
      "epoch": 7.612121212121212,
      "grad_norm": 0.0665106475353241,
      "learning_rate": 2e-05,
      "loss": 0.0393,
      "step": 6280
    },
    {
      "epoch": 7.618181818181818,
      "grad_norm": 0.06215237081050873,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 6285
    },
    {
      "epoch": 7.624242424242424,
      "grad_norm": 0.07134270668029785,
      "learning_rate": 2e-05,
      "loss": 0.039,
      "step": 6290
    },
    {
      "epoch": 7.63030303030303,
      "grad_norm": 0.06753616780042648,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 6295
    },
    {
      "epoch": 7.636363636363637,
      "grad_norm": 0.05801332741975784,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 6300
    },
    {
      "epoch": 7.642424242424243,
      "grad_norm": 0.10522516071796417,
      "learning_rate": 2e-05,
      "loss": 0.0402,
      "step": 6305
    },
    {
      "epoch": 7.648484848484848,
      "grad_norm": 0.07155576348304749,
      "learning_rate": 2e-05,
      "loss": 0.0395,
      "step": 6310
    },
    {
      "epoch": 7.654545454545454,
      "grad_norm": 0.060331422835588455,
      "learning_rate": 2e-05,
      "loss": 0.0385,
      "step": 6315
    },
    {
      "epoch": 7.66060606060606,
      "grad_norm": 0.09707453101873398,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 6320
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 0.07640527933835983,
      "learning_rate": 2e-05,
      "loss": 0.0393,
      "step": 6325
    },
    {
      "epoch": 7.672727272727273,
      "grad_norm": 0.09753958135843277,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 6330
    },
    {
      "epoch": 7.678787878787879,
      "grad_norm": 0.10445615649223328,
      "learning_rate": 2e-05,
      "loss": 0.0412,
      "step": 6335
    },
    {
      "epoch": 7.684848484848485,
      "grad_norm": 0.0779973641037941,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 6340
    },
    {
      "epoch": 7.6909090909090905,
      "grad_norm": 0.09603080153465271,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 6345
    },
    {
      "epoch": 7.696969696969697,
      "grad_norm": 0.06496620178222656,
      "learning_rate": 2e-05,
      "loss": 0.0383,
      "step": 6350
    },
    {
      "epoch": 7.703030303030303,
      "grad_norm": 0.060922615230083466,
      "learning_rate": 2e-05,
      "loss": 0.0376,
      "step": 6355
    },
    {
      "epoch": 7.709090909090909,
      "grad_norm": 0.07853992283344269,
      "learning_rate": 2e-05,
      "loss": 0.0401,
      "step": 6360
    },
    {
      "epoch": 7.715151515151515,
      "grad_norm": 0.07063229382038116,
      "learning_rate": 2e-05,
      "loss": 0.0387,
      "step": 6365
    },
    {
      "epoch": 7.721212121212122,
      "grad_norm": 0.06712353974580765,
      "learning_rate": 2e-05,
      "loss": 0.0373,
      "step": 6370
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 0.0755358561873436,
      "learning_rate": 2e-05,
      "loss": 0.0386,
      "step": 6375
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 0.0740831270813942,
      "learning_rate": 2e-05,
      "loss": 0.0376,
      "step": 6380
    },
    {
      "epoch": 7.739393939393939,
      "grad_norm": 0.07504250109195709,
      "learning_rate": 2e-05,
      "loss": 0.0373,
      "step": 6385
    },
    {
      "epoch": 7.745454545454545,
      "grad_norm": 0.09924783557653427,
      "learning_rate": 2e-05,
      "loss": 0.0419,
      "step": 6390
    },
    {
      "epoch": 7.751515151515152,
      "grad_norm": 0.06220968812704086,
      "learning_rate": 2e-05,
      "loss": 0.0375,
      "step": 6395
    },
    {
      "epoch": 7.757575757575758,
      "grad_norm": 0.0650143250823021,
      "learning_rate": 2e-05,
      "loss": 0.0384,
      "step": 6400
    },
    {
      "epoch": 7.763636363636364,
      "grad_norm": 0.08040845394134521,
      "learning_rate": 2e-05,
      "loss": 0.0385,
      "step": 6405
    },
    {
      "epoch": 7.7696969696969695,
      "grad_norm": 0.07287304103374481,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 6410
    },
    {
      "epoch": 7.775757575757575,
      "grad_norm": 0.06790123879909515,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 6415
    },
    {
      "epoch": 7.781818181818182,
      "grad_norm": 0.07540931552648544,
      "learning_rate": 2e-05,
      "loss": 0.0378,
      "step": 6420
    },
    {
      "epoch": 7.787878787878788,
      "grad_norm": 0.058715030550956726,
      "learning_rate": 2e-05,
      "loss": 0.0378,
      "step": 6425
    },
    {
      "epoch": 7.793939393939394,
      "grad_norm": 0.06845410913228989,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 6430
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.07562275975942612,
      "learning_rate": 2e-05,
      "loss": 0.0382,
      "step": 6435
    },
    {
      "epoch": 7.806060606060606,
      "grad_norm": 0.11641979217529297,
      "learning_rate": 2e-05,
      "loss": 0.0377,
      "step": 6440
    },
    {
      "epoch": 7.8121212121212125,
      "grad_norm": 0.0641135722398758,
      "learning_rate": 2e-05,
      "loss": 0.0391,
      "step": 6445
    },
    {
      "epoch": 7.818181818181818,
      "grad_norm": 0.058358341455459595,
      "learning_rate": 2e-05,
      "loss": 0.036,
      "step": 6450
    },
    {
      "epoch": 7.824242424242424,
      "grad_norm": 0.0581836923956871,
      "learning_rate": 2e-05,
      "loss": 0.0387,
      "step": 6455
    },
    {
      "epoch": 7.83030303030303,
      "grad_norm": 0.06485643982887268,
      "learning_rate": 2e-05,
      "loss": 0.0396,
      "step": 6460
    },
    {
      "epoch": 7.836363636363636,
      "grad_norm": 0.15562303364276886,
      "learning_rate": 2e-05,
      "loss": 0.0399,
      "step": 6465
    },
    {
      "epoch": 7.842424242424243,
      "grad_norm": 0.0950160101056099,
      "learning_rate": 2e-05,
      "loss": 0.0387,
      "step": 6470
    },
    {
      "epoch": 7.848484848484849,
      "grad_norm": 0.07556512951850891,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 6475
    },
    {
      "epoch": 7.8545454545454545,
      "grad_norm": 0.08298871666193008,
      "learning_rate": 2e-05,
      "loss": 0.0362,
      "step": 6480
    },
    {
      "epoch": 7.86060606060606,
      "grad_norm": 0.07246028631925583,
      "learning_rate": 2e-05,
      "loss": 0.0385,
      "step": 6485
    },
    {
      "epoch": 7.866666666666666,
      "grad_norm": 0.0771624743938446,
      "learning_rate": 2e-05,
      "loss": 0.0382,
      "step": 6490
    },
    {
      "epoch": 7.872727272727273,
      "grad_norm": 0.056556034833192825,
      "learning_rate": 2e-05,
      "loss": 0.0367,
      "step": 6495
    },
    {
      "epoch": 7.878787878787879,
      "grad_norm": 0.08389294147491455,
      "learning_rate": 2e-05,
      "loss": 0.0434,
      "step": 6500
    },
    {
      "epoch": 7.884848484848485,
      "grad_norm": 0.09007108211517334,
      "learning_rate": 2e-05,
      "loss": 0.0393,
      "step": 6505
    },
    {
      "epoch": 7.890909090909091,
      "grad_norm": 0.06966616958379745,
      "learning_rate": 2e-05,
      "loss": 0.0397,
      "step": 6510
    },
    {
      "epoch": 7.8969696969696965,
      "grad_norm": 0.06560228765010834,
      "learning_rate": 2e-05,
      "loss": 0.04,
      "step": 6515
    },
    {
      "epoch": 7.903030303030303,
      "grad_norm": 0.09806632995605469,
      "learning_rate": 2e-05,
      "loss": 0.0394,
      "step": 6520
    },
    {
      "epoch": 7.909090909090909,
      "grad_norm": 0.08812654763460159,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 6525
    },
    {
      "epoch": 7.915151515151515,
      "grad_norm": 0.06433847546577454,
      "learning_rate": 2e-05,
      "loss": 0.0384,
      "step": 6530
    },
    {
      "epoch": 7.921212121212121,
      "grad_norm": 0.05822347104549408,
      "learning_rate": 2e-05,
      "loss": 0.0377,
      "step": 6535
    },
    {
      "epoch": 7.927272727272728,
      "grad_norm": 0.0786736011505127,
      "learning_rate": 2e-05,
      "loss": 0.0397,
      "step": 6540
    },
    {
      "epoch": 7.933333333333334,
      "grad_norm": 0.06781142204999924,
      "learning_rate": 2e-05,
      "loss": 0.0396,
      "step": 6545
    },
    {
      "epoch": 7.9393939393939394,
      "grad_norm": 0.05793853849172592,
      "learning_rate": 2e-05,
      "loss": 0.0394,
      "step": 6550
    },
    {
      "epoch": 7.945454545454545,
      "grad_norm": 0.0849328264594078,
      "learning_rate": 2e-05,
      "loss": 0.038,
      "step": 6555
    },
    {
      "epoch": 7.951515151515151,
      "grad_norm": 0.0594235397875309,
      "learning_rate": 2e-05,
      "loss": 0.0376,
      "step": 6560
    },
    {
      "epoch": 7.957575757575758,
      "grad_norm": 0.07251393795013428,
      "learning_rate": 2e-05,
      "loss": 0.0393,
      "step": 6565
    },
    {
      "epoch": 7.963636363636364,
      "grad_norm": 0.05602628365159035,
      "learning_rate": 2e-05,
      "loss": 0.0387,
      "step": 6570
    },
    {
      "epoch": 7.96969696969697,
      "grad_norm": 0.06856775283813477,
      "learning_rate": 2e-05,
      "loss": 0.0384,
      "step": 6575
    },
    {
      "epoch": 7.975757575757576,
      "grad_norm": 0.07422703504562378,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 6580
    },
    {
      "epoch": 7.9818181818181815,
      "grad_norm": 0.06913068145513535,
      "learning_rate": 2e-05,
      "loss": 0.0366,
      "step": 6585
    },
    {
      "epoch": 7.987878787878788,
      "grad_norm": 0.07570809870958328,
      "learning_rate": 2e-05,
      "loss": 0.0389,
      "step": 6590
    },
    {
      "epoch": 7.993939393939394,
      "grad_norm": 0.09066050499677658,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 6595
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.08155206590890884,
      "learning_rate": 2e-05,
      "loss": 0.0376,
      "step": 6600
    },
    {
      "epoch": 8.0,
      "eval_average": 0.5176530532595325,
      "eval_crossner_ai": 0.48532731372052906,
      "eval_crossner_literature": 0.5279069766940806,
      "eval_crossner_music": 0.6471816283423962,
      "eval_crossner_politics": 0.5377207062099747,
      "eval_crossner_science": 0.6064382138649375,
      "eval_mit-movie": 0.4876712328271391,
      "eval_mit-restaurant": 0.33132530115766984,
      "eval_runtime": 36.403,
      "eval_samples_per_second": 19.229,
      "eval_steps_per_second": 0.192,
      "step": 6600
    },
    {
      "epoch": 8.006060606060606,
      "grad_norm": 0.07488363236188889,
      "learning_rate": 2e-05,
      "loss": 0.0402,
      "step": 6605
    },
    {
      "epoch": 8.012121212121212,
      "grad_norm": 0.07590808719396591,
      "learning_rate": 2e-05,
      "loss": 0.0337,
      "step": 6610
    },
    {
      "epoch": 8.018181818181818,
      "grad_norm": 0.08402812480926514,
      "learning_rate": 2e-05,
      "loss": 0.0393,
      "step": 6615
    },
    {
      "epoch": 8.024242424242424,
      "grad_norm": 0.06686277687549591,
      "learning_rate": 2e-05,
      "loss": 0.0383,
      "step": 6620
    },
    {
      "epoch": 8.030303030303031,
      "grad_norm": 0.08314063400030136,
      "learning_rate": 2e-05,
      "loss": 0.0403,
      "step": 6625
    },
    {
      "epoch": 8.036363636363637,
      "grad_norm": 0.08677475154399872,
      "learning_rate": 2e-05,
      "loss": 0.0385,
      "step": 6630
    },
    {
      "epoch": 8.042424242424243,
      "grad_norm": 0.06936859339475632,
      "learning_rate": 2e-05,
      "loss": 0.0405,
      "step": 6635
    },
    {
      "epoch": 8.048484848484849,
      "grad_norm": 0.10966386646032333,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 6640
    },
    {
      "epoch": 8.054545454545455,
      "grad_norm": 0.07067538797855377,
      "learning_rate": 2e-05,
      "loss": 0.0387,
      "step": 6645
    },
    {
      "epoch": 8.06060606060606,
      "grad_norm": 0.07339715957641602,
      "learning_rate": 2e-05,
      "loss": 0.0372,
      "step": 6650
    },
    {
      "epoch": 8.066666666666666,
      "grad_norm": 0.0718403235077858,
      "learning_rate": 2e-05,
      "loss": 0.0369,
      "step": 6655
    },
    {
      "epoch": 8.072727272727272,
      "grad_norm": 0.06994477659463882,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 6660
    },
    {
      "epoch": 8.078787878787878,
      "grad_norm": 0.07339758425951004,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 6665
    },
    {
      "epoch": 8.084848484848484,
      "grad_norm": 0.06306014209985733,
      "learning_rate": 2e-05,
      "loss": 0.0373,
      "step": 6670
    },
    {
      "epoch": 8.090909090909092,
      "grad_norm": 0.07986609637737274,
      "learning_rate": 2e-05,
      "loss": 0.0394,
      "step": 6675
    },
    {
      "epoch": 8.096969696969698,
      "grad_norm": 0.10133442282676697,
      "learning_rate": 2e-05,
      "loss": 0.0382,
      "step": 6680
    },
    {
      "epoch": 8.103030303030303,
      "grad_norm": 0.0614766925573349,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 6685
    },
    {
      "epoch": 8.10909090909091,
      "grad_norm": 0.061616189777851105,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 6690
    },
    {
      "epoch": 8.115151515151515,
      "grad_norm": 0.0720783919095993,
      "learning_rate": 2e-05,
      "loss": 0.0394,
      "step": 6695
    },
    {
      "epoch": 8.121212121212121,
      "grad_norm": 0.05291038751602173,
      "learning_rate": 2e-05,
      "loss": 0.0363,
      "step": 6700
    },
    {
      "epoch": 8.127272727272727,
      "grad_norm": 0.06882176548242569,
      "learning_rate": 2e-05,
      "loss": 0.0382,
      "step": 6705
    },
    {
      "epoch": 8.133333333333333,
      "grad_norm": 0.05809345468878746,
      "learning_rate": 2e-05,
      "loss": 0.0371,
      "step": 6710
    },
    {
      "epoch": 8.139393939393939,
      "grad_norm": 0.07137241214513779,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 6715
    },
    {
      "epoch": 8.145454545454545,
      "grad_norm": 0.058852970600128174,
      "learning_rate": 2e-05,
      "loss": 0.0375,
      "step": 6720
    },
    {
      "epoch": 8.151515151515152,
      "grad_norm": 0.10675439983606339,
      "learning_rate": 2e-05,
      "loss": 0.0391,
      "step": 6725
    },
    {
      "epoch": 8.157575757575758,
      "grad_norm": 0.07177643477916718,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 6730
    },
    {
      "epoch": 8.163636363636364,
      "grad_norm": 0.07597062736749649,
      "learning_rate": 2e-05,
      "loss": 0.0396,
      "step": 6735
    },
    {
      "epoch": 8.16969696969697,
      "grad_norm": 0.08713960647583008,
      "learning_rate": 2e-05,
      "loss": 0.0386,
      "step": 6740
    },
    {
      "epoch": 8.175757575757576,
      "grad_norm": 0.05834011361002922,
      "learning_rate": 2e-05,
      "loss": 0.0393,
      "step": 6745
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 0.06981927156448364,
      "learning_rate": 2e-05,
      "loss": 0.038,
      "step": 6750
    },
    {
      "epoch": 8.187878787878788,
      "grad_norm": 0.059987518936395645,
      "learning_rate": 2e-05,
      "loss": 0.0384,
      "step": 6755
    },
    {
      "epoch": 8.193939393939393,
      "grad_norm": 0.054009102284908295,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 6760
    },
    {
      "epoch": 8.2,
      "grad_norm": 0.07248809188604355,
      "learning_rate": 2e-05,
      "loss": 0.0383,
      "step": 6765
    },
    {
      "epoch": 8.206060606060607,
      "grad_norm": 0.0604398176074028,
      "learning_rate": 2e-05,
      "loss": 0.0378,
      "step": 6770
    },
    {
      "epoch": 8.212121212121213,
      "grad_norm": 0.05766739696264267,
      "learning_rate": 2e-05,
      "loss": 0.0385,
      "step": 6775
    },
    {
      "epoch": 8.218181818181819,
      "grad_norm": 0.0669344961643219,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 6780
    },
    {
      "epoch": 8.224242424242425,
      "grad_norm": 0.07848073542118073,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 6785
    },
    {
      "epoch": 8.23030303030303,
      "grad_norm": 0.0721970796585083,
      "learning_rate": 2e-05,
      "loss": 0.0395,
      "step": 6790
    },
    {
      "epoch": 8.236363636363636,
      "grad_norm": 0.05908706784248352,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 6795
    },
    {
      "epoch": 8.242424242424242,
      "grad_norm": 0.08760562539100647,
      "learning_rate": 2e-05,
      "loss": 0.0372,
      "step": 6800
    },
    {
      "epoch": 8.248484848484848,
      "grad_norm": 0.12199732661247253,
      "learning_rate": 2e-05,
      "loss": 0.037,
      "step": 6805
    },
    {
      "epoch": 8.254545454545454,
      "grad_norm": 0.06648868322372437,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 6810
    },
    {
      "epoch": 8.26060606060606,
      "grad_norm": 0.06494362652301788,
      "learning_rate": 2e-05,
      "loss": 0.0365,
      "step": 6815
    },
    {
      "epoch": 8.266666666666667,
      "grad_norm": 0.06942671537399292,
      "learning_rate": 2e-05,
      "loss": 0.0378,
      "step": 6820
    },
    {
      "epoch": 8.272727272727273,
      "grad_norm": 0.11042214930057526,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 6825
    },
    {
      "epoch": 8.27878787878788,
      "grad_norm": 0.07361873239278793,
      "learning_rate": 2e-05,
      "loss": 0.039,
      "step": 6830
    },
    {
      "epoch": 8.284848484848485,
      "grad_norm": 0.06298653036355972,
      "learning_rate": 2e-05,
      "loss": 0.0378,
      "step": 6835
    },
    {
      "epoch": 8.290909090909091,
      "grad_norm": 0.10913924872875214,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 6840
    },
    {
      "epoch": 8.296969696969697,
      "grad_norm": 0.06194254010915756,
      "learning_rate": 2e-05,
      "loss": 0.0384,
      "step": 6845
    },
    {
      "epoch": 8.303030303030303,
      "grad_norm": 0.0884651243686676,
      "learning_rate": 2e-05,
      "loss": 0.0371,
      "step": 6850
    },
    {
      "epoch": 8.309090909090909,
      "grad_norm": 0.07964688539505005,
      "learning_rate": 2e-05,
      "loss": 0.0386,
      "step": 6855
    },
    {
      "epoch": 8.315151515151515,
      "grad_norm": 0.08844174444675446,
      "learning_rate": 2e-05,
      "loss": 0.0371,
      "step": 6860
    },
    {
      "epoch": 8.32121212121212,
      "grad_norm": 0.06055307015776634,
      "learning_rate": 2e-05,
      "loss": 0.0386,
      "step": 6865
    },
    {
      "epoch": 8.327272727272728,
      "grad_norm": 0.0710262879729271,
      "learning_rate": 2e-05,
      "loss": 0.0389,
      "step": 6870
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 0.06946073472499847,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 6875
    },
    {
      "epoch": 8.33939393939394,
      "grad_norm": 0.059831663966178894,
      "learning_rate": 2e-05,
      "loss": 0.037,
      "step": 6880
    },
    {
      "epoch": 8.345454545454546,
      "grad_norm": 0.063246950507164,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 6885
    },
    {
      "epoch": 8.351515151515152,
      "grad_norm": 0.05927722156047821,
      "learning_rate": 2e-05,
      "loss": 0.0372,
      "step": 6890
    },
    {
      "epoch": 8.357575757575757,
      "grad_norm": 0.05595341697335243,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 6895
    },
    {
      "epoch": 8.363636363636363,
      "grad_norm": 0.059193797409534454,
      "learning_rate": 2e-05,
      "loss": 0.0377,
      "step": 6900
    },
    {
      "epoch": 8.36969696969697,
      "grad_norm": 0.08129574358463287,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 6905
    },
    {
      "epoch": 8.375757575757575,
      "grad_norm": 0.08375454694032669,
      "learning_rate": 2e-05,
      "loss": 0.0389,
      "step": 6910
    },
    {
      "epoch": 8.381818181818181,
      "grad_norm": 0.06394309550523758,
      "learning_rate": 2e-05,
      "loss": 0.0367,
      "step": 6915
    },
    {
      "epoch": 8.387878787878789,
      "grad_norm": 0.05599651858210564,
      "learning_rate": 2e-05,
      "loss": 0.0369,
      "step": 6920
    },
    {
      "epoch": 8.393939393939394,
      "grad_norm": 0.0854937955737114,
      "learning_rate": 2e-05,
      "loss": 0.038,
      "step": 6925
    },
    {
      "epoch": 8.4,
      "grad_norm": 0.06673761457204819,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 6930
    },
    {
      "epoch": 8.406060606060606,
      "grad_norm": 0.0746322050690651,
      "learning_rate": 2e-05,
      "loss": 0.0372,
      "step": 6935
    },
    {
      "epoch": 8.412121212121212,
      "grad_norm": 0.053435955196619034,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 6940
    },
    {
      "epoch": 8.418181818181818,
      "grad_norm": 0.054804906249046326,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 6945
    },
    {
      "epoch": 8.424242424242424,
      "grad_norm": 0.08973141014575958,
      "learning_rate": 2e-05,
      "loss": 0.0398,
      "step": 6950
    },
    {
      "epoch": 8.43030303030303,
      "grad_norm": 0.08118865638971329,
      "learning_rate": 2e-05,
      "loss": 0.0388,
      "step": 6955
    },
    {
      "epoch": 8.436363636363636,
      "grad_norm": 0.0940985381603241,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 6960
    },
    {
      "epoch": 8.442424242424243,
      "grad_norm": 0.06851347535848618,
      "learning_rate": 2e-05,
      "loss": 0.0377,
      "step": 6965
    },
    {
      "epoch": 8.44848484848485,
      "grad_norm": 0.0772867351770401,
      "learning_rate": 2e-05,
      "loss": 0.0367,
      "step": 6970
    },
    {
      "epoch": 8.454545454545455,
      "grad_norm": 0.07418852299451828,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 6975
    },
    {
      "epoch": 8.460606060606061,
      "grad_norm": 0.06544093787670135,
      "learning_rate": 2e-05,
      "loss": 0.0369,
      "step": 6980
    },
    {
      "epoch": 8.466666666666667,
      "grad_norm": 0.07214388996362686,
      "learning_rate": 2e-05,
      "loss": 0.039,
      "step": 6985
    },
    {
      "epoch": 8.472727272727273,
      "grad_norm": 0.057813990861177444,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 6990
    },
    {
      "epoch": 8.478787878787879,
      "grad_norm": 0.06350751966238022,
      "learning_rate": 2e-05,
      "loss": 0.0375,
      "step": 6995
    },
    {
      "epoch": 8.484848484848484,
      "grad_norm": 0.07650060206651688,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 7000
    },
    {
      "epoch": 8.49090909090909,
      "grad_norm": 0.05713680759072304,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 7005
    },
    {
      "epoch": 8.496969696969696,
      "grad_norm": 0.06935583800077438,
      "learning_rate": 2e-05,
      "loss": 0.0386,
      "step": 7010
    },
    {
      "epoch": 8.49939393939394,
      "eval_average": 0.546609660055247,
      "eval_crossner_ai": 0.4932126696340073,
      "eval_crossner_literature": 0.5674418604150016,
      "eval_crossner_music": 0.6767746381304843,
      "eval_crossner_politics": 0.5521669341393465,
      "eval_crossner_science": 0.6356107659956505,
      "eval_mit-movie": 0.5245901638847981,
      "eval_mit-restaurant": 0.3764705881874413,
      "eval_runtime": 36.2549,
      "eval_samples_per_second": 19.308,
      "eval_steps_per_second": 0.193,
      "step": 7012
    },
    {
      "epoch": 8.503030303030304,
      "grad_norm": 0.08235906809568405,
      "learning_rate": 2e-05,
      "loss": 0.0394,
      "step": 7015
    },
    {
      "epoch": 8.50909090909091,
      "grad_norm": 0.05722066015005112,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 7020
    },
    {
      "epoch": 8.515151515151516,
      "grad_norm": 0.06811252236366272,
      "learning_rate": 2e-05,
      "loss": 0.0376,
      "step": 7025
    },
    {
      "epoch": 8.521212121212121,
      "grad_norm": 0.07772858440876007,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 7030
    },
    {
      "epoch": 8.527272727272727,
      "grad_norm": 0.059315621852874756,
      "learning_rate": 2e-05,
      "loss": 0.0366,
      "step": 7035
    },
    {
      "epoch": 8.533333333333333,
      "grad_norm": 0.06783352047204971,
      "learning_rate": 2e-05,
      "loss": 0.0378,
      "step": 7040
    },
    {
      "epoch": 8.539393939393939,
      "grad_norm": 0.07263880968093872,
      "learning_rate": 2e-05,
      "loss": 0.0398,
      "step": 7045
    },
    {
      "epoch": 8.545454545454545,
      "grad_norm": 0.05460241436958313,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 7050
    },
    {
      "epoch": 8.55151515151515,
      "grad_norm": 0.08017434924840927,
      "learning_rate": 2e-05,
      "loss": 0.0362,
      "step": 7055
    },
    {
      "epoch": 8.557575757575757,
      "grad_norm": 0.0718221366405487,
      "learning_rate": 2e-05,
      "loss": 0.0385,
      "step": 7060
    },
    {
      "epoch": 8.563636363636364,
      "grad_norm": 0.051010314375162125,
      "learning_rate": 2e-05,
      "loss": 0.0365,
      "step": 7065
    },
    {
      "epoch": 8.56969696969697,
      "grad_norm": 0.06108296290040016,
      "learning_rate": 2e-05,
      "loss": 0.0376,
      "step": 7070
    },
    {
      "epoch": 8.575757575757576,
      "grad_norm": 0.05247277766466141,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 7075
    },
    {
      "epoch": 8.581818181818182,
      "grad_norm": 0.07456517964601517,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 7080
    },
    {
      "epoch": 8.587878787878788,
      "grad_norm": 0.0793888047337532,
      "learning_rate": 2e-05,
      "loss": 0.0373,
      "step": 7085
    },
    {
      "epoch": 8.593939393939394,
      "grad_norm": 0.09273354709148407,
      "learning_rate": 2e-05,
      "loss": 0.0352,
      "step": 7090
    },
    {
      "epoch": 8.6,
      "grad_norm": 0.12454524636268616,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 7095
    },
    {
      "epoch": 8.606060606060606,
      "grad_norm": 0.10354413092136383,
      "learning_rate": 2e-05,
      "loss": 0.0367,
      "step": 7100
    },
    {
      "epoch": 8.612121212121211,
      "grad_norm": 0.05853302776813507,
      "learning_rate": 2e-05,
      "loss": 0.0377,
      "step": 7105
    },
    {
      "epoch": 8.618181818181819,
      "grad_norm": 0.06492096185684204,
      "learning_rate": 2e-05,
      "loss": 0.0366,
      "step": 7110
    },
    {
      "epoch": 8.624242424242425,
      "grad_norm": 0.07959199696779251,
      "learning_rate": 2e-05,
      "loss": 0.0378,
      "step": 7115
    },
    {
      "epoch": 8.63030303030303,
      "grad_norm": 0.07135902345180511,
      "learning_rate": 2e-05,
      "loss": 0.038,
      "step": 7120
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 0.06961671262979507,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 7125
    },
    {
      "epoch": 8.642424242424243,
      "grad_norm": 0.05604270100593567,
      "learning_rate": 2e-05,
      "loss": 0.0396,
      "step": 7130
    },
    {
      "epoch": 8.648484848484848,
      "grad_norm": 0.07550181448459625,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 7135
    },
    {
      "epoch": 8.654545454545454,
      "grad_norm": 0.06619889289140701,
      "learning_rate": 2e-05,
      "loss": 0.0366,
      "step": 7140
    },
    {
      "epoch": 8.66060606060606,
      "grad_norm": 0.062803253531456,
      "learning_rate": 2e-05,
      "loss": 0.0369,
      "step": 7145
    },
    {
      "epoch": 8.666666666666666,
      "grad_norm": 0.06292985379695892,
      "learning_rate": 2e-05,
      "loss": 0.0394,
      "step": 7150
    },
    {
      "epoch": 8.672727272727272,
      "grad_norm": 0.07448059320449829,
      "learning_rate": 2e-05,
      "loss": 0.0388,
      "step": 7155
    },
    {
      "epoch": 8.67878787878788,
      "grad_norm": 0.07674341648817062,
      "learning_rate": 2e-05,
      "loss": 0.0375,
      "step": 7160
    },
    {
      "epoch": 8.684848484848485,
      "grad_norm": 0.0699254721403122,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 7165
    },
    {
      "epoch": 8.690909090909091,
      "grad_norm": 0.07266099750995636,
      "learning_rate": 2e-05,
      "loss": 0.037,
      "step": 7170
    },
    {
      "epoch": 8.696969696969697,
      "grad_norm": 0.06079838052392006,
      "learning_rate": 2e-05,
      "loss": 0.0385,
      "step": 7175
    },
    {
      "epoch": 8.703030303030303,
      "grad_norm": 0.09297673404216766,
      "learning_rate": 2e-05,
      "loss": 0.0404,
      "step": 7180
    },
    {
      "epoch": 8.709090909090909,
      "grad_norm": 0.09409554302692413,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 7185
    },
    {
      "epoch": 8.715151515151515,
      "grad_norm": 0.0676223486661911,
      "learning_rate": 2e-05,
      "loss": 0.0378,
      "step": 7190
    },
    {
      "epoch": 8.72121212121212,
      "grad_norm": 0.057450924068689346,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 7195
    },
    {
      "epoch": 8.727272727272727,
      "grad_norm": 0.06775377690792084,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 7200
    },
    {
      "epoch": 8.733333333333333,
      "grad_norm": 0.07777692377567291,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 7205
    },
    {
      "epoch": 8.73939393939394,
      "grad_norm": 0.049024589359760284,
      "learning_rate": 2e-05,
      "loss": 0.037,
      "step": 7210
    },
    {
      "epoch": 8.745454545454546,
      "grad_norm": 0.06033145263791084,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 7215
    },
    {
      "epoch": 8.751515151515152,
      "grad_norm": 0.09884617477655411,
      "learning_rate": 2e-05,
      "loss": 0.0384,
      "step": 7220
    },
    {
      "epoch": 8.757575757575758,
      "grad_norm": 0.07041825354099274,
      "learning_rate": 2e-05,
      "loss": 0.038,
      "step": 7225
    },
    {
      "epoch": 8.763636363636364,
      "grad_norm": 0.07934544235467911,
      "learning_rate": 2e-05,
      "loss": 0.0366,
      "step": 7230
    },
    {
      "epoch": 8.76969696969697,
      "grad_norm": 0.0830884575843811,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 7235
    },
    {
      "epoch": 8.775757575757575,
      "grad_norm": 0.0792221873998642,
      "learning_rate": 2e-05,
      "loss": 0.0365,
      "step": 7240
    },
    {
      "epoch": 8.781818181818181,
      "grad_norm": 0.07010115683078766,
      "learning_rate": 2e-05,
      "loss": 0.0339,
      "step": 7245
    },
    {
      "epoch": 8.787878787878787,
      "grad_norm": 0.12119844555854797,
      "learning_rate": 2e-05,
      "loss": 0.0369,
      "step": 7250
    },
    {
      "epoch": 8.793939393939393,
      "grad_norm": 0.05615924298763275,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 7255
    },
    {
      "epoch": 8.8,
      "grad_norm": 0.061844900250434875,
      "learning_rate": 2e-05,
      "loss": 0.0409,
      "step": 7260
    },
    {
      "epoch": 8.806060606060607,
      "grad_norm": 0.06388964504003525,
      "learning_rate": 2e-05,
      "loss": 0.037,
      "step": 7265
    },
    {
      "epoch": 8.812121212121212,
      "grad_norm": 0.06888846307992935,
      "learning_rate": 2e-05,
      "loss": 0.038,
      "step": 7270
    },
    {
      "epoch": 8.818181818181818,
      "grad_norm": 0.07163749635219574,
      "learning_rate": 2e-05,
      "loss": 0.036,
      "step": 7275
    },
    {
      "epoch": 8.824242424242424,
      "grad_norm": 0.0748160257935524,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 7280
    },
    {
      "epoch": 8.83030303030303,
      "grad_norm": 0.05949686840176582,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 7285
    },
    {
      "epoch": 8.836363636363636,
      "grad_norm": 0.08438722789287567,
      "learning_rate": 2e-05,
      "loss": 0.0367,
      "step": 7290
    },
    {
      "epoch": 8.842424242424242,
      "grad_norm": 0.07309660315513611,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 7295
    },
    {
      "epoch": 8.848484848484848,
      "grad_norm": 0.06228306144475937,
      "learning_rate": 2e-05,
      "loss": 0.038,
      "step": 7300
    },
    {
      "epoch": 8.854545454545455,
      "grad_norm": 0.048668429255485535,
      "learning_rate": 2e-05,
      "loss": 0.037,
      "step": 7305
    },
    {
      "epoch": 8.860606060606061,
      "grad_norm": 0.059888314455747604,
      "learning_rate": 2e-05,
      "loss": 0.038,
      "step": 7310
    },
    {
      "epoch": 8.866666666666667,
      "grad_norm": 0.05224710330367088,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 7315
    },
    {
      "epoch": 8.872727272727273,
      "grad_norm": 0.058597877621650696,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 7320
    },
    {
      "epoch": 8.878787878787879,
      "grad_norm": 0.0902194157242775,
      "learning_rate": 2e-05,
      "loss": 0.0348,
      "step": 7325
    },
    {
      "epoch": 8.884848484848485,
      "grad_norm": 0.057649627327919006,
      "learning_rate": 2e-05,
      "loss": 0.0376,
      "step": 7330
    },
    {
      "epoch": 8.89090909090909,
      "grad_norm": 0.09679240733385086,
      "learning_rate": 2e-05,
      "loss": 0.037,
      "step": 7335
    },
    {
      "epoch": 8.896969696969697,
      "grad_norm": 0.05234590545296669,
      "learning_rate": 2e-05,
      "loss": 0.0348,
      "step": 7340
    },
    {
      "epoch": 8.903030303030302,
      "grad_norm": 0.06153740733861923,
      "learning_rate": 2e-05,
      "loss": 0.0375,
      "step": 7345
    },
    {
      "epoch": 8.909090909090908,
      "grad_norm": 0.07434524595737457,
      "learning_rate": 2e-05,
      "loss": 0.0399,
      "step": 7350
    },
    {
      "epoch": 8.915151515151516,
      "grad_norm": 0.07171857357025146,
      "learning_rate": 2e-05,
      "loss": 0.0388,
      "step": 7355
    },
    {
      "epoch": 8.921212121212122,
      "grad_norm": 0.0836423709988594,
      "learning_rate": 2e-05,
      "loss": 0.0367,
      "step": 7360
    },
    {
      "epoch": 8.927272727272728,
      "grad_norm": 0.07069074362516403,
      "learning_rate": 2e-05,
      "loss": 0.0385,
      "step": 7365
    },
    {
      "epoch": 8.933333333333334,
      "grad_norm": 0.09074937552213669,
      "learning_rate": 2e-05,
      "loss": 0.0361,
      "step": 7370
    },
    {
      "epoch": 8.93939393939394,
      "grad_norm": 0.08080684393644333,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 7375
    },
    {
      "epoch": 8.945454545454545,
      "grad_norm": 0.07533455640077591,
      "learning_rate": 2e-05,
      "loss": 0.0372,
      "step": 7380
    },
    {
      "epoch": 8.951515151515151,
      "grad_norm": 0.07182735949754715,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 7385
    },
    {
      "epoch": 8.957575757575757,
      "grad_norm": 0.06565538048744202,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 7390
    },
    {
      "epoch": 8.963636363636363,
      "grad_norm": 0.06797400116920471,
      "learning_rate": 2e-05,
      "loss": 0.0373,
      "step": 7395
    },
    {
      "epoch": 8.969696969696969,
      "grad_norm": 0.0829702839255333,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 7400
    },
    {
      "epoch": 8.975757575757576,
      "grad_norm": 0.07277894020080566,
      "learning_rate": 2e-05,
      "loss": 0.0376,
      "step": 7405
    },
    {
      "epoch": 8.981818181818182,
      "grad_norm": 0.08893908560276031,
      "learning_rate": 2e-05,
      "loss": 0.0373,
      "step": 7410
    },
    {
      "epoch": 8.987878787878788,
      "grad_norm": 0.06162470579147339,
      "learning_rate": 2e-05,
      "loss": 0.0375,
      "step": 7415
    },
    {
      "epoch": 8.993939393939394,
      "grad_norm": 0.08540613949298859,
      "learning_rate": 2e-05,
      "loss": 0.0375,
      "step": 7420
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.059949979186058044,
      "learning_rate": 2e-05,
      "loss": 0.0387,
      "step": 7425
    },
    {
      "epoch": 9.0,
      "eval_average": 0.5260352774807038,
      "eval_crossner_ai": 0.4836895387584665,
      "eval_crossner_literature": 0.5583524026958679,
      "eval_crossner_music": 0.6425591098247392,
      "eval_crossner_politics": 0.5585149313462144,
      "eval_crossner_science": 0.6066945606195008,
      "eval_mit-movie": 0.49315068488193065,
      "eval_mit-restaurant": 0.33928571423820686,
      "eval_runtime": 36.7466,
      "eval_samples_per_second": 19.049,
      "eval_steps_per_second": 0.19,
      "step": 7425
    },
    {
      "epoch": 9.006060606060606,
      "grad_norm": 0.0692012831568718,
      "learning_rate": 2e-05,
      "loss": 0.0348,
      "step": 7430
    },
    {
      "epoch": 9.012121212121212,
      "grad_norm": 0.05484351888298988,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 7435
    },
    {
      "epoch": 9.018181818181818,
      "grad_norm": 0.10812561213970184,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 7440
    },
    {
      "epoch": 9.024242424242424,
      "grad_norm": 0.09486900269985199,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 7445
    },
    {
      "epoch": 9.030303030303031,
      "grad_norm": 0.05384368449449539,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 7450
    },
    {
      "epoch": 9.036363636363637,
      "grad_norm": 0.05459607020020485,
      "learning_rate": 2e-05,
      "loss": 0.0384,
      "step": 7455
    },
    {
      "epoch": 9.042424242424243,
      "grad_norm": 0.06227105110883713,
      "learning_rate": 2e-05,
      "loss": 0.0366,
      "step": 7460
    },
    {
      "epoch": 9.048484848484849,
      "grad_norm": 0.05832022801041603,
      "learning_rate": 2e-05,
      "loss": 0.0365,
      "step": 7465
    },
    {
      "epoch": 9.054545454545455,
      "grad_norm": 0.135147362947464,
      "learning_rate": 2e-05,
      "loss": 0.0394,
      "step": 7470
    },
    {
      "epoch": 9.06060606060606,
      "grad_norm": 0.0879848450422287,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 7475
    },
    {
      "epoch": 9.066666666666666,
      "grad_norm": 0.059603217989206314,
      "learning_rate": 2e-05,
      "loss": 0.039,
      "step": 7480
    },
    {
      "epoch": 9.072727272727272,
      "grad_norm": 0.09815582633018494,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 7485
    },
    {
      "epoch": 9.078787878787878,
      "grad_norm": 0.06690331548452377,
      "learning_rate": 2e-05,
      "loss": 0.0362,
      "step": 7490
    },
    {
      "epoch": 9.084848484848484,
      "grad_norm": 0.09776587784290314,
      "learning_rate": 2e-05,
      "loss": 0.039,
      "step": 7495
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 0.05356934666633606,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 7500
    },
    {
      "epoch": 9.096969696969698,
      "grad_norm": 0.06813405454158783,
      "learning_rate": 2e-05,
      "loss": 0.0363,
      "step": 7505
    },
    {
      "epoch": 9.103030303030303,
      "grad_norm": 0.09668922424316406,
      "learning_rate": 2e-05,
      "loss": 0.0367,
      "step": 7510
    },
    {
      "epoch": 9.10909090909091,
      "grad_norm": 0.06655094027519226,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 7515
    },
    {
      "epoch": 9.115151515151515,
      "grad_norm": 0.12149322032928467,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 7520
    },
    {
      "epoch": 9.121212121212121,
      "grad_norm": 0.06381669640541077,
      "learning_rate": 2e-05,
      "loss": 0.038,
      "step": 7525
    },
    {
      "epoch": 9.127272727272727,
      "grad_norm": 0.07538360357284546,
      "learning_rate": 2e-05,
      "loss": 0.0373,
      "step": 7530
    },
    {
      "epoch": 9.133333333333333,
      "grad_norm": 0.08198513090610504,
      "learning_rate": 2e-05,
      "loss": 0.0393,
      "step": 7535
    },
    {
      "epoch": 9.139393939393939,
      "grad_norm": 0.05077312886714935,
      "learning_rate": 2e-05,
      "loss": 0.0344,
      "step": 7540
    },
    {
      "epoch": 9.145454545454545,
      "grad_norm": 0.06786604970693588,
      "learning_rate": 2e-05,
      "loss": 0.0361,
      "step": 7545
    },
    {
      "epoch": 9.151515151515152,
      "grad_norm": 0.08257638663053513,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 7550
    },
    {
      "epoch": 9.157575757575758,
      "grad_norm": 0.07715490460395813,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 7555
    },
    {
      "epoch": 9.163636363636364,
      "grad_norm": 0.05000684782862663,
      "learning_rate": 2e-05,
      "loss": 0.0372,
      "step": 7560
    },
    {
      "epoch": 9.16969696969697,
      "grad_norm": 0.07949917018413544,
      "learning_rate": 2e-05,
      "loss": 0.0369,
      "step": 7565
    },
    {
      "epoch": 9.175757575757576,
      "grad_norm": 0.06086595728993416,
      "learning_rate": 2e-05,
      "loss": 0.0366,
      "step": 7570
    },
    {
      "epoch": 9.181818181818182,
      "grad_norm": 0.06530802696943283,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 7575
    },
    {
      "epoch": 9.187878787878788,
      "grad_norm": 0.06835327297449112,
      "learning_rate": 2e-05,
      "loss": 0.0365,
      "step": 7580
    },
    {
      "epoch": 9.193939393939393,
      "grad_norm": 0.05546588450670242,
      "learning_rate": 2e-05,
      "loss": 0.0353,
      "step": 7585
    },
    {
      "epoch": 9.2,
      "grad_norm": 0.053930994123220444,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 7590
    },
    {
      "epoch": 9.206060606060607,
      "grad_norm": 0.07839416712522507,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 7595
    },
    {
      "epoch": 9.212121212121213,
      "grad_norm": 0.07041022181510925,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 7600
    },
    {
      "epoch": 9.218181818181819,
      "grad_norm": 0.06777437031269073,
      "learning_rate": 2e-05,
      "loss": 0.0363,
      "step": 7605
    },
    {
      "epoch": 9.224242424242425,
      "grad_norm": 0.07589852064847946,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 7610
    },
    {
      "epoch": 9.23030303030303,
      "grad_norm": 0.06517258286476135,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 7615
    },
    {
      "epoch": 9.236363636363636,
      "grad_norm": 0.06582725793123245,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 7620
    },
    {
      "epoch": 9.242424242424242,
      "grad_norm": 0.06620017439126968,
      "learning_rate": 2e-05,
      "loss": 0.0373,
      "step": 7625
    },
    {
      "epoch": 9.248484848484848,
      "grad_norm": 0.09012220799922943,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 7630
    },
    {
      "epoch": 9.254545454545454,
      "grad_norm": 0.07736538350582123,
      "learning_rate": 2e-05,
      "loss": 0.0352,
      "step": 7635
    },
    {
      "epoch": 9.26060606060606,
      "grad_norm": 0.07945325970649719,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 7640
    },
    {
      "epoch": 9.266666666666667,
      "grad_norm": 0.08399825543165207,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 7645
    },
    {
      "epoch": 9.272727272727273,
      "grad_norm": 0.0975123792886734,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 7650
    },
    {
      "epoch": 9.27878787878788,
      "grad_norm": 0.06422252207994461,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 7655
    },
    {
      "epoch": 9.284848484848485,
      "grad_norm": 0.08148510009050369,
      "learning_rate": 2e-05,
      "loss": 0.0365,
      "step": 7660
    },
    {
      "epoch": 9.290909090909091,
      "grad_norm": 0.050272390246391296,
      "learning_rate": 2e-05,
      "loss": 0.0372,
      "step": 7665
    },
    {
      "epoch": 9.296969696969697,
      "grad_norm": 0.051921967417001724,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 7670
    },
    {
      "epoch": 9.303030303030303,
      "grad_norm": 0.051832232624292374,
      "learning_rate": 2e-05,
      "loss": 0.0362,
      "step": 7675
    },
    {
      "epoch": 9.309090909090909,
      "grad_norm": 0.06910082697868347,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 7680
    },
    {
      "epoch": 9.315151515151515,
      "grad_norm": 0.06420612335205078,
      "learning_rate": 2e-05,
      "loss": 0.037,
      "step": 7685
    },
    {
      "epoch": 9.32121212121212,
      "grad_norm": 0.05509186536073685,
      "learning_rate": 2e-05,
      "loss": 0.0361,
      "step": 7690
    },
    {
      "epoch": 9.327272727272728,
      "grad_norm": 0.06671104580163956,
      "learning_rate": 2e-05,
      "loss": 0.0369,
      "step": 7695
    },
    {
      "epoch": 9.333333333333334,
      "grad_norm": 0.07951301336288452,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 7700
    },
    {
      "epoch": 9.33939393939394,
      "grad_norm": 0.05751718953251839,
      "learning_rate": 2e-05,
      "loss": 0.0371,
      "step": 7705
    },
    {
      "epoch": 9.345454545454546,
      "grad_norm": 0.06010221317410469,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 7710
    },
    {
      "epoch": 9.351515151515152,
      "grad_norm": 0.08020653575658798,
      "learning_rate": 2e-05,
      "loss": 0.0377,
      "step": 7715
    },
    {
      "epoch": 9.357575757575757,
      "grad_norm": 0.05553017929196358,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 7720
    },
    {
      "epoch": 9.363636363636363,
      "grad_norm": 0.06923727691173553,
      "learning_rate": 2e-05,
      "loss": 0.0392,
      "step": 7725
    },
    {
      "epoch": 9.36969696969697,
      "grad_norm": 0.07240588217973709,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 7730
    },
    {
      "epoch": 9.375757575757575,
      "grad_norm": 0.056411199271678925,
      "learning_rate": 2e-05,
      "loss": 0.0361,
      "step": 7735
    },
    {
      "epoch": 9.381818181818181,
      "grad_norm": 0.0537358820438385,
      "learning_rate": 2e-05,
      "loss": 0.038,
      "step": 7740
    },
    {
      "epoch": 9.387878787878789,
      "grad_norm": 0.06568323075771332,
      "learning_rate": 2e-05,
      "loss": 0.0353,
      "step": 7745
    },
    {
      "epoch": 9.393939393939394,
      "grad_norm": 0.0657566487789154,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 7750
    },
    {
      "epoch": 9.4,
      "grad_norm": 0.06322069466114044,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 7755
    },
    {
      "epoch": 9.406060606060606,
      "grad_norm": 0.06571845710277557,
      "learning_rate": 2e-05,
      "loss": 0.0383,
      "step": 7760
    },
    {
      "epoch": 9.412121212121212,
      "grad_norm": 0.05943026766180992,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 7765
    },
    {
      "epoch": 9.418181818181818,
      "grad_norm": 0.06098408252000809,
      "learning_rate": 2e-05,
      "loss": 0.0361,
      "step": 7770
    },
    {
      "epoch": 9.424242424242424,
      "grad_norm": 0.08538319915533066,
      "learning_rate": 2e-05,
      "loss": 0.0394,
      "step": 7775
    },
    {
      "epoch": 9.43030303030303,
      "grad_norm": 0.06641833484172821,
      "learning_rate": 2e-05,
      "loss": 0.0362,
      "step": 7780
    },
    {
      "epoch": 9.436363636363636,
      "grad_norm": 0.08196935802698135,
      "learning_rate": 2e-05,
      "loss": 0.035,
      "step": 7785
    },
    {
      "epoch": 9.442424242424243,
      "grad_norm": 0.0932110920548439,
      "learning_rate": 2e-05,
      "loss": 0.035,
      "step": 7790
    },
    {
      "epoch": 9.44848484848485,
      "grad_norm": 0.09952477365732193,
      "learning_rate": 2e-05,
      "loss": 0.036,
      "step": 7795
    },
    {
      "epoch": 9.454545454545455,
      "grad_norm": 0.05080961436033249,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 7800
    },
    {
      "epoch": 9.460606060606061,
      "grad_norm": 0.0607106052339077,
      "learning_rate": 2e-05,
      "loss": 0.0365,
      "step": 7805
    },
    {
      "epoch": 9.466666666666667,
      "grad_norm": 0.07556666433811188,
      "learning_rate": 2e-05,
      "loss": 0.036,
      "step": 7810
    },
    {
      "epoch": 9.472727272727273,
      "grad_norm": 0.08311734348535538,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 7815
    },
    {
      "epoch": 9.478787878787879,
      "grad_norm": 0.09263867884874344,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 7820
    },
    {
      "epoch": 9.484848484848484,
      "grad_norm": 0.06658779829740524,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 7825
    },
    {
      "epoch": 9.49090909090909,
      "grad_norm": 0.08956949412822723,
      "learning_rate": 2e-05,
      "loss": 0.0356,
      "step": 7830
    },
    {
      "epoch": 9.496969696969696,
      "grad_norm": 0.07210570573806763,
      "learning_rate": 2e-05,
      "loss": 0.037,
      "step": 7835
    },
    {
      "epoch": 9.50060606060606,
      "eval_average": 0.5496985908216393,
      "eval_crossner_ai": 0.4869762173912814,
      "eval_crossner_literature": 0.571428571378444,
      "eval_crossner_music": 0.6827919833638741,
      "eval_crossner_politics": 0.5626515763444262,
      "eval_crossner_science": 0.6182965299184681,
      "eval_mit-movie": 0.525745257402848,
      "eval_mit-restaurant": 0.3999999999521332,
      "eval_runtime": 36.4164,
      "eval_samples_per_second": 19.222,
      "eval_steps_per_second": 0.192,
      "step": 7838
    },
    {
      "epoch": 9.503030303030304,
      "grad_norm": 0.06876339018344879,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 7840
    },
    {
      "epoch": 9.50909090909091,
      "grad_norm": 0.04993756487965584,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 7845
    },
    {
      "epoch": 9.515151515151516,
      "grad_norm": 0.06963396817445755,
      "learning_rate": 2e-05,
      "loss": 0.0365,
      "step": 7850
    },
    {
      "epoch": 9.521212121212121,
      "grad_norm": 0.06930342316627502,
      "learning_rate": 2e-05,
      "loss": 0.0356,
      "step": 7855
    },
    {
      "epoch": 9.527272727272727,
      "grad_norm": 0.07375678420066833,
      "learning_rate": 2e-05,
      "loss": 0.0369,
      "step": 7860
    },
    {
      "epoch": 9.533333333333333,
      "grad_norm": 0.0557105727493763,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 7865
    },
    {
      "epoch": 9.539393939393939,
      "grad_norm": 0.1078275665640831,
      "learning_rate": 2e-05,
      "loss": 0.037,
      "step": 7870
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 0.06003042310476303,
      "learning_rate": 2e-05,
      "loss": 0.035,
      "step": 7875
    },
    {
      "epoch": 9.55151515151515,
      "grad_norm": 0.06987373530864716,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 7880
    },
    {
      "epoch": 9.557575757575757,
      "grad_norm": 0.08343280851840973,
      "learning_rate": 2e-05,
      "loss": 0.0363,
      "step": 7885
    },
    {
      "epoch": 9.563636363636364,
      "grad_norm": 0.07263052463531494,
      "learning_rate": 2e-05,
      "loss": 0.0366,
      "step": 7890
    },
    {
      "epoch": 9.56969696969697,
      "grad_norm": 0.07185333967208862,
      "learning_rate": 2e-05,
      "loss": 0.0376,
      "step": 7895
    },
    {
      "epoch": 9.575757575757576,
      "grad_norm": 0.06514746695756912,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 7900
    },
    {
      "epoch": 9.581818181818182,
      "grad_norm": 0.06329866498708725,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 7905
    },
    {
      "epoch": 9.587878787878788,
      "grad_norm": 0.08489063382148743,
      "learning_rate": 2e-05,
      "loss": 0.0363,
      "step": 7910
    },
    {
      "epoch": 9.593939393939394,
      "grad_norm": 0.08175100386142731,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 7915
    },
    {
      "epoch": 9.6,
      "grad_norm": 0.07127023488283157,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 7920
    },
    {
      "epoch": 9.606060606060606,
      "grad_norm": 0.08028451353311539,
      "learning_rate": 2e-05,
      "loss": 0.0348,
      "step": 7925
    },
    {
      "epoch": 9.612121212121211,
      "grad_norm": 0.049901675432920456,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 7930
    },
    {
      "epoch": 9.618181818181819,
      "grad_norm": 0.05445423722267151,
      "learning_rate": 2e-05,
      "loss": 0.037,
      "step": 7935
    },
    {
      "epoch": 9.624242424242425,
      "grad_norm": 0.0847054049372673,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 7940
    },
    {
      "epoch": 9.63030303030303,
      "grad_norm": 0.08854314684867859,
      "learning_rate": 2e-05,
      "loss": 0.0367,
      "step": 7945
    },
    {
      "epoch": 9.636363636363637,
      "grad_norm": 0.08798287063837051,
      "learning_rate": 2e-05,
      "loss": 0.0372,
      "step": 7950
    },
    {
      "epoch": 9.642424242424243,
      "grad_norm": 0.08015827089548111,
      "learning_rate": 2e-05,
      "loss": 0.0366,
      "step": 7955
    },
    {
      "epoch": 9.648484848484848,
      "grad_norm": 0.05027025192975998,
      "learning_rate": 2e-05,
      "loss": 0.0336,
      "step": 7960
    },
    {
      "epoch": 9.654545454545454,
      "grad_norm": 0.07889017462730408,
      "learning_rate": 2e-05,
      "loss": 0.036,
      "step": 7965
    },
    {
      "epoch": 9.66060606060606,
      "grad_norm": 0.05313864350318909,
      "learning_rate": 2e-05,
      "loss": 0.0376,
      "step": 7970
    },
    {
      "epoch": 9.666666666666666,
      "grad_norm": 0.062378983944654465,
      "learning_rate": 2e-05,
      "loss": 0.0356,
      "step": 7975
    },
    {
      "epoch": 9.672727272727272,
      "grad_norm": 0.05063150078058243,
      "learning_rate": 2e-05,
      "loss": 0.0345,
      "step": 7980
    },
    {
      "epoch": 9.67878787878788,
      "grad_norm": 0.06624194234609604,
      "learning_rate": 2e-05,
      "loss": 0.036,
      "step": 7985
    },
    {
      "epoch": 9.684848484848485,
      "grad_norm": 0.07168786972761154,
      "learning_rate": 2e-05,
      "loss": 0.035,
      "step": 7990
    },
    {
      "epoch": 9.690909090909091,
      "grad_norm": 0.06529130041599274,
      "learning_rate": 2e-05,
      "loss": 0.0378,
      "step": 7995
    },
    {
      "epoch": 9.696969696969697,
      "grad_norm": 0.06532076746225357,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 8000
    },
    {
      "epoch": 9.703030303030303,
      "grad_norm": 0.067130908370018,
      "learning_rate": 2e-05,
      "loss": 0.034,
      "step": 8005
    },
    {
      "epoch": 9.709090909090909,
      "grad_norm": 0.059714749455451965,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 8010
    },
    {
      "epoch": 9.715151515151515,
      "grad_norm": 0.11327677220106125,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 8015
    },
    {
      "epoch": 9.72121212121212,
      "grad_norm": 0.07751626521348953,
      "learning_rate": 2e-05,
      "loss": 0.0356,
      "step": 8020
    },
    {
      "epoch": 9.727272727272727,
      "grad_norm": 0.0892062559723854,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 8025
    },
    {
      "epoch": 9.733333333333333,
      "grad_norm": 0.06256701052188873,
      "learning_rate": 2e-05,
      "loss": 0.0384,
      "step": 8030
    },
    {
      "epoch": 9.73939393939394,
      "grad_norm": 0.06895994395017624,
      "learning_rate": 2e-05,
      "loss": 0.0378,
      "step": 8035
    },
    {
      "epoch": 9.745454545454546,
      "grad_norm": 0.07852846384048462,
      "learning_rate": 2e-05,
      "loss": 0.0374,
      "step": 8040
    },
    {
      "epoch": 9.751515151515152,
      "grad_norm": 0.12223924696445465,
      "learning_rate": 2e-05,
      "loss": 0.037,
      "step": 8045
    },
    {
      "epoch": 9.757575757575758,
      "grad_norm": 0.06881153583526611,
      "learning_rate": 2e-05,
      "loss": 0.0362,
      "step": 8050
    },
    {
      "epoch": 9.763636363636364,
      "grad_norm": 0.08441278338432312,
      "learning_rate": 2e-05,
      "loss": 0.0353,
      "step": 8055
    },
    {
      "epoch": 9.76969696969697,
      "grad_norm": 0.08180771768093109,
      "learning_rate": 2e-05,
      "loss": 0.0365,
      "step": 8060
    },
    {
      "epoch": 9.775757575757575,
      "grad_norm": 0.05563979223370552,
      "learning_rate": 2e-05,
      "loss": 0.0352,
      "step": 8065
    },
    {
      "epoch": 9.781818181818181,
      "grad_norm": 0.06195652112364769,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 8070
    },
    {
      "epoch": 9.787878787878787,
      "grad_norm": 0.06044267490506172,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 8075
    },
    {
      "epoch": 9.793939393939393,
      "grad_norm": 0.06882531195878983,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 8080
    },
    {
      "epoch": 9.8,
      "grad_norm": 0.08473272621631622,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 8085
    },
    {
      "epoch": 9.806060606060607,
      "grad_norm": 0.05947127193212509,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 8090
    },
    {
      "epoch": 9.812121212121212,
      "grad_norm": 0.0790707916021347,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 8095
    },
    {
      "epoch": 9.818181818181818,
      "grad_norm": 0.04342213273048401,
      "learning_rate": 2e-05,
      "loss": 0.037,
      "step": 8100
    },
    {
      "epoch": 9.824242424242424,
      "grad_norm": 0.06520581990480423,
      "learning_rate": 2e-05,
      "loss": 0.0371,
      "step": 8105
    },
    {
      "epoch": 9.83030303030303,
      "grad_norm": 0.079977847635746,
      "learning_rate": 2e-05,
      "loss": 0.0371,
      "step": 8110
    },
    {
      "epoch": 9.836363636363636,
      "grad_norm": 0.10824750363826752,
      "learning_rate": 2e-05,
      "loss": 0.0367,
      "step": 8115
    },
    {
      "epoch": 9.842424242424242,
      "grad_norm": 0.06828770041465759,
      "learning_rate": 2e-05,
      "loss": 0.0383,
      "step": 8120
    },
    {
      "epoch": 9.848484848484848,
      "grad_norm": 0.07242914289236069,
      "learning_rate": 2e-05,
      "loss": 0.0377,
      "step": 8125
    },
    {
      "epoch": 9.854545454545455,
      "grad_norm": 0.06586694717407227,
      "learning_rate": 2e-05,
      "loss": 0.0371,
      "step": 8130
    },
    {
      "epoch": 9.860606060606061,
      "grad_norm": 0.05400095880031586,
      "learning_rate": 2e-05,
      "loss": 0.0356,
      "step": 8135
    },
    {
      "epoch": 9.866666666666667,
      "grad_norm": 0.0710742175579071,
      "learning_rate": 2e-05,
      "loss": 0.0352,
      "step": 8140
    },
    {
      "epoch": 9.872727272727273,
      "grad_norm": 0.06351179629564285,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 8145
    },
    {
      "epoch": 9.878787878787879,
      "grad_norm": 0.04503264278173447,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 8150
    },
    {
      "epoch": 9.884848484848485,
      "grad_norm": 0.12468431890010834,
      "learning_rate": 2e-05,
      "loss": 0.036,
      "step": 8155
    },
    {
      "epoch": 9.89090909090909,
      "grad_norm": 0.05926182121038437,
      "learning_rate": 2e-05,
      "loss": 0.0372,
      "step": 8160
    },
    {
      "epoch": 9.896969696969697,
      "grad_norm": 0.23243235051631927,
      "learning_rate": 2e-05,
      "loss": 0.0363,
      "step": 8165
    },
    {
      "epoch": 9.903030303030302,
      "grad_norm": 0.05993377044796944,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 8170
    },
    {
      "epoch": 9.909090909090908,
      "grad_norm": 0.05101558193564415,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 8175
    },
    {
      "epoch": 9.915151515151516,
      "grad_norm": 0.07792197912931442,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 8180
    },
    {
      "epoch": 9.921212121212122,
      "grad_norm": 0.10977891087532043,
      "learning_rate": 2e-05,
      "loss": 0.0386,
      "step": 8185
    },
    {
      "epoch": 9.927272727272728,
      "grad_norm": 0.056987464427948,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 8190
    },
    {
      "epoch": 9.933333333333334,
      "grad_norm": 0.054209232330322266,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 8195
    },
    {
      "epoch": 9.93939393939394,
      "grad_norm": 0.06979290395975113,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 8200
    },
    {
      "epoch": 9.945454545454545,
      "grad_norm": 0.06448860466480255,
      "learning_rate": 2e-05,
      "loss": 0.0369,
      "step": 8205
    },
    {
      "epoch": 9.951515151515151,
      "grad_norm": 0.061215661466121674,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 8210
    },
    {
      "epoch": 9.957575757575757,
      "grad_norm": 0.09444697201251984,
      "learning_rate": 2e-05,
      "loss": 0.0382,
      "step": 8215
    },
    {
      "epoch": 9.963636363636363,
      "grad_norm": 0.05738440155982971,
      "learning_rate": 2e-05,
      "loss": 0.0367,
      "step": 8220
    },
    {
      "epoch": 9.969696969696969,
      "grad_norm": 0.17595355212688446,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 8225
    },
    {
      "epoch": 9.975757575757576,
      "grad_norm": 0.07549010962247849,
      "learning_rate": 2e-05,
      "loss": 0.036,
      "step": 8230
    },
    {
      "epoch": 9.981818181818182,
      "grad_norm": 0.05957892909646034,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 8235
    },
    {
      "epoch": 9.987878787878788,
      "grad_norm": 0.056622691452503204,
      "learning_rate": 2e-05,
      "loss": 0.035,
      "step": 8240
    },
    {
      "epoch": 9.993939393939394,
      "grad_norm": 0.11240080744028091,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 8245
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.06306196004152298,
      "learning_rate": 2e-05,
      "loss": 0.0342,
      "step": 8250
    },
    {
      "epoch": 10.0,
      "eval_average": 0.5604548525281161,
      "eval_crossner_ai": 0.5034642031837804,
      "eval_crossner_literature": 0.5617715617214528,
      "eval_crossner_music": 0.6898470096856505,
      "eval_crossner_politics": 0.5714285713785086,
      "eval_crossner_science": 0.6381156316415785,
      "eval_mit-movie": 0.5573770491306818,
      "eval_mit-restaurant": 0.4011799409551606,
      "eval_runtime": 35.8676,
      "eval_samples_per_second": 19.516,
      "eval_steps_per_second": 0.195,
      "step": 8250
    },
    {
      "epoch": 10.006060606060606,
      "grad_norm": 0.07045993953943253,
      "learning_rate": 2e-05,
      "loss": 0.0375,
      "step": 8255
    },
    {
      "epoch": 10.012121212121212,
      "grad_norm": 0.07160765677690506,
      "learning_rate": 2e-05,
      "loss": 0.0323,
      "step": 8260
    },
    {
      "epoch": 10.018181818181818,
      "grad_norm": 0.05337976664304733,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 8265
    },
    {
      "epoch": 10.024242424242424,
      "grad_norm": 0.0643855556845665,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 8270
    },
    {
      "epoch": 10.030303030303031,
      "grad_norm": 0.05798095837235451,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 8275
    },
    {
      "epoch": 10.036363636363637,
      "grad_norm": 0.06807353347539902,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 8280
    },
    {
      "epoch": 10.042424242424243,
      "grad_norm": 0.06670030206441879,
      "learning_rate": 2e-05,
      "loss": 0.0363,
      "step": 8285
    },
    {
      "epoch": 10.048484848484849,
      "grad_norm": 0.08452675491571426,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 8290
    },
    {
      "epoch": 10.054545454545455,
      "grad_norm": 0.06137235835194588,
      "learning_rate": 2e-05,
      "loss": 0.0353,
      "step": 8295
    },
    {
      "epoch": 10.06060606060606,
      "grad_norm": 0.057844460010528564,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 8300
    },
    {
      "epoch": 10.066666666666666,
      "grad_norm": 0.05935242027044296,
      "learning_rate": 2e-05,
      "loss": 0.0353,
      "step": 8305
    },
    {
      "epoch": 10.072727272727272,
      "grad_norm": 0.08027797937393188,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 8310
    },
    {
      "epoch": 10.078787878787878,
      "grad_norm": 0.05353584513068199,
      "learning_rate": 2e-05,
      "loss": 0.0344,
      "step": 8315
    },
    {
      "epoch": 10.084848484848484,
      "grad_norm": 0.06041082367300987,
      "learning_rate": 2e-05,
      "loss": 0.0388,
      "step": 8320
    },
    {
      "epoch": 10.090909090909092,
      "grad_norm": 0.05651536583900452,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 8325
    },
    {
      "epoch": 10.096969696969698,
      "grad_norm": 0.06011400744318962,
      "learning_rate": 2e-05,
      "loss": 0.0352,
      "step": 8330
    },
    {
      "epoch": 10.103030303030303,
      "grad_norm": 0.06396784633398056,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 8335
    },
    {
      "epoch": 10.10909090909091,
      "grad_norm": 0.05153815075755119,
      "learning_rate": 2e-05,
      "loss": 0.035,
      "step": 8340
    },
    {
      "epoch": 10.115151515151515,
      "grad_norm": 0.0820695236325264,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 8345
    },
    {
      "epoch": 10.121212121212121,
      "grad_norm": 0.10086359083652496,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 8350
    },
    {
      "epoch": 10.127272727272727,
      "grad_norm": 0.07116901874542236,
      "learning_rate": 2e-05,
      "loss": 0.0356,
      "step": 8355
    },
    {
      "epoch": 10.133333333333333,
      "grad_norm": 0.06180110573768616,
      "learning_rate": 2e-05,
      "loss": 0.0341,
      "step": 8360
    },
    {
      "epoch": 10.139393939393939,
      "grad_norm": 0.0747726783156395,
      "learning_rate": 2e-05,
      "loss": 0.0399,
      "step": 8365
    },
    {
      "epoch": 10.145454545454545,
      "grad_norm": 0.07980061322450638,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 8370
    },
    {
      "epoch": 10.151515151515152,
      "grad_norm": 0.06450752168893814,
      "learning_rate": 2e-05,
      "loss": 0.0367,
      "step": 8375
    },
    {
      "epoch": 10.157575757575758,
      "grad_norm": 0.06774438917636871,
      "learning_rate": 2e-05,
      "loss": 0.0377,
      "step": 8380
    },
    {
      "epoch": 10.163636363636364,
      "grad_norm": 0.060421548783779144,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 8385
    },
    {
      "epoch": 10.16969696969697,
      "grad_norm": 0.06469512730836868,
      "learning_rate": 2e-05,
      "loss": 0.0352,
      "step": 8390
    },
    {
      "epoch": 10.175757575757576,
      "grad_norm": 0.06471338123083115,
      "learning_rate": 2e-05,
      "loss": 0.0333,
      "step": 8395
    },
    {
      "epoch": 10.181818181818182,
      "grad_norm": 0.08747787028551102,
      "learning_rate": 2e-05,
      "loss": 0.0363,
      "step": 8400
    },
    {
      "epoch": 10.187878787878788,
      "grad_norm": 0.052692629396915436,
      "learning_rate": 2e-05,
      "loss": 0.0338,
      "step": 8405
    },
    {
      "epoch": 10.193939393939393,
      "grad_norm": 0.07685012370347977,
      "learning_rate": 2e-05,
      "loss": 0.0373,
      "step": 8410
    },
    {
      "epoch": 10.2,
      "grad_norm": 0.06114453077316284,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 8415
    },
    {
      "epoch": 10.206060606060607,
      "grad_norm": 0.06348665803670883,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 8420
    },
    {
      "epoch": 10.212121212121213,
      "grad_norm": 0.05254657194018364,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 8425
    },
    {
      "epoch": 10.218181818181819,
      "grad_norm": 0.060849614441394806,
      "learning_rate": 2e-05,
      "loss": 0.0342,
      "step": 8430
    },
    {
      "epoch": 10.224242424242425,
      "grad_norm": 0.0635908842086792,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 8435
    },
    {
      "epoch": 10.23030303030303,
      "grad_norm": 0.13551755249500275,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 8440
    },
    {
      "epoch": 10.236363636363636,
      "grad_norm": 0.07391311973333359,
      "learning_rate": 2e-05,
      "loss": 0.0343,
      "step": 8445
    },
    {
      "epoch": 10.242424242424242,
      "grad_norm": 0.06109628826379776,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 8450
    },
    {
      "epoch": 10.248484848484848,
      "grad_norm": 0.08111785352230072,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 8455
    },
    {
      "epoch": 10.254545454545454,
      "grad_norm": 0.0708143562078476,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 8460
    },
    {
      "epoch": 10.26060606060606,
      "grad_norm": 0.08846642822027206,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 8465
    },
    {
      "epoch": 10.266666666666667,
      "grad_norm": 0.1228480413556099,
      "learning_rate": 2e-05,
      "loss": 0.034,
      "step": 8470
    },
    {
      "epoch": 10.272727272727273,
      "grad_norm": 0.052531756460666656,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 8475
    },
    {
      "epoch": 10.27878787878788,
      "grad_norm": 0.06289011240005493,
      "learning_rate": 2e-05,
      "loss": 0.0339,
      "step": 8480
    },
    {
      "epoch": 10.284848484848485,
      "grad_norm": 0.06702423840761185,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 8485
    },
    {
      "epoch": 10.290909090909091,
      "grad_norm": 0.06492431461811066,
      "learning_rate": 2e-05,
      "loss": 0.0335,
      "step": 8490
    },
    {
      "epoch": 10.296969696969697,
      "grad_norm": 0.07374793291091919,
      "learning_rate": 2e-05,
      "loss": 0.0377,
      "step": 8495
    },
    {
      "epoch": 10.303030303030303,
      "grad_norm": 0.05934082344174385,
      "learning_rate": 2e-05,
      "loss": 0.0352,
      "step": 8500
    },
    {
      "epoch": 10.309090909090909,
      "grad_norm": 0.06567947566509247,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 8505
    },
    {
      "epoch": 10.315151515151515,
      "grad_norm": 0.047277919948101044,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 8510
    },
    {
      "epoch": 10.32121212121212,
      "grad_norm": 0.07660836726427078,
      "learning_rate": 2e-05,
      "loss": 0.0371,
      "step": 8515
    },
    {
      "epoch": 10.327272727272728,
      "grad_norm": 0.07880470901727676,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 8520
    },
    {
      "epoch": 10.333333333333334,
      "grad_norm": 0.1353038251399994,
      "learning_rate": 2e-05,
      "loss": 0.0381,
      "step": 8525
    },
    {
      "epoch": 10.33939393939394,
      "grad_norm": 0.05458790063858032,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 8530
    },
    {
      "epoch": 10.345454545454546,
      "grad_norm": 0.06073189899325371,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 8535
    },
    {
      "epoch": 10.351515151515152,
      "grad_norm": 0.07267476618289948,
      "learning_rate": 2e-05,
      "loss": 0.0336,
      "step": 8540
    },
    {
      "epoch": 10.357575757575757,
      "grad_norm": 0.06704974919557571,
      "learning_rate": 2e-05,
      "loss": 0.0366,
      "step": 8545
    },
    {
      "epoch": 10.363636363636363,
      "grad_norm": 0.05392264574766159,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 8550
    },
    {
      "epoch": 10.36969696969697,
      "grad_norm": 0.05853956565260887,
      "learning_rate": 2e-05,
      "loss": 0.036,
      "step": 8555
    },
    {
      "epoch": 10.375757575757575,
      "grad_norm": 0.052804991602897644,
      "learning_rate": 2e-05,
      "loss": 0.0356,
      "step": 8560
    },
    {
      "epoch": 10.381818181818181,
      "grad_norm": 0.053022462874650955,
      "learning_rate": 2e-05,
      "loss": 0.0336,
      "step": 8565
    },
    {
      "epoch": 10.387878787878789,
      "grad_norm": 0.0602053627371788,
      "learning_rate": 2e-05,
      "loss": 0.0379,
      "step": 8570
    },
    {
      "epoch": 10.393939393939394,
      "grad_norm": 0.0684080496430397,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 8575
    },
    {
      "epoch": 10.4,
      "grad_norm": 0.05770762264728546,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 8580
    },
    {
      "epoch": 10.406060606060606,
      "grad_norm": 0.05647977069020271,
      "learning_rate": 2e-05,
      "loss": 0.0348,
      "step": 8585
    },
    {
      "epoch": 10.412121212121212,
      "grad_norm": 0.08048047870397568,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 8590
    },
    {
      "epoch": 10.418181818181818,
      "grad_norm": 0.09785835444927216,
      "learning_rate": 2e-05,
      "loss": 0.0373,
      "step": 8595
    },
    {
      "epoch": 10.424242424242424,
      "grad_norm": 0.06985962390899658,
      "learning_rate": 2e-05,
      "loss": 0.0348,
      "step": 8600
    },
    {
      "epoch": 10.43030303030303,
      "grad_norm": 0.06155668571591377,
      "learning_rate": 2e-05,
      "loss": 0.0345,
      "step": 8605
    },
    {
      "epoch": 10.436363636363636,
      "grad_norm": 0.0853213369846344,
      "learning_rate": 2e-05,
      "loss": 0.0342,
      "step": 8610
    },
    {
      "epoch": 10.442424242424243,
      "grad_norm": 0.0747256726026535,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 8615
    },
    {
      "epoch": 10.44848484848485,
      "grad_norm": 0.06729007512331009,
      "learning_rate": 2e-05,
      "loss": 0.034,
      "step": 8620
    },
    {
      "epoch": 10.454545454545455,
      "grad_norm": 0.09745758026838303,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 8625
    },
    {
      "epoch": 10.460606060606061,
      "grad_norm": 0.06079762801527977,
      "learning_rate": 2e-05,
      "loss": 0.0338,
      "step": 8630
    },
    {
      "epoch": 10.466666666666667,
      "grad_norm": 0.10671280324459076,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 8635
    },
    {
      "epoch": 10.472727272727273,
      "grad_norm": 0.05745483562350273,
      "learning_rate": 2e-05,
      "loss": 0.0335,
      "step": 8640
    },
    {
      "epoch": 10.478787878787879,
      "grad_norm": 0.09027478098869324,
      "learning_rate": 2e-05,
      "loss": 0.0348,
      "step": 8645
    },
    {
      "epoch": 10.484848484848484,
      "grad_norm": 0.054434943944215775,
      "learning_rate": 2e-05,
      "loss": 0.036,
      "step": 8650
    },
    {
      "epoch": 10.49090909090909,
      "grad_norm": 0.057137370109558105,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 8655
    },
    {
      "epoch": 10.496969696969696,
      "grad_norm": 0.08497217297554016,
      "learning_rate": 2e-05,
      "loss": 0.0338,
      "step": 8660
    },
    {
      "epoch": 10.49939393939394,
      "eval_average": 0.5514724441673461,
      "eval_crossner_ai": 0.49206349201421573,
      "eval_crossner_literature": 0.5638051043582349,
      "eval_crossner_music": 0.6772413792602628,
      "eval_crossner_politics": 0.5364291432646007,
      "eval_crossner_science": 0.632911392355058,
      "eval_mit-movie": 0.5543478260372461,
      "eval_mit-restaurant": 0.40350877188180473,
      "eval_runtime": 36.2615,
      "eval_samples_per_second": 19.304,
      "eval_steps_per_second": 0.193,
      "step": 8662
    },
    {
      "epoch": 10.503030303030304,
      "grad_norm": 0.08156637102365494,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 8665
    },
    {
      "epoch": 10.50909090909091,
      "grad_norm": 0.06936068832874298,
      "learning_rate": 2e-05,
      "loss": 0.0353,
      "step": 8670
    },
    {
      "epoch": 10.515151515151516,
      "grad_norm": 0.05864539369940758,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 8675
    },
    {
      "epoch": 10.521212121212121,
      "grad_norm": 0.062349822372198105,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 8680
    },
    {
      "epoch": 10.527272727272727,
      "grad_norm": 0.06831477582454681,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 8685
    },
    {
      "epoch": 10.533333333333333,
      "grad_norm": 0.055887334048748016,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 8690
    },
    {
      "epoch": 10.539393939393939,
      "grad_norm": 0.061256155371665955,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 8695
    },
    {
      "epoch": 10.545454545454545,
      "grad_norm": 0.05276453495025635,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 8700
    },
    {
      "epoch": 10.55151515151515,
      "grad_norm": 0.05928725004196167,
      "learning_rate": 2e-05,
      "loss": 0.0344,
      "step": 8705
    },
    {
      "epoch": 10.557575757575757,
      "grad_norm": 0.05315482243895531,
      "learning_rate": 2e-05,
      "loss": 0.0353,
      "step": 8710
    },
    {
      "epoch": 10.563636363636364,
      "grad_norm": 0.05120604857802391,
      "learning_rate": 2e-05,
      "loss": 0.0341,
      "step": 8715
    },
    {
      "epoch": 10.56969696969697,
      "grad_norm": 0.06960416585206985,
      "learning_rate": 2e-05,
      "loss": 0.0343,
      "step": 8720
    },
    {
      "epoch": 10.575757575757576,
      "grad_norm": 0.056335411965847015,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 8725
    },
    {
      "epoch": 10.581818181818182,
      "grad_norm": 0.05362728610634804,
      "learning_rate": 2e-05,
      "loss": 0.035,
      "step": 8730
    },
    {
      "epoch": 10.587878787878788,
      "grad_norm": 0.0639548972249031,
      "learning_rate": 2e-05,
      "loss": 0.0326,
      "step": 8735
    },
    {
      "epoch": 10.593939393939394,
      "grad_norm": 0.06033525615930557,
      "learning_rate": 2e-05,
      "loss": 0.0363,
      "step": 8740
    },
    {
      "epoch": 10.6,
      "grad_norm": 0.07596589624881744,
      "learning_rate": 2e-05,
      "loss": 0.0342,
      "step": 8745
    },
    {
      "epoch": 10.606060606060606,
      "grad_norm": 0.07151444256305695,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 8750
    },
    {
      "epoch": 10.612121212121211,
      "grad_norm": 0.08867489546537399,
      "learning_rate": 2e-05,
      "loss": 0.0371,
      "step": 8755
    },
    {
      "epoch": 10.618181818181819,
      "grad_norm": 0.08657610416412354,
      "learning_rate": 2e-05,
      "loss": 0.0365,
      "step": 8760
    },
    {
      "epoch": 10.624242424242425,
      "grad_norm": 0.07510240375995636,
      "learning_rate": 2e-05,
      "loss": 0.0368,
      "step": 8765
    },
    {
      "epoch": 10.63030303030303,
      "grad_norm": 0.05272945016622543,
      "learning_rate": 2e-05,
      "loss": 0.0335,
      "step": 8770
    },
    {
      "epoch": 10.636363636363637,
      "grad_norm": 0.0556483156979084,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 8775
    },
    {
      "epoch": 10.642424242424243,
      "grad_norm": 0.06464412063360214,
      "learning_rate": 2e-05,
      "loss": 0.0342,
      "step": 8780
    },
    {
      "epoch": 10.648484848484848,
      "grad_norm": 0.07899536937475204,
      "learning_rate": 2e-05,
      "loss": 0.0338,
      "step": 8785
    },
    {
      "epoch": 10.654545454545454,
      "grad_norm": 0.05449217930436134,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 8790
    },
    {
      "epoch": 10.66060606060606,
      "grad_norm": 0.06560563296079636,
      "learning_rate": 2e-05,
      "loss": 0.0365,
      "step": 8795
    },
    {
      "epoch": 10.666666666666666,
      "grad_norm": 0.0658951848745346,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 8800
    },
    {
      "epoch": 10.672727272727272,
      "grad_norm": 0.06289789825677872,
      "learning_rate": 2e-05,
      "loss": 0.0345,
      "step": 8805
    },
    {
      "epoch": 10.67878787878788,
      "grad_norm": 0.09711417555809021,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 8810
    },
    {
      "epoch": 10.684848484848485,
      "grad_norm": 0.09256220608949661,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 8815
    },
    {
      "epoch": 10.690909090909091,
      "grad_norm": 0.06598836928606033,
      "learning_rate": 2e-05,
      "loss": 0.0338,
      "step": 8820
    },
    {
      "epoch": 10.696969696969697,
      "grad_norm": 0.08827780187129974,
      "learning_rate": 2e-05,
      "loss": 0.0361,
      "step": 8825
    },
    {
      "epoch": 10.703030303030303,
      "grad_norm": 0.08930052071809769,
      "learning_rate": 2e-05,
      "loss": 0.0369,
      "step": 8830
    },
    {
      "epoch": 10.709090909090909,
      "grad_norm": 0.06797231733798981,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 8835
    },
    {
      "epoch": 10.715151515151515,
      "grad_norm": 0.07256189733743668,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 8840
    },
    {
      "epoch": 10.72121212121212,
      "grad_norm": 0.08067004382610321,
      "learning_rate": 2e-05,
      "loss": 0.0332,
      "step": 8845
    },
    {
      "epoch": 10.727272727272727,
      "grad_norm": 0.07399828732013702,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 8850
    },
    {
      "epoch": 10.733333333333333,
      "grad_norm": 0.10137246549129486,
      "learning_rate": 2e-05,
      "loss": 0.0356,
      "step": 8855
    },
    {
      "epoch": 10.73939393939394,
      "grad_norm": 0.16506972908973694,
      "learning_rate": 2e-05,
      "loss": 0.0356,
      "step": 8860
    },
    {
      "epoch": 10.745454545454546,
      "grad_norm": 0.062434207648038864,
      "learning_rate": 2e-05,
      "loss": 0.0354,
      "step": 8865
    },
    {
      "epoch": 10.751515151515152,
      "grad_norm": 0.08467533439397812,
      "learning_rate": 2e-05,
      "loss": 0.0356,
      "step": 8870
    },
    {
      "epoch": 10.757575757575758,
      "grad_norm": 0.052738290280103683,
      "learning_rate": 2e-05,
      "loss": 0.034,
      "step": 8875
    },
    {
      "epoch": 10.763636363636364,
      "grad_norm": 0.06483982503414154,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 8880
    },
    {
      "epoch": 10.76969696969697,
      "grad_norm": 0.05598549544811249,
      "learning_rate": 2e-05,
      "loss": 0.0352,
      "step": 8885
    },
    {
      "epoch": 10.775757575757575,
      "grad_norm": 0.06063396856188774,
      "learning_rate": 2e-05,
      "loss": 0.0352,
      "step": 8890
    },
    {
      "epoch": 10.781818181818181,
      "grad_norm": 0.052525024861097336,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 8895
    },
    {
      "epoch": 10.787878787878787,
      "grad_norm": 0.05048929527401924,
      "learning_rate": 2e-05,
      "loss": 0.0363,
      "step": 8900
    },
    {
      "epoch": 10.793939393939393,
      "grad_norm": 0.053058136254549026,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 8905
    },
    {
      "epoch": 10.8,
      "grad_norm": 0.05418204888701439,
      "learning_rate": 2e-05,
      "loss": 0.0338,
      "step": 8910
    },
    {
      "epoch": 10.806060606060607,
      "grad_norm": 0.09113385528326035,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 8915
    },
    {
      "epoch": 10.812121212121212,
      "grad_norm": 0.07427836954593658,
      "learning_rate": 2e-05,
      "loss": 0.0366,
      "step": 8920
    },
    {
      "epoch": 10.818181818181818,
      "grad_norm": 0.07447119802236557,
      "learning_rate": 2e-05,
      "loss": 0.0364,
      "step": 8925
    },
    {
      "epoch": 10.824242424242424,
      "grad_norm": 0.06201273575425148,
      "learning_rate": 2e-05,
      "loss": 0.0342,
      "step": 8930
    },
    {
      "epoch": 10.83030303030303,
      "grad_norm": 0.07626570761203766,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 8935
    },
    {
      "epoch": 10.836363636363636,
      "grad_norm": 0.06534110754728317,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 8940
    },
    {
      "epoch": 10.842424242424242,
      "grad_norm": 0.05215996131300926,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 8945
    },
    {
      "epoch": 10.848484848484848,
      "grad_norm": 0.04827525094151497,
      "learning_rate": 2e-05,
      "loss": 0.0345,
      "step": 8950
    },
    {
      "epoch": 10.854545454545455,
      "grad_norm": 0.04101363196969032,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 8955
    },
    {
      "epoch": 10.860606060606061,
      "grad_norm": 0.0673036128282547,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 8960
    },
    {
      "epoch": 10.866666666666667,
      "grad_norm": 0.05762849748134613,
      "learning_rate": 2e-05,
      "loss": 0.0339,
      "step": 8965
    },
    {
      "epoch": 10.872727272727273,
      "grad_norm": 0.08397756516933441,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 8970
    },
    {
      "epoch": 10.878787878787879,
      "grad_norm": 0.06611070036888123,
      "learning_rate": 2e-05,
      "loss": 0.035,
      "step": 8975
    },
    {
      "epoch": 10.884848484848485,
      "grad_norm": 0.05299513787031174,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 8980
    },
    {
      "epoch": 10.89090909090909,
      "grad_norm": 0.1582731157541275,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 8985
    },
    {
      "epoch": 10.896969696969697,
      "grad_norm": 0.12266021966934204,
      "learning_rate": 2e-05,
      "loss": 0.0343,
      "step": 8990
    },
    {
      "epoch": 10.903030303030302,
      "grad_norm": 0.05005129426717758,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 8995
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 0.06471771746873856,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 9000
    },
    {
      "epoch": 10.915151515151516,
      "grad_norm": 0.057976365089416504,
      "learning_rate": 2e-05,
      "loss": 0.0372,
      "step": 9005
    },
    {
      "epoch": 10.921212121212122,
      "grad_norm": 0.07037782669067383,
      "learning_rate": 2e-05,
      "loss": 0.0345,
      "step": 9010
    },
    {
      "epoch": 10.927272727272728,
      "grad_norm": 0.10195526480674744,
      "learning_rate": 2e-05,
      "loss": 0.036,
      "step": 9015
    },
    {
      "epoch": 10.933333333333334,
      "grad_norm": 0.06719958037137985,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 9020
    },
    {
      "epoch": 10.93939393939394,
      "grad_norm": 0.06564094126224518,
      "learning_rate": 2e-05,
      "loss": 0.0353,
      "step": 9025
    },
    {
      "epoch": 10.945454545454545,
      "grad_norm": 0.06630858033895493,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 9030
    },
    {
      "epoch": 10.951515151515151,
      "grad_norm": 0.05503815412521362,
      "learning_rate": 2e-05,
      "loss": 0.035,
      "step": 9035
    },
    {
      "epoch": 10.957575757575757,
      "grad_norm": 0.062370408326387405,
      "learning_rate": 2e-05,
      "loss": 0.0335,
      "step": 9040
    },
    {
      "epoch": 10.963636363636363,
      "grad_norm": 0.07081656903028488,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 9045
    },
    {
      "epoch": 10.969696969696969,
      "grad_norm": 0.08671484142541885,
      "learning_rate": 2e-05,
      "loss": 0.0359,
      "step": 9050
    },
    {
      "epoch": 10.975757575757576,
      "grad_norm": 0.24297195672988892,
      "learning_rate": 2e-05,
      "loss": 0.0366,
      "step": 9055
    },
    {
      "epoch": 10.981818181818182,
      "grad_norm": 0.16813303530216217,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 9060
    },
    {
      "epoch": 10.987878787878788,
      "grad_norm": 0.08351045101881027,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 9065
    },
    {
      "epoch": 10.993939393939394,
      "grad_norm": 0.059402164071798325,
      "learning_rate": 2e-05,
      "loss": 0.0348,
      "step": 9070
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.10539423674345016,
      "learning_rate": 2e-05,
      "loss": 0.0338,
      "step": 9075
    },
    {
      "epoch": 11.0,
      "eval_average": 0.5485595894127892,
      "eval_crossner_ai": 0.48813559317110256,
      "eval_crossner_literature": 0.5635103925595762,
      "eval_crossner_music": 0.6740638002273037,
      "eval_crossner_politics": 0.5563549160170964,
      "eval_crossner_science": 0.6332622600778789,
      "eval_mit-movie": 0.5245901638847981,
      "eval_mit-restaurant": 0.3999999999517681,
      "eval_runtime": 36.2775,
      "eval_samples_per_second": 19.296,
      "eval_steps_per_second": 0.193,
      "step": 9075
    },
    {
      "epoch": 11.006060606060606,
      "grad_norm": 0.07653152197599411,
      "learning_rate": 2e-05,
      "loss": 0.0323,
      "step": 9080
    },
    {
      "epoch": 11.012121212121212,
      "grad_norm": 0.05073432996869087,
      "learning_rate": 2e-05,
      "loss": 0.0343,
      "step": 9085
    },
    {
      "epoch": 11.018181818181818,
      "grad_norm": 0.0641869306564331,
      "learning_rate": 2e-05,
      "loss": 0.0323,
      "step": 9090
    },
    {
      "epoch": 11.024242424242424,
      "grad_norm": 0.05616452172398567,
      "learning_rate": 2e-05,
      "loss": 0.0336,
      "step": 9095
    },
    {
      "epoch": 11.030303030303031,
      "grad_norm": 0.05868394300341606,
      "learning_rate": 2e-05,
      "loss": 0.0343,
      "step": 9100
    },
    {
      "epoch": 11.036363636363637,
      "grad_norm": 0.10222932696342468,
      "learning_rate": 2e-05,
      "loss": 0.0344,
      "step": 9105
    },
    {
      "epoch": 11.042424242424243,
      "grad_norm": 0.05593447759747505,
      "learning_rate": 2e-05,
      "loss": 0.0348,
      "step": 9110
    },
    {
      "epoch": 11.048484848484849,
      "grad_norm": 0.06995227932929993,
      "learning_rate": 2e-05,
      "loss": 0.0338,
      "step": 9115
    },
    {
      "epoch": 11.054545454545455,
      "grad_norm": 0.06706847250461578,
      "learning_rate": 2e-05,
      "loss": 0.0331,
      "step": 9120
    },
    {
      "epoch": 11.06060606060606,
      "grad_norm": 0.10770189017057419,
      "learning_rate": 2e-05,
      "loss": 0.0331,
      "step": 9125
    },
    {
      "epoch": 11.066666666666666,
      "grad_norm": 0.06048400327563286,
      "learning_rate": 2e-05,
      "loss": 0.0365,
      "step": 9130
    },
    {
      "epoch": 11.072727272727272,
      "grad_norm": 0.05730077996850014,
      "learning_rate": 2e-05,
      "loss": 0.0339,
      "step": 9135
    },
    {
      "epoch": 11.078787878787878,
      "grad_norm": 0.07158230245113373,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 9140
    },
    {
      "epoch": 11.084848484848484,
      "grad_norm": 0.05587439611554146,
      "learning_rate": 2e-05,
      "loss": 0.0342,
      "step": 9145
    },
    {
      "epoch": 11.090909090909092,
      "grad_norm": 0.08079665899276733,
      "learning_rate": 2e-05,
      "loss": 0.0357,
      "step": 9150
    },
    {
      "epoch": 11.096969696969698,
      "grad_norm": 0.058018237352371216,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 9155
    },
    {
      "epoch": 11.103030303030303,
      "grad_norm": 0.07105304300785065,
      "learning_rate": 2e-05,
      "loss": 0.0343,
      "step": 9160
    },
    {
      "epoch": 11.10909090909091,
      "grad_norm": 0.08576417714357376,
      "learning_rate": 2e-05,
      "loss": 0.0332,
      "step": 9165
    },
    {
      "epoch": 11.115151515151515,
      "grad_norm": 0.07425839453935623,
      "learning_rate": 2e-05,
      "loss": 0.0339,
      "step": 9170
    },
    {
      "epoch": 11.121212121212121,
      "grad_norm": 0.0667019858956337,
      "learning_rate": 2e-05,
      "loss": 0.0348,
      "step": 9175
    },
    {
      "epoch": 11.127272727272727,
      "grad_norm": 0.06418083608150482,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 9180
    },
    {
      "epoch": 11.133333333333333,
      "grad_norm": 0.08051218837499619,
      "learning_rate": 2e-05,
      "loss": 0.0344,
      "step": 9185
    },
    {
      "epoch": 11.139393939393939,
      "grad_norm": 0.06728152930736542,
      "learning_rate": 2e-05,
      "loss": 0.0335,
      "step": 9190
    },
    {
      "epoch": 11.145454545454545,
      "grad_norm": 0.06386993825435638,
      "learning_rate": 2e-05,
      "loss": 0.0365,
      "step": 9195
    },
    {
      "epoch": 11.151515151515152,
      "grad_norm": 0.09009920805692673,
      "learning_rate": 2e-05,
      "loss": 0.0342,
      "step": 9200
    },
    {
      "epoch": 11.157575757575758,
      "grad_norm": 0.05656999349594116,
      "learning_rate": 2e-05,
      "loss": 0.0343,
      "step": 9205
    },
    {
      "epoch": 11.163636363636364,
      "grad_norm": 0.05707170441746712,
      "learning_rate": 2e-05,
      "loss": 0.0362,
      "step": 9210
    },
    {
      "epoch": 11.16969696969697,
      "grad_norm": 0.05176430195569992,
      "learning_rate": 2e-05,
      "loss": 0.0332,
      "step": 9215
    },
    {
      "epoch": 11.175757575757576,
      "grad_norm": 0.06307116895914078,
      "learning_rate": 2e-05,
      "loss": 0.0337,
      "step": 9220
    },
    {
      "epoch": 11.181818181818182,
      "grad_norm": 0.07754940539598465,
      "learning_rate": 2e-05,
      "loss": 0.0327,
      "step": 9225
    },
    {
      "epoch": 11.187878787878788,
      "grad_norm": 0.055052246898412704,
      "learning_rate": 2e-05,
      "loss": 0.0352,
      "step": 9230
    },
    {
      "epoch": 11.193939393939393,
      "grad_norm": 0.0650998204946518,
      "learning_rate": 2e-05,
      "loss": 0.0344,
      "step": 9235
    },
    {
      "epoch": 11.2,
      "grad_norm": 0.12160302698612213,
      "learning_rate": 2e-05,
      "loss": 0.0366,
      "step": 9240
    },
    {
      "epoch": 11.206060606060607,
      "grad_norm": 0.08589264750480652,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 9245
    },
    {
      "epoch": 11.212121212121213,
      "grad_norm": 0.07140443474054337,
      "learning_rate": 2e-05,
      "loss": 0.0371,
      "step": 9250
    },
    {
      "epoch": 11.218181818181819,
      "grad_norm": 0.062275879085063934,
      "learning_rate": 2e-05,
      "loss": 0.0351,
      "step": 9255
    },
    {
      "epoch": 11.224242424242425,
      "grad_norm": 0.04661446809768677,
      "learning_rate": 2e-05,
      "loss": 0.0319,
      "step": 9260
    },
    {
      "epoch": 11.23030303030303,
      "grad_norm": 0.07274936139583588,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 9265
    },
    {
      "epoch": 11.236363636363636,
      "grad_norm": 0.0631476417183876,
      "learning_rate": 2e-05,
      "loss": 0.0342,
      "step": 9270
    },
    {
      "epoch": 11.242424242424242,
      "grad_norm": 0.08337438851594925,
      "learning_rate": 2e-05,
      "loss": 0.0339,
      "step": 9275
    },
    {
      "epoch": 11.248484848484848,
      "grad_norm": 0.061854660511016846,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 9280
    },
    {
      "epoch": 11.254545454545454,
      "grad_norm": 0.06839317083358765,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 9285
    },
    {
      "epoch": 11.26060606060606,
      "grad_norm": 0.13381442427635193,
      "learning_rate": 2e-05,
      "loss": 0.0348,
      "step": 9290
    },
    {
      "epoch": 11.266666666666667,
      "grad_norm": 0.07070592790842056,
      "learning_rate": 2e-05,
      "loss": 0.0335,
      "step": 9295
    },
    {
      "epoch": 11.272727272727273,
      "grad_norm": 0.060118500143289566,
      "learning_rate": 2e-05,
      "loss": 0.0336,
      "step": 9300
    },
    {
      "epoch": 11.27878787878788,
      "grad_norm": 0.07994817197322845,
      "learning_rate": 2e-05,
      "loss": 0.0335,
      "step": 9305
    },
    {
      "epoch": 11.284848484848485,
      "grad_norm": 0.061955783516168594,
      "learning_rate": 2e-05,
      "loss": 0.0339,
      "step": 9310
    },
    {
      "epoch": 11.290909090909091,
      "grad_norm": 0.05727755278348923,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 9315
    },
    {
      "epoch": 11.296969696969697,
      "grad_norm": 0.04960627108812332,
      "learning_rate": 2e-05,
      "loss": 0.0327,
      "step": 9320
    },
    {
      "epoch": 11.303030303030303,
      "grad_norm": 0.06478551775217056,
      "learning_rate": 2e-05,
      "loss": 0.0337,
      "step": 9325
    },
    {
      "epoch": 11.309090909090909,
      "grad_norm": 0.0701477900147438,
      "learning_rate": 2e-05,
      "loss": 0.0333,
      "step": 9330
    },
    {
      "epoch": 11.315151515151515,
      "grad_norm": 0.06651836633682251,
      "learning_rate": 2e-05,
      "loss": 0.0352,
      "step": 9335
    },
    {
      "epoch": 11.32121212121212,
      "grad_norm": 0.07331939041614532,
      "learning_rate": 2e-05,
      "loss": 0.035,
      "step": 9340
    },
    {
      "epoch": 11.327272727272728,
      "grad_norm": 0.057014357298612595,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 9345
    },
    {
      "epoch": 11.333333333333334,
      "grad_norm": 0.08552021533250809,
      "learning_rate": 2e-05,
      "loss": 0.0353,
      "step": 9350
    },
    {
      "epoch": 11.33939393939394,
      "grad_norm": 0.06341728568077087,
      "learning_rate": 2e-05,
      "loss": 0.0329,
      "step": 9355
    },
    {
      "epoch": 11.345454545454546,
      "grad_norm": 0.05560430884361267,
      "learning_rate": 2e-05,
      "loss": 0.0324,
      "step": 9360
    },
    {
      "epoch": 11.351515151515152,
      "grad_norm": 0.05429435521364212,
      "learning_rate": 2e-05,
      "loss": 0.0326,
      "step": 9365
    },
    {
      "epoch": 11.357575757575757,
      "grad_norm": 0.06525517255067825,
      "learning_rate": 2e-05,
      "loss": 0.0343,
      "step": 9370
    },
    {
      "epoch": 11.363636363636363,
      "grad_norm": 0.05056731775403023,
      "learning_rate": 2e-05,
      "loss": 0.0341,
      "step": 9375
    },
    {
      "epoch": 11.36969696969697,
      "grad_norm": 0.06970223039388657,
      "learning_rate": 2e-05,
      "loss": 0.0339,
      "step": 9380
    },
    {
      "epoch": 11.375757575757575,
      "grad_norm": 0.04593997448682785,
      "learning_rate": 2e-05,
      "loss": 0.0341,
      "step": 9385
    },
    {
      "epoch": 11.381818181818181,
      "grad_norm": 0.05524768307805061,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 9390
    },
    {
      "epoch": 11.387878787878789,
      "grad_norm": 0.07135839015245438,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 9395
    },
    {
      "epoch": 11.393939393939394,
      "grad_norm": 0.05587061494588852,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 9400
    },
    {
      "epoch": 11.4,
      "grad_norm": 0.10183482617139816,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 9405
    },
    {
      "epoch": 11.406060606060606,
      "grad_norm": 0.05376429855823517,
      "learning_rate": 2e-05,
      "loss": 0.0358,
      "step": 9410
    },
    {
      "epoch": 11.412121212121212,
      "grad_norm": 0.05618961155414581,
      "learning_rate": 2e-05,
      "loss": 0.0341,
      "step": 9415
    },
    {
      "epoch": 11.418181818181818,
      "grad_norm": 0.048928115516901016,
      "learning_rate": 2e-05,
      "loss": 0.0333,
      "step": 9420
    },
    {
      "epoch": 11.424242424242424,
      "grad_norm": 0.06062551587820053,
      "learning_rate": 2e-05,
      "loss": 0.0335,
      "step": 9425
    },
    {
      "epoch": 11.43030303030303,
      "grad_norm": 0.06843335181474686,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 9430
    },
    {
      "epoch": 11.436363636363636,
      "grad_norm": 0.04969555512070656,
      "learning_rate": 2e-05,
      "loss": 0.0328,
      "step": 9435
    },
    {
      "epoch": 11.442424242424243,
      "grad_norm": 0.11370905488729477,
      "learning_rate": 2e-05,
      "loss": 0.0355,
      "step": 9440
    },
    {
      "epoch": 11.44848484848485,
      "grad_norm": 0.05745076388120651,
      "learning_rate": 2e-05,
      "loss": 0.0339,
      "step": 9445
    },
    {
      "epoch": 11.454545454545455,
      "grad_norm": 0.07235843688249588,
      "learning_rate": 2e-05,
      "loss": 0.032,
      "step": 9450
    },
    {
      "epoch": 11.460606060606061,
      "grad_norm": 0.04633094370365143,
      "learning_rate": 2e-05,
      "loss": 0.0329,
      "step": 9455
    },
    {
      "epoch": 11.466666666666667,
      "grad_norm": 0.08183887600898743,
      "learning_rate": 2e-05,
      "loss": 0.0361,
      "step": 9460
    },
    {
      "epoch": 11.472727272727273,
      "grad_norm": 0.10501110553741455,
      "learning_rate": 2e-05,
      "loss": 0.0347,
      "step": 9465
    },
    {
      "epoch": 11.478787878787879,
      "grad_norm": 0.07911278307437897,
      "learning_rate": 2e-05,
      "loss": 0.0349,
      "step": 9470
    },
    {
      "epoch": 11.484848484848484,
      "grad_norm": 0.10180061310529709,
      "learning_rate": 2e-05,
      "loss": 0.0346,
      "step": 9475
    },
    {
      "epoch": 11.49090909090909,
      "grad_norm": 0.05232217162847519,
      "learning_rate": 2e-05,
      "loss": 0.0344,
      "step": 9480
    },
    {
      "epoch": 11.496969696969696,
      "grad_norm": 0.07313653081655502,
      "learning_rate": 2e-05,
      "loss": 0.0331,
      "step": 9485
    },
    {
      "epoch": 11.50060606060606,
      "eval_average": 0.5582378101121367,
      "eval_crossner_ai": 0.5069124423468602,
      "eval_crossner_literature": 0.5628604382428397,
      "eval_crossner_music": 0.7136929460080003,
      "eval_crossner_politics": 0.552274541051312,
      "eval_crossner_science": 0.6322580644660452,
      "eval_mit-movie": 0.5420054200044654,
      "eval_mit-restaurant": 0.39766081866543385,
      "eval_runtime": 36.3388,
      "eval_samples_per_second": 19.263,
      "eval_steps_per_second": 0.193,
      "step": 9488
    }
  ],
  "logging_steps": 5,
  "max_steps": 9900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 12,
  "save_steps": 9223372036854775807,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.443163535194194e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
