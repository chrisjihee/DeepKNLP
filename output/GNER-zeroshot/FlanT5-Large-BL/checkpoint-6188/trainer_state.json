{
  "best_metric": 0.64513710636598,
  "best_model_checkpoint": "output/GNER-zeroshot/FlanT5-Large-BL/checkpoint-6188",
  "epoch": 7.500606060606061,
  "eval_steps": 9223372036854775807,
  "global_step": 6188,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006060606060606061,
      "grad_norm": 4.046901226043701,
      "learning_rate": 1.3876635607780456e-05,
      "loss": 0.9017,
      "step": 5
    },
    {
      "epoch": 0.012121212121212121,
      "grad_norm": 1.97663414478302,
      "learning_rate": 1.985297726897803e-05,
      "loss": 0.7006,
      "step": 10
    },
    {
      "epoch": 0.01818181818181818,
      "grad_norm": 0.9701758027076721,
      "learning_rate": 2.3348913032276192e-05,
      "loss": 0.4447,
      "step": 15
    },
    {
      "epoch": 0.024242424242424242,
      "grad_norm": 0.5324599742889404,
      "learning_rate": 2.5829318930175607e-05,
      "loss": 0.3324,
      "step": 20
    },
    {
      "epoch": 0.030303030303030304,
      "grad_norm": 0.4710332155227661,
      "learning_rate": 2.775327121556091e-05,
      "loss": 0.2509,
      "step": 25
    },
    {
      "epoch": 0.03636363636363636,
      "grad_norm": 0.3397379219532013,
      "learning_rate": 2.9325254693473768e-05,
      "loss": 0.2046,
      "step": 30
    },
    {
      "epoch": 0.04242424242424243,
      "grad_norm": 0.24426321685314178,
      "learning_rate": 3.065434778624138e-05,
      "loss": 0.1692,
      "step": 35
    },
    {
      "epoch": 0.048484848484848485,
      "grad_norm": 0.1887962967157364,
      "learning_rate": 3.180566059137318e-05,
      "loss": 0.1499,
      "step": 40
    },
    {
      "epoch": 0.05454545454545454,
      "grad_norm": 0.18593016266822815,
      "learning_rate": 3.2821190456771925e-05,
      "loss": 0.1326,
      "step": 45
    },
    {
      "epoch": 0.06060606060606061,
      "grad_norm": 0.18066665530204773,
      "learning_rate": 3.372961287675849e-05,
      "loss": 0.1277,
      "step": 50
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.13913339376449585,
      "learning_rate": 3.45513809143067e-05,
      "loss": 0.1151,
      "step": 55
    },
    {
      "epoch": 0.07272727272727272,
      "grad_norm": 0.1646784543991089,
      "learning_rate": 3.530159635467133e-05,
      "loss": 0.1072,
      "step": 60
    },
    {
      "epoch": 0.07878787878787878,
      "grad_norm": 0.17179140448570251,
      "learning_rate": 3.599172766005727e-05,
      "loss": 0.1079,
      "step": 65
    },
    {
      "epoch": 0.08484848484848485,
      "grad_norm": 0.12976709008216858,
      "learning_rate": 3.663068944743896e-05,
      "loss": 0.1041,
      "step": 70
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.14459846913814545,
      "learning_rate": 3.722554864005665e-05,
      "loss": 0.1021,
      "step": 75
    },
    {
      "epoch": 0.09696969696969697,
      "grad_norm": 0.12800127267837524,
      "learning_rate": 3.778200225257075e-05,
      "loss": 0.0925,
      "step": 80
    },
    {
      "epoch": 0.10303030303030303,
      "grad_norm": 0.12398647516965866,
      "learning_rate": 3.830471007454187e-05,
      "loss": 0.0927,
      "step": 85
    },
    {
      "epoch": 0.10909090909090909,
      "grad_norm": 0.12775097787380219,
      "learning_rate": 3.87975321179695e-05,
      "loss": 0.0887,
      "step": 90
    },
    {
      "epoch": 0.11515151515151516,
      "grad_norm": 0.12274578958749771,
      "learning_rate": 3.926370178012077e-05,
      "loss": 0.0833,
      "step": 95
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 0.12219884991645813,
      "learning_rate": 3.970595453795606e-05,
      "loss": 0.0869,
      "step": 100
    },
    {
      "epoch": 0.12727272727272726,
      "grad_norm": 0.11911065131425858,
      "learning_rate": 4.012662521073712e-05,
      "loss": 0.0793,
      "step": 105
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.1518755406141281,
      "learning_rate": 4.0527722575504266e-05,
      "loss": 0.0809,
      "step": 110
    },
    {
      "epoch": 0.1393939393939394,
      "grad_norm": 0.1116311103105545,
      "learning_rate": 4.091098738277237e-05,
      "loss": 0.0751,
      "step": 115
    },
    {
      "epoch": 0.14545454545454545,
      "grad_norm": 0.13979539275169373,
      "learning_rate": 4.127793801586891e-05,
      "loss": 0.0742,
      "step": 120
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 0.10625769197940826,
      "learning_rate": 4.162990682334138e-05,
      "loss": 0.0744,
      "step": 125
    },
    {
      "epoch": 0.15757575757575756,
      "grad_norm": 0.11492222547531128,
      "learning_rate": 4.196806932125484e-05,
      "loss": 0.0706,
      "step": 130
    },
    {
      "epoch": 0.16363636363636364,
      "grad_norm": 0.10018116980791092,
      "learning_rate": 4.229346788126766e-05,
      "loss": 0.0703,
      "step": 135
    },
    {
      "epoch": 0.1696969696969697,
      "grad_norm": 0.09279649704694748,
      "learning_rate": 4.260703110863653e-05,
      "loss": 0.0713,
      "step": 140
    },
    {
      "epoch": 0.17575757575757575,
      "grad_norm": 0.09390941262245178,
      "learning_rate": 4.290958981826741e-05,
      "loss": 0.0703,
      "step": 145
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.11048471927642822,
      "learning_rate": 4.320189030125422e-05,
      "loss": 0.0691,
      "step": 150
    },
    {
      "epoch": 0.18787878787878787,
      "grad_norm": 0.099603570997715,
      "learning_rate": 4.3484605415296846e-05,
      "loss": 0.0682,
      "step": 155
    },
    {
      "epoch": 0.19393939393939394,
      "grad_norm": 0.09020955115556717,
      "learning_rate": 4.3758343913768324e-05,
      "loss": 0.0671,
      "step": 160
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.09008670598268509,
      "learning_rate": 4.402365833880243e-05,
      "loss": 0.0661,
      "step": 165
    },
    {
      "epoch": 0.20606060606060606,
      "grad_norm": 0.08527328073978424,
      "learning_rate": 4.428105173573944e-05,
      "loss": 0.0649,
      "step": 170
    },
    {
      "epoch": 0.21212121212121213,
      "grad_norm": 0.08131042867898941,
      "learning_rate": 4.453098339402185e-05,
      "loss": 0.0625,
      "step": 175
    },
    {
      "epoch": 0.21818181818181817,
      "grad_norm": 0.10500146448612213,
      "learning_rate": 4.4773873779167076e-05,
      "loss": 0.0649,
      "step": 180
    },
    {
      "epoch": 0.22424242424242424,
      "grad_norm": 0.08857536315917969,
      "learning_rate": 4.501010878885466e-05,
      "loss": 0.0617,
      "step": 185
    },
    {
      "epoch": 0.23030303030303031,
      "grad_norm": 0.09479103982448578,
      "learning_rate": 4.5240043441318346e-05,
      "loss": 0.0624,
      "step": 190
    },
    {
      "epoch": 0.23636363636363636,
      "grad_norm": 0.08805734664201736,
      "learning_rate": 4.546400508455301e-05,
      "loss": 0.059,
      "step": 195
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 0.09729664027690887,
      "learning_rate": 4.568229619915363e-05,
      "loss": 0.059,
      "step": 200
    },
    {
      "epoch": 0.24848484848484848,
      "grad_norm": 0.08607779443264008,
      "learning_rate": 4.589519685501209e-05,
      "loss": 0.061,
      "step": 205
    },
    {
      "epoch": 0.2545454545454545,
      "grad_norm": 0.07713980972766876,
      "learning_rate": 4.610296687193469e-05,
      "loss": 0.0606,
      "step": 210
    },
    {
      "epoch": 0.2606060606060606,
      "grad_norm": 0.12076300382614136,
      "learning_rate": 4.630584772599463e-05,
      "loss": 0.0599,
      "step": 215
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.07972604036331177,
      "learning_rate": 4.650406423670184e-05,
      "loss": 0.0562,
      "step": 220
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.1340852677822113,
      "learning_rate": 4.6697826064552384e-05,
      "loss": 0.0616,
      "step": 225
    },
    {
      "epoch": 0.2787878787878788,
      "grad_norm": 0.08125965297222137,
      "learning_rate": 4.688732904396995e-05,
      "loss": 0.0587,
      "step": 230
    },
    {
      "epoch": 0.28484848484848485,
      "grad_norm": 0.10544276982545853,
      "learning_rate": 4.7072756372885114e-05,
      "loss": 0.0593,
      "step": 235
    },
    {
      "epoch": 0.2909090909090909,
      "grad_norm": 0.08077772706747055,
      "learning_rate": 4.7254279677066484e-05,
      "loss": 0.0552,
      "step": 240
    },
    {
      "epoch": 0.296969696969697,
      "grad_norm": 0.07437910884618759,
      "learning_rate": 4.7432059964702314e-05,
      "loss": 0.0547,
      "step": 245
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 0.08270061761140823,
      "learning_rate": 4.760624848453894e-05,
      "loss": 0.0557,
      "step": 250
    },
    {
      "epoch": 0.3090909090909091,
      "grad_norm": 0.07739997655153275,
      "learning_rate": 4.77769874990376e-05,
      "loss": 0.0552,
      "step": 255
    },
    {
      "epoch": 0.3151515151515151,
      "grad_norm": 0.0752233937382698,
      "learning_rate": 4.794441098245242e-05,
      "loss": 0.0557,
      "step": 260
    },
    {
      "epoch": 0.3212121212121212,
      "grad_norm": 0.08248833566904068,
      "learning_rate": 4.8108645252412246e-05,
      "loss": 0.0539,
      "step": 265
    },
    {
      "epoch": 0.32727272727272727,
      "grad_norm": 0.10833122581243515,
      "learning_rate": 4.826980954246524e-05,
      "loss": 0.0569,
      "step": 270
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.0912037119269371,
      "learning_rate": 4.8428016522087156e-05,
      "loss": 0.0534,
      "step": 275
    },
    {
      "epoch": 0.3393939393939394,
      "grad_norm": 0.08106330782175064,
      "learning_rate": 4.85833727698341e-05,
      "loss": 0.0521,
      "step": 280
    },
    {
      "epoch": 0.34545454545454546,
      "grad_norm": 0.07765581458806992,
      "learning_rate": 4.8735979204616514e-05,
      "loss": 0.0531,
      "step": 285
    },
    {
      "epoch": 0.3515151515151515,
      "grad_norm": 0.06302791833877563,
      "learning_rate": 4.888593147946499e-05,
      "loss": 0.0508,
      "step": 290
    },
    {
      "epoch": 0.3575757575757576,
      "grad_norm": 0.08348563313484192,
      "learning_rate": 4.903332034163597e-05,
      "loss": 0.0539,
      "step": 295
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.08858634531497955,
      "learning_rate": 4.917823196245179e-05,
      "loss": 0.0542,
      "step": 300
    },
    {
      "epoch": 0.3696969696969697,
      "grad_norm": 0.07703841477632523,
      "learning_rate": 4.932074823987751e-05,
      "loss": 0.0529,
      "step": 305
    },
    {
      "epoch": 0.37575757575757573,
      "grad_norm": 0.08854634314775467,
      "learning_rate": 4.946094707649442e-05,
      "loss": 0.0551,
      "step": 310
    },
    {
      "epoch": 0.38181818181818183,
      "grad_norm": 0.07711368054151535,
      "learning_rate": 4.959890263523285e-05,
      "loss": 0.049,
      "step": 315
    },
    {
      "epoch": 0.3878787878787879,
      "grad_norm": 0.06365226209163666,
      "learning_rate": 4.97346855749659e-05,
      "loss": 0.0506,
      "step": 320
    },
    {
      "epoch": 0.3939393939393939,
      "grad_norm": 0.08175402134656906,
      "learning_rate": 4.986836326783773e-05,
      "loss": 0.0511,
      "step": 325
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.08622811734676361,
      "learning_rate": 5e-05,
      "loss": 0.0504,
      "step": 330
    },
    {
      "epoch": 0.40606060606060607,
      "grad_norm": 0.08636809885501862,
      "learning_rate": 5e-05,
      "loss": 0.0508,
      "step": 335
    },
    {
      "epoch": 0.4121212121212121,
      "grad_norm": 0.08705171197652817,
      "learning_rate": 5e-05,
      "loss": 0.0493,
      "step": 340
    },
    {
      "epoch": 0.41818181818181815,
      "grad_norm": 0.07095494121313095,
      "learning_rate": 5e-05,
      "loss": 0.0497,
      "step": 345
    },
    {
      "epoch": 0.42424242424242425,
      "grad_norm": 0.09758516401052475,
      "learning_rate": 5e-05,
      "loss": 0.0504,
      "step": 350
    },
    {
      "epoch": 0.4303030303030303,
      "grad_norm": 0.09316767007112503,
      "learning_rate": 5e-05,
      "loss": 0.0496,
      "step": 355
    },
    {
      "epoch": 0.43636363636363634,
      "grad_norm": 0.07314623892307281,
      "learning_rate": 5e-05,
      "loss": 0.0517,
      "step": 360
    },
    {
      "epoch": 0.44242424242424244,
      "grad_norm": 0.07510468363761902,
      "learning_rate": 5e-05,
      "loss": 0.0505,
      "step": 365
    },
    {
      "epoch": 0.4484848484848485,
      "grad_norm": 0.0752117857336998,
      "learning_rate": 5e-05,
      "loss": 0.0492,
      "step": 370
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.06690796464681625,
      "learning_rate": 5e-05,
      "loss": 0.0454,
      "step": 375
    },
    {
      "epoch": 0.46060606060606063,
      "grad_norm": 0.08725765347480774,
      "learning_rate": 5e-05,
      "loss": 0.0498,
      "step": 380
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.07218991219997406,
      "learning_rate": 5e-05,
      "loss": 0.0485,
      "step": 385
    },
    {
      "epoch": 0.4727272727272727,
      "grad_norm": 0.06260170787572861,
      "learning_rate": 5e-05,
      "loss": 0.0485,
      "step": 390
    },
    {
      "epoch": 0.47878787878787876,
      "grad_norm": 0.05896778404712677,
      "learning_rate": 5e-05,
      "loss": 0.0485,
      "step": 395
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 0.07341188192367554,
      "learning_rate": 5e-05,
      "loss": 0.0477,
      "step": 400
    },
    {
      "epoch": 0.4909090909090909,
      "grad_norm": 0.07457422465085983,
      "learning_rate": 5e-05,
      "loss": 0.0463,
      "step": 405
    },
    {
      "epoch": 0.49696969696969695,
      "grad_norm": 0.06451789289712906,
      "learning_rate": 5e-05,
      "loss": 0.0462,
      "step": 410
    },
    {
      "epoch": 0.4993939393939394,
      "eval_average": 0.539245548775615,
      "eval_crossner_ai": 0.5292119564720356,
      "eval_crossner_literature": 0.5481911803038132,
      "eval_crossner_music": 0.6747178775979639,
      "eval_crossner_politics": 0.5681213827123838,
      "eval_crossner_science": 0.610161970218592,
      "eval_mit-movie": 0.48071866778666344,
      "eval_mit-restaurant": 0.363595806337854,
      "eval_runtime": 616.4027,
      "eval_samples_per_second": 9.872,
      "eval_steps_per_second": 0.156,
      "step": 412
    },
    {
      "epoch": 0.503030303030303,
      "grad_norm": 0.10177741199731827,
      "learning_rate": 5e-05,
      "loss": 0.0476,
      "step": 415
    },
    {
      "epoch": 0.509090909090909,
      "grad_norm": 0.06637368351221085,
      "learning_rate": 5e-05,
      "loss": 0.0455,
      "step": 420
    },
    {
      "epoch": 0.5151515151515151,
      "grad_norm": 0.08064117282629013,
      "learning_rate": 5e-05,
      "loss": 0.0461,
      "step": 425
    },
    {
      "epoch": 0.5212121212121212,
      "grad_norm": 0.05947975441813469,
      "learning_rate": 5e-05,
      "loss": 0.0474,
      "step": 430
    },
    {
      "epoch": 0.5272727272727272,
      "grad_norm": 0.06643734872341156,
      "learning_rate": 5e-05,
      "loss": 0.047,
      "step": 435
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.0614270381629467,
      "learning_rate": 5e-05,
      "loss": 0.0477,
      "step": 440
    },
    {
      "epoch": 0.5393939393939394,
      "grad_norm": 0.08672108501195908,
      "learning_rate": 5e-05,
      "loss": 0.046,
      "step": 445
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.06954322755336761,
      "learning_rate": 5e-05,
      "loss": 0.0429,
      "step": 450
    },
    {
      "epoch": 0.5515151515151515,
      "grad_norm": 0.0691697895526886,
      "learning_rate": 5e-05,
      "loss": 0.0463,
      "step": 455
    },
    {
      "epoch": 0.5575757575757576,
      "grad_norm": 0.06702960282564163,
      "learning_rate": 5e-05,
      "loss": 0.0436,
      "step": 460
    },
    {
      "epoch": 0.5636363636363636,
      "grad_norm": 0.06404989212751389,
      "learning_rate": 5e-05,
      "loss": 0.0429,
      "step": 465
    },
    {
      "epoch": 0.5696969696969697,
      "grad_norm": 0.062136679887771606,
      "learning_rate": 5e-05,
      "loss": 0.0455,
      "step": 470
    },
    {
      "epoch": 0.5757575757575758,
      "grad_norm": 0.05784112587571144,
      "learning_rate": 5e-05,
      "loss": 0.0457,
      "step": 475
    },
    {
      "epoch": 0.5818181818181818,
      "grad_norm": 0.0752132385969162,
      "learning_rate": 5e-05,
      "loss": 0.0459,
      "step": 480
    },
    {
      "epoch": 0.5878787878787879,
      "grad_norm": 0.06955651193857193,
      "learning_rate": 5e-05,
      "loss": 0.0437,
      "step": 485
    },
    {
      "epoch": 0.593939393939394,
      "grad_norm": 0.0742943212389946,
      "learning_rate": 5e-05,
      "loss": 0.0449,
      "step": 490
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.06008589267730713,
      "learning_rate": 5e-05,
      "loss": 0.0428,
      "step": 495
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 0.08646351844072342,
      "learning_rate": 5e-05,
      "loss": 0.0461,
      "step": 500
    },
    {
      "epoch": 0.6121212121212121,
      "grad_norm": 0.06591599434614182,
      "learning_rate": 5e-05,
      "loss": 0.0439,
      "step": 505
    },
    {
      "epoch": 0.6181818181818182,
      "grad_norm": 0.10617472976446152,
      "learning_rate": 5e-05,
      "loss": 0.0447,
      "step": 510
    },
    {
      "epoch": 0.6242424242424243,
      "grad_norm": 0.07022710889577866,
      "learning_rate": 5e-05,
      "loss": 0.0428,
      "step": 515
    },
    {
      "epoch": 0.6303030303030303,
      "grad_norm": 0.05212225019931793,
      "learning_rate": 5e-05,
      "loss": 0.0428,
      "step": 520
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 0.0714239552617073,
      "learning_rate": 5e-05,
      "loss": 0.0429,
      "step": 525
    },
    {
      "epoch": 0.6424242424242425,
      "grad_norm": 0.0646820068359375,
      "learning_rate": 5e-05,
      "loss": 0.0431,
      "step": 530
    },
    {
      "epoch": 0.6484848484848484,
      "grad_norm": 0.056608896702528,
      "learning_rate": 5e-05,
      "loss": 0.0429,
      "step": 535
    },
    {
      "epoch": 0.6545454545454545,
      "grad_norm": 0.06089337542653084,
      "learning_rate": 5e-05,
      "loss": 0.0408,
      "step": 540
    },
    {
      "epoch": 0.6606060606060606,
      "grad_norm": 0.091875359416008,
      "learning_rate": 5e-05,
      "loss": 0.0459,
      "step": 545
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.08239804208278656,
      "learning_rate": 5e-05,
      "loss": 0.0433,
      "step": 550
    },
    {
      "epoch": 0.6727272727272727,
      "grad_norm": 0.05339879170060158,
      "learning_rate": 5e-05,
      "loss": 0.0449,
      "step": 555
    },
    {
      "epoch": 0.6787878787878788,
      "grad_norm": 0.07528854161500931,
      "learning_rate": 5e-05,
      "loss": 0.042,
      "step": 560
    },
    {
      "epoch": 0.6848484848484848,
      "grad_norm": 0.059170451015233994,
      "learning_rate": 5e-05,
      "loss": 0.0446,
      "step": 565
    },
    {
      "epoch": 0.6909090909090909,
      "grad_norm": 0.05051819235086441,
      "learning_rate": 5e-05,
      "loss": 0.0414,
      "step": 570
    },
    {
      "epoch": 0.696969696969697,
      "grad_norm": 0.10162129998207092,
      "learning_rate": 5e-05,
      "loss": 0.0433,
      "step": 575
    },
    {
      "epoch": 0.703030303030303,
      "grad_norm": 0.06278280913829803,
      "learning_rate": 5e-05,
      "loss": 0.0432,
      "step": 580
    },
    {
      "epoch": 0.7090909090909091,
      "grad_norm": 0.06362797319889069,
      "learning_rate": 5e-05,
      "loss": 0.0432,
      "step": 585
    },
    {
      "epoch": 0.7151515151515152,
      "grad_norm": 0.06649009883403778,
      "learning_rate": 5e-05,
      "loss": 0.0428,
      "step": 590
    },
    {
      "epoch": 0.7212121212121212,
      "grad_norm": 0.0633002445101738,
      "learning_rate": 5e-05,
      "loss": 0.0417,
      "step": 595
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.0507904589176178,
      "learning_rate": 5e-05,
      "loss": 0.0428,
      "step": 600
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.08937716484069824,
      "learning_rate": 5e-05,
      "loss": 0.0414,
      "step": 605
    },
    {
      "epoch": 0.7393939393939394,
      "grad_norm": 0.07784295082092285,
      "learning_rate": 5e-05,
      "loss": 0.0444,
      "step": 610
    },
    {
      "epoch": 0.7454545454545455,
      "grad_norm": 0.05329716578125954,
      "learning_rate": 5e-05,
      "loss": 0.0416,
      "step": 615
    },
    {
      "epoch": 0.7515151515151515,
      "grad_norm": 0.0715050920844078,
      "learning_rate": 5e-05,
      "loss": 0.0398,
      "step": 620
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 0.08860881626605988,
      "learning_rate": 5e-05,
      "loss": 0.0412,
      "step": 625
    },
    {
      "epoch": 0.7636363636363637,
      "grad_norm": 0.06299905478954315,
      "learning_rate": 5e-05,
      "loss": 0.0421,
      "step": 630
    },
    {
      "epoch": 0.7696969696969697,
      "grad_norm": 0.06962543725967407,
      "learning_rate": 5e-05,
      "loss": 0.0407,
      "step": 635
    },
    {
      "epoch": 0.7757575757575758,
      "grad_norm": 0.05671074613928795,
      "learning_rate": 5e-05,
      "loss": 0.0402,
      "step": 640
    },
    {
      "epoch": 0.7818181818181819,
      "grad_norm": 0.05789550393819809,
      "learning_rate": 5e-05,
      "loss": 0.0435,
      "step": 645
    },
    {
      "epoch": 0.7878787878787878,
      "grad_norm": 0.0684400275349617,
      "learning_rate": 5e-05,
      "loss": 0.0419,
      "step": 650
    },
    {
      "epoch": 0.793939393939394,
      "grad_norm": 0.04573892429471016,
      "learning_rate": 5e-05,
      "loss": 0.041,
      "step": 655
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.05360494926571846,
      "learning_rate": 5e-05,
      "loss": 0.0424,
      "step": 660
    },
    {
      "epoch": 0.806060606060606,
      "grad_norm": 0.06444638222455978,
      "learning_rate": 5e-05,
      "loss": 0.0417,
      "step": 665
    },
    {
      "epoch": 0.8121212121212121,
      "grad_norm": 0.05968843027949333,
      "learning_rate": 5e-05,
      "loss": 0.0422,
      "step": 670
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 0.12967434525489807,
      "learning_rate": 5e-05,
      "loss": 0.0421,
      "step": 675
    },
    {
      "epoch": 0.8242424242424242,
      "grad_norm": 0.05992211028933525,
      "learning_rate": 5e-05,
      "loss": 0.0406,
      "step": 680
    },
    {
      "epoch": 0.8303030303030303,
      "grad_norm": 0.060657888650894165,
      "learning_rate": 5e-05,
      "loss": 0.0409,
      "step": 685
    },
    {
      "epoch": 0.8363636363636363,
      "grad_norm": 0.05210661515593529,
      "learning_rate": 5e-05,
      "loss": 0.0412,
      "step": 690
    },
    {
      "epoch": 0.8424242424242424,
      "grad_norm": 0.07650677859783173,
      "learning_rate": 5e-05,
      "loss": 0.0409,
      "step": 695
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 0.05840892344713211,
      "learning_rate": 5e-05,
      "loss": 0.0423,
      "step": 700
    },
    {
      "epoch": 0.8545454545454545,
      "grad_norm": 0.08993217349052429,
      "learning_rate": 5e-05,
      "loss": 0.0424,
      "step": 705
    },
    {
      "epoch": 0.8606060606060606,
      "grad_norm": 0.08560488373041153,
      "learning_rate": 5e-05,
      "loss": 0.0407,
      "step": 710
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.07248505204916,
      "learning_rate": 5e-05,
      "loss": 0.04,
      "step": 715
    },
    {
      "epoch": 0.8727272727272727,
      "grad_norm": 0.05626336857676506,
      "learning_rate": 5e-05,
      "loss": 0.0406,
      "step": 720
    },
    {
      "epoch": 0.8787878787878788,
      "grad_norm": 0.06836812943220139,
      "learning_rate": 5e-05,
      "loss": 0.0399,
      "step": 725
    },
    {
      "epoch": 0.8848484848484849,
      "grad_norm": 0.06419489532709122,
      "learning_rate": 5e-05,
      "loss": 0.0409,
      "step": 730
    },
    {
      "epoch": 0.8909090909090909,
      "grad_norm": 0.06056707724928856,
      "learning_rate": 5e-05,
      "loss": 0.0389,
      "step": 735
    },
    {
      "epoch": 0.896969696969697,
      "grad_norm": 0.08151907473802567,
      "learning_rate": 5e-05,
      "loss": 0.04,
      "step": 740
    },
    {
      "epoch": 0.9030303030303031,
      "grad_norm": 0.053881678730249405,
      "learning_rate": 5e-05,
      "loss": 0.0405,
      "step": 745
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.06354305148124695,
      "learning_rate": 5e-05,
      "loss": 0.0382,
      "step": 750
    },
    {
      "epoch": 0.9151515151515152,
      "grad_norm": 0.06347355991601944,
      "learning_rate": 5e-05,
      "loss": 0.0395,
      "step": 755
    },
    {
      "epoch": 0.9212121212121213,
      "grad_norm": 0.06410380452871323,
      "learning_rate": 5e-05,
      "loss": 0.041,
      "step": 760
    },
    {
      "epoch": 0.9272727272727272,
      "grad_norm": 0.05657907575368881,
      "learning_rate": 5e-05,
      "loss": 0.039,
      "step": 765
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.061304524540901184,
      "learning_rate": 5e-05,
      "loss": 0.0395,
      "step": 770
    },
    {
      "epoch": 0.9393939393939394,
      "grad_norm": 0.0674808993935585,
      "learning_rate": 5e-05,
      "loss": 0.0367,
      "step": 775
    },
    {
      "epoch": 0.9454545454545454,
      "grad_norm": 0.08632343262434006,
      "learning_rate": 5e-05,
      "loss": 0.0391,
      "step": 780
    },
    {
      "epoch": 0.9515151515151515,
      "grad_norm": 0.05572963133454323,
      "learning_rate": 5e-05,
      "loss": 0.0395,
      "step": 785
    },
    {
      "epoch": 0.9575757575757575,
      "grad_norm": 0.06920275092124939,
      "learning_rate": 5e-05,
      "loss": 0.0409,
      "step": 790
    },
    {
      "epoch": 0.9636363636363636,
      "grad_norm": 0.052370596677064896,
      "learning_rate": 5e-05,
      "loss": 0.0387,
      "step": 795
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 0.06825511157512665,
      "learning_rate": 5e-05,
      "loss": 0.0402,
      "step": 800
    },
    {
      "epoch": 0.9757575757575757,
      "grad_norm": 0.060106631368398666,
      "learning_rate": 5e-05,
      "loss": 0.0387,
      "step": 805
    },
    {
      "epoch": 0.9818181818181818,
      "grad_norm": 0.051948413252830505,
      "learning_rate": 5e-05,
      "loss": 0.0381,
      "step": 810
    },
    {
      "epoch": 0.9878787878787879,
      "grad_norm": 0.049039583653211594,
      "learning_rate": 5e-05,
      "loss": 0.0374,
      "step": 815
    },
    {
      "epoch": 0.9939393939393939,
      "grad_norm": 0.05785385146737099,
      "learning_rate": 5e-05,
      "loss": 0.0409,
      "step": 820
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.07080261409282684,
      "learning_rate": 5e-05,
      "loss": 0.0407,
      "step": 825
    },
    {
      "epoch": 1.0,
      "eval_average": 0.5892908187185253,
      "eval_crossner_ai": 0.5930069929571004,
      "eval_crossner_literature": 0.5761517614676207,
      "eval_crossner_music": 0.7345238094737806,
      "eval_crossner_politics": 0.5781003470626536,
      "eval_crossner_science": 0.6896239238285963,
      "eval_mit-movie": 0.5375040588320326,
      "eval_mit-restaurant": 0.4161248374078935,
      "eval_runtime": 601.0932,
      "eval_samples_per_second": 10.123,
      "eval_steps_per_second": 0.16,
      "step": 825
    },
    {
      "epoch": 1.006060606060606,
      "grad_norm": 0.05571639910340309,
      "learning_rate": 5e-05,
      "loss": 0.0411,
      "step": 830
    },
    {
      "epoch": 1.0121212121212122,
      "grad_norm": 0.050410810858011246,
      "learning_rate": 5e-05,
      "loss": 0.0376,
      "step": 835
    },
    {
      "epoch": 1.018181818181818,
      "grad_norm": 0.05336032062768936,
      "learning_rate": 5e-05,
      "loss": 0.0373,
      "step": 840
    },
    {
      "epoch": 1.0242424242424242,
      "grad_norm": 0.05889789015054703,
      "learning_rate": 5e-05,
      "loss": 0.0383,
      "step": 845
    },
    {
      "epoch": 1.0303030303030303,
      "grad_norm": 0.06480494141578674,
      "learning_rate": 5e-05,
      "loss": 0.0402,
      "step": 850
    },
    {
      "epoch": 1.0363636363636364,
      "grad_norm": 0.07225315272808075,
      "learning_rate": 5e-05,
      "loss": 0.0375,
      "step": 855
    },
    {
      "epoch": 1.0424242424242425,
      "grad_norm": 0.05820128321647644,
      "learning_rate": 5e-05,
      "loss": 0.0375,
      "step": 860
    },
    {
      "epoch": 1.0484848484848486,
      "grad_norm": 0.05618251487612724,
      "learning_rate": 5e-05,
      "loss": 0.0365,
      "step": 865
    },
    {
      "epoch": 1.0545454545454545,
      "grad_norm": 0.05271080508828163,
      "learning_rate": 5e-05,
      "loss": 0.0397,
      "step": 870
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 0.04709843918681145,
      "learning_rate": 5e-05,
      "loss": 0.0381,
      "step": 875
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.06278301030397415,
      "learning_rate": 5e-05,
      "loss": 0.0381,
      "step": 880
    },
    {
      "epoch": 1.0727272727272728,
      "grad_norm": 0.048907093703746796,
      "learning_rate": 5e-05,
      "loss": 0.0384,
      "step": 885
    },
    {
      "epoch": 1.0787878787878789,
      "grad_norm": 0.05512024462223053,
      "learning_rate": 5e-05,
      "loss": 0.0368,
      "step": 890
    },
    {
      "epoch": 1.084848484848485,
      "grad_norm": 0.0567765049636364,
      "learning_rate": 5e-05,
      "loss": 0.0365,
      "step": 895
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 0.058819692581892014,
      "learning_rate": 5e-05,
      "loss": 0.039,
      "step": 900
    },
    {
      "epoch": 1.096969696969697,
      "grad_norm": 0.05236516147851944,
      "learning_rate": 5e-05,
      "loss": 0.0367,
      "step": 905
    },
    {
      "epoch": 1.103030303030303,
      "grad_norm": 0.05858192592859268,
      "learning_rate": 5e-05,
      "loss": 0.0387,
      "step": 910
    },
    {
      "epoch": 1.1090909090909091,
      "grad_norm": 0.05132931098341942,
      "learning_rate": 5e-05,
      "loss": 0.0366,
      "step": 915
    },
    {
      "epoch": 1.1151515151515152,
      "grad_norm": 0.05442747846245766,
      "learning_rate": 5e-05,
      "loss": 0.0373,
      "step": 920
    },
    {
      "epoch": 1.121212121212121,
      "grad_norm": 0.05363503471016884,
      "learning_rate": 5e-05,
      "loss": 0.0397,
      "step": 925
    },
    {
      "epoch": 1.1272727272727272,
      "grad_norm": 0.0668424591422081,
      "learning_rate": 5e-05,
      "loss": 0.0384,
      "step": 930
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.06481209397315979,
      "learning_rate": 5e-05,
      "loss": 0.0388,
      "step": 935
    },
    {
      "epoch": 1.1393939393939394,
      "grad_norm": 0.04904526099562645,
      "learning_rate": 5e-05,
      "loss": 0.0365,
      "step": 940
    },
    {
      "epoch": 1.1454545454545455,
      "grad_norm": 0.06261900067329407,
      "learning_rate": 5e-05,
      "loss": 0.0382,
      "step": 945
    },
    {
      "epoch": 1.1515151515151516,
      "grad_norm": 0.04443095624446869,
      "learning_rate": 5e-05,
      "loss": 0.0359,
      "step": 950
    },
    {
      "epoch": 1.1575757575757575,
      "grad_norm": 0.05857642740011215,
      "learning_rate": 5e-05,
      "loss": 0.036,
      "step": 955
    },
    {
      "epoch": 1.1636363636363636,
      "grad_norm": 0.06877943128347397,
      "learning_rate": 5e-05,
      "loss": 0.0375,
      "step": 960
    },
    {
      "epoch": 1.1696969696969697,
      "grad_norm": 0.08464127033948898,
      "learning_rate": 5e-05,
      "loss": 0.0356,
      "step": 965
    },
    {
      "epoch": 1.1757575757575758,
      "grad_norm": 0.05602359026670456,
      "learning_rate": 5e-05,
      "loss": 0.0375,
      "step": 970
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 0.06075448915362358,
      "learning_rate": 5e-05,
      "loss": 0.0363,
      "step": 975
    },
    {
      "epoch": 1.187878787878788,
      "grad_norm": 0.04033958911895752,
      "learning_rate": 5e-05,
      "loss": 0.0363,
      "step": 980
    },
    {
      "epoch": 1.1939393939393939,
      "grad_norm": 0.04911636933684349,
      "learning_rate": 5e-05,
      "loss": 0.0378,
      "step": 985
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.04847073554992676,
      "learning_rate": 5e-05,
      "loss": 0.0372,
      "step": 990
    },
    {
      "epoch": 1.206060606060606,
      "grad_norm": 0.06285303831100464,
      "learning_rate": 5e-05,
      "loss": 0.0351,
      "step": 995
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 0.04938977584242821,
      "learning_rate": 5e-05,
      "loss": 0.0363,
      "step": 1000
    },
    {
      "epoch": 1.2181818181818183,
      "grad_norm": 0.06372888386249542,
      "learning_rate": 5e-05,
      "loss": 0.0361,
      "step": 1005
    },
    {
      "epoch": 1.2242424242424241,
      "grad_norm": 0.054489001631736755,
      "learning_rate": 5e-05,
      "loss": 0.0371,
      "step": 1010
    },
    {
      "epoch": 1.2303030303030302,
      "grad_norm": 0.0555727556347847,
      "learning_rate": 5e-05,
      "loss": 0.0368,
      "step": 1015
    },
    {
      "epoch": 1.2363636363636363,
      "grad_norm": 0.056656282395124435,
      "learning_rate": 5e-05,
      "loss": 0.0348,
      "step": 1020
    },
    {
      "epoch": 1.2424242424242424,
      "grad_norm": 0.060463834553956985,
      "learning_rate": 5e-05,
      "loss": 0.0378,
      "step": 1025
    },
    {
      "epoch": 1.2484848484848485,
      "grad_norm": 0.07909663766622543,
      "learning_rate": 5e-05,
      "loss": 0.0369,
      "step": 1030
    },
    {
      "epoch": 1.2545454545454546,
      "grad_norm": 0.056844886392354965,
      "learning_rate": 5e-05,
      "loss": 0.0379,
      "step": 1035
    },
    {
      "epoch": 1.2606060606060605,
      "grad_norm": 0.07618656754493713,
      "learning_rate": 5e-05,
      "loss": 0.0376,
      "step": 1040
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.08222167938947678,
      "learning_rate": 5e-05,
      "loss": 0.0352,
      "step": 1045
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 0.04645279049873352,
      "learning_rate": 5e-05,
      "loss": 0.0354,
      "step": 1050
    },
    {
      "epoch": 1.2787878787878788,
      "grad_norm": 0.07957610487937927,
      "learning_rate": 5e-05,
      "loss": 0.0359,
      "step": 1055
    },
    {
      "epoch": 1.284848484848485,
      "grad_norm": 0.07708561420440674,
      "learning_rate": 5e-05,
      "loss": 0.0374,
      "step": 1060
    },
    {
      "epoch": 1.290909090909091,
      "grad_norm": 0.07003803551197052,
      "learning_rate": 5e-05,
      "loss": 0.0356,
      "step": 1065
    },
    {
      "epoch": 1.2969696969696969,
      "grad_norm": 0.057690974324941635,
      "learning_rate": 5e-05,
      "loss": 0.0372,
      "step": 1070
    },
    {
      "epoch": 1.303030303030303,
      "grad_norm": 0.03984489291906357,
      "learning_rate": 5e-05,
      "loss": 0.0358,
      "step": 1075
    },
    {
      "epoch": 1.309090909090909,
      "grad_norm": 0.042269740253686905,
      "learning_rate": 5e-05,
      "loss": 0.0348,
      "step": 1080
    },
    {
      "epoch": 1.3151515151515152,
      "grad_norm": 0.04831789806485176,
      "learning_rate": 5e-05,
      "loss": 0.0355,
      "step": 1085
    },
    {
      "epoch": 1.3212121212121213,
      "grad_norm": 0.0655519962310791,
      "learning_rate": 5e-05,
      "loss": 0.0367,
      "step": 1090
    },
    {
      "epoch": 1.3272727272727274,
      "grad_norm": 0.055255427956581116,
      "learning_rate": 5e-05,
      "loss": 0.0365,
      "step": 1095
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.05880599468946457,
      "learning_rate": 5e-05,
      "loss": 0.0382,
      "step": 1100
    },
    {
      "epoch": 1.3393939393939394,
      "grad_norm": 0.0549844391644001,
      "learning_rate": 5e-05,
      "loss": 0.0354,
      "step": 1105
    },
    {
      "epoch": 1.3454545454545455,
      "grad_norm": 0.06160914525389671,
      "learning_rate": 5e-05,
      "loss": 0.0357,
      "step": 1110
    },
    {
      "epoch": 1.3515151515151516,
      "grad_norm": 0.06471824645996094,
      "learning_rate": 5e-05,
      "loss": 0.0357,
      "step": 1115
    },
    {
      "epoch": 1.3575757575757577,
      "grad_norm": 0.06760215014219284,
      "learning_rate": 5e-05,
      "loss": 0.0378,
      "step": 1120
    },
    {
      "epoch": 1.3636363636363635,
      "grad_norm": 0.05764429271221161,
      "learning_rate": 5e-05,
      "loss": 0.0347,
      "step": 1125
    },
    {
      "epoch": 1.3696969696969696,
      "grad_norm": 0.051816005259752274,
      "learning_rate": 5e-05,
      "loss": 0.0361,
      "step": 1130
    },
    {
      "epoch": 1.3757575757575757,
      "grad_norm": 0.05801701918244362,
      "learning_rate": 5e-05,
      "loss": 0.0373,
      "step": 1135
    },
    {
      "epoch": 1.3818181818181818,
      "grad_norm": 0.06368235498666763,
      "learning_rate": 5e-05,
      "loss": 0.0353,
      "step": 1140
    },
    {
      "epoch": 1.387878787878788,
      "grad_norm": 0.05148416385054588,
      "learning_rate": 5e-05,
      "loss": 0.0363,
      "step": 1145
    },
    {
      "epoch": 1.393939393939394,
      "grad_norm": 0.0749480128288269,
      "learning_rate": 5e-05,
      "loss": 0.0372,
      "step": 1150
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.056558482348918915,
      "learning_rate": 5e-05,
      "loss": 0.0353,
      "step": 1155
    },
    {
      "epoch": 1.406060606060606,
      "grad_norm": 0.060892414301633835,
      "learning_rate": 5e-05,
      "loss": 0.0366,
      "step": 1160
    },
    {
      "epoch": 1.412121212121212,
      "grad_norm": 0.054665058851242065,
      "learning_rate": 5e-05,
      "loss": 0.0372,
      "step": 1165
    },
    {
      "epoch": 1.4181818181818182,
      "grad_norm": 0.04738722741603851,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 1170
    },
    {
      "epoch": 1.4242424242424243,
      "grad_norm": 0.06556525081396103,
      "learning_rate": 5e-05,
      "loss": 0.0365,
      "step": 1175
    },
    {
      "epoch": 1.4303030303030304,
      "grad_norm": 0.053856220096349716,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 1180
    },
    {
      "epoch": 1.4363636363636363,
      "grad_norm": 0.08281533420085907,
      "learning_rate": 5e-05,
      "loss": 0.0362,
      "step": 1185
    },
    {
      "epoch": 1.4424242424242424,
      "grad_norm": 0.0457710437476635,
      "learning_rate": 5e-05,
      "loss": 0.0342,
      "step": 1190
    },
    {
      "epoch": 1.4484848484848485,
      "grad_norm": 0.053298220038414,
      "learning_rate": 5e-05,
      "loss": 0.0358,
      "step": 1195
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 0.047214705497026443,
      "learning_rate": 5e-05,
      "loss": 0.0379,
      "step": 1200
    },
    {
      "epoch": 1.4606060606060607,
      "grad_norm": 0.06587118655443192,
      "learning_rate": 5e-05,
      "loss": 0.0353,
      "step": 1205
    },
    {
      "epoch": 1.4666666666666666,
      "grad_norm": 0.05413132533431053,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 1210
    },
    {
      "epoch": 1.4727272727272727,
      "grad_norm": 0.05740585923194885,
      "learning_rate": 5e-05,
      "loss": 0.033,
      "step": 1215
    },
    {
      "epoch": 1.4787878787878788,
      "grad_norm": 0.048915714025497437,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 1220
    },
    {
      "epoch": 1.4848484848484849,
      "grad_norm": 0.04696843400597572,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 1225
    },
    {
      "epoch": 1.490909090909091,
      "grad_norm": 0.05023559182882309,
      "learning_rate": 5e-05,
      "loss": 0.0357,
      "step": 1230
    },
    {
      "epoch": 1.496969696969697,
      "grad_norm": 0.05131855234503746,
      "learning_rate": 5e-05,
      "loss": 0.0352,
      "step": 1235
    },
    {
      "epoch": 1.5006060606060605,
      "eval_average": 0.5922461075058437,
      "eval_crossner_ai": 0.5854849964609353,
      "eval_crossner_literature": 0.5396739129934935,
      "eval_crossner_music": 0.6933122278882393,
      "eval_crossner_politics": 0.6348165495206315,
      "eval_crossner_science": 0.6672678087868367,
      "eval_mit-movie": 0.5627705627213629,
      "eval_mit-restaurant": 0.4623966941694066,
      "eval_runtime": 602.4181,
      "eval_samples_per_second": 10.101,
      "eval_steps_per_second": 0.159,
      "step": 1238
    },
    {
      "epoch": 1.503030303030303,
      "grad_norm": 0.06400558352470398,
      "learning_rate": 5e-05,
      "loss": 0.0359,
      "step": 1240
    },
    {
      "epoch": 1.509090909090909,
      "grad_norm": 0.04228353500366211,
      "learning_rate": 5e-05,
      "loss": 0.036,
      "step": 1245
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 0.05078113451600075,
      "learning_rate": 5e-05,
      "loss": 0.0353,
      "step": 1250
    },
    {
      "epoch": 1.5212121212121212,
      "grad_norm": 0.05498822405934334,
      "learning_rate": 5e-05,
      "loss": 0.0328,
      "step": 1255
    },
    {
      "epoch": 1.5272727272727273,
      "grad_norm": 0.045320674777030945,
      "learning_rate": 5e-05,
      "loss": 0.035,
      "step": 1260
    },
    {
      "epoch": 1.5333333333333334,
      "grad_norm": 0.04272303357720375,
      "learning_rate": 5e-05,
      "loss": 0.0349,
      "step": 1265
    },
    {
      "epoch": 1.5393939393939393,
      "grad_norm": 0.06665275990962982,
      "learning_rate": 5e-05,
      "loss": 0.0344,
      "step": 1270
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 0.04776114225387573,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 1275
    },
    {
      "epoch": 1.5515151515151515,
      "grad_norm": 0.039244480431079865,
      "learning_rate": 5e-05,
      "loss": 0.0356,
      "step": 1280
    },
    {
      "epoch": 1.5575757575757576,
      "grad_norm": 0.041528474539518356,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 1285
    },
    {
      "epoch": 1.5636363636363637,
      "grad_norm": 0.04804516211152077,
      "learning_rate": 5e-05,
      "loss": 0.0351,
      "step": 1290
    },
    {
      "epoch": 1.5696969696969696,
      "grad_norm": 0.07505177706480026,
      "learning_rate": 5e-05,
      "loss": 0.0342,
      "step": 1295
    },
    {
      "epoch": 1.5757575757575757,
      "grad_norm": 0.042077213525772095,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 1300
    },
    {
      "epoch": 1.5818181818181818,
      "grad_norm": 0.05200699344277382,
      "learning_rate": 5e-05,
      "loss": 0.0332,
      "step": 1305
    },
    {
      "epoch": 1.587878787878788,
      "grad_norm": 0.04583054035902023,
      "learning_rate": 5e-05,
      "loss": 0.0349,
      "step": 1310
    },
    {
      "epoch": 1.593939393939394,
      "grad_norm": 0.05436132103204727,
      "learning_rate": 5e-05,
      "loss": 0.035,
      "step": 1315
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.06180718168616295,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 1320
    },
    {
      "epoch": 1.606060606060606,
      "grad_norm": 0.0410441979765892,
      "learning_rate": 5e-05,
      "loss": 0.0332,
      "step": 1325
    },
    {
      "epoch": 1.612121212121212,
      "grad_norm": 0.053606078028678894,
      "learning_rate": 5e-05,
      "loss": 0.0355,
      "step": 1330
    },
    {
      "epoch": 1.6181818181818182,
      "grad_norm": 0.04404682666063309,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 1335
    },
    {
      "epoch": 1.6242424242424243,
      "grad_norm": 0.062164608389139175,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 1340
    },
    {
      "epoch": 1.6303030303030304,
      "grad_norm": 0.07089866697788239,
      "learning_rate": 5e-05,
      "loss": 0.0354,
      "step": 1345
    },
    {
      "epoch": 1.6363636363636365,
      "grad_norm": 0.05570244789123535,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 1350
    },
    {
      "epoch": 1.6424242424242423,
      "grad_norm": 0.049189090728759766,
      "learning_rate": 5e-05,
      "loss": 0.035,
      "step": 1355
    },
    {
      "epoch": 1.6484848484848484,
      "grad_norm": 0.04998036473989487,
      "learning_rate": 5e-05,
      "loss": 0.0332,
      "step": 1360
    },
    {
      "epoch": 1.6545454545454545,
      "grad_norm": 0.060768187046051025,
      "learning_rate": 5e-05,
      "loss": 0.0341,
      "step": 1365
    },
    {
      "epoch": 1.6606060606060606,
      "grad_norm": 0.039364997297525406,
      "learning_rate": 5e-05,
      "loss": 0.0346,
      "step": 1370
    },
    {
      "epoch": 1.6666666666666667,
      "grad_norm": 0.0425075888633728,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 1375
    },
    {
      "epoch": 1.6727272727272726,
      "grad_norm": 0.050194039940834045,
      "learning_rate": 5e-05,
      "loss": 0.0336,
      "step": 1380
    },
    {
      "epoch": 1.6787878787878787,
      "grad_norm": 0.04598807916045189,
      "learning_rate": 5e-05,
      "loss": 0.0324,
      "step": 1385
    },
    {
      "epoch": 1.6848484848484848,
      "grad_norm": 0.05568886548280716,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 1390
    },
    {
      "epoch": 1.690909090909091,
      "grad_norm": 0.0758465975522995,
      "learning_rate": 5e-05,
      "loss": 0.0346,
      "step": 1395
    },
    {
      "epoch": 1.696969696969697,
      "grad_norm": 0.0353296659886837,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 1400
    },
    {
      "epoch": 1.7030303030303031,
      "grad_norm": 0.04681263491511345,
      "learning_rate": 5e-05,
      "loss": 0.0324,
      "step": 1405
    },
    {
      "epoch": 1.709090909090909,
      "grad_norm": 0.05017056316137314,
      "learning_rate": 5e-05,
      "loss": 0.0331,
      "step": 1410
    },
    {
      "epoch": 1.715151515151515,
      "grad_norm": 0.04423948749899864,
      "learning_rate": 5e-05,
      "loss": 0.0336,
      "step": 1415
    },
    {
      "epoch": 1.7212121212121212,
      "grad_norm": 0.05365236848592758,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 1420
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.06594178080558777,
      "learning_rate": 5e-05,
      "loss": 0.0334,
      "step": 1425
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.05260341614484787,
      "learning_rate": 5e-05,
      "loss": 0.0338,
      "step": 1430
    },
    {
      "epoch": 1.7393939393939395,
      "grad_norm": 0.05100301653146744,
      "learning_rate": 5e-05,
      "loss": 0.0343,
      "step": 1435
    },
    {
      "epoch": 1.7454545454545454,
      "grad_norm": 0.06499028205871582,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 1440
    },
    {
      "epoch": 1.7515151515151515,
      "grad_norm": 0.04994543269276619,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 1445
    },
    {
      "epoch": 1.7575757575757576,
      "grad_norm": 0.06329216808080673,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 1450
    },
    {
      "epoch": 1.7636363636363637,
      "grad_norm": 0.05679098144173622,
      "learning_rate": 5e-05,
      "loss": 0.0336,
      "step": 1455
    },
    {
      "epoch": 1.7696969696969698,
      "grad_norm": 0.05254993587732315,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 1460
    },
    {
      "epoch": 1.7757575757575759,
      "grad_norm": 0.043924640864133835,
      "learning_rate": 5e-05,
      "loss": 0.0326,
      "step": 1465
    },
    {
      "epoch": 1.7818181818181817,
      "grad_norm": 0.04603690281510353,
      "learning_rate": 5e-05,
      "loss": 0.0345,
      "step": 1470
    },
    {
      "epoch": 1.7878787878787878,
      "grad_norm": 0.05199052020907402,
      "learning_rate": 5e-05,
      "loss": 0.0326,
      "step": 1475
    },
    {
      "epoch": 1.793939393939394,
      "grad_norm": 0.040388356894254684,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 1480
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.04748731479048729,
      "learning_rate": 5e-05,
      "loss": 0.0332,
      "step": 1485
    },
    {
      "epoch": 1.8060606060606061,
      "grad_norm": 0.060038451105356216,
      "learning_rate": 5e-05,
      "loss": 0.034,
      "step": 1490
    },
    {
      "epoch": 1.812121212121212,
      "grad_norm": 0.05110161751508713,
      "learning_rate": 5e-05,
      "loss": 0.0329,
      "step": 1495
    },
    {
      "epoch": 1.8181818181818181,
      "grad_norm": 0.04168778285384178,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 1500
    },
    {
      "epoch": 1.8242424242424242,
      "grad_norm": 0.05895324423909187,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 1505
    },
    {
      "epoch": 1.8303030303030303,
      "grad_norm": 0.0398712158203125,
      "learning_rate": 5e-05,
      "loss": 0.0334,
      "step": 1510
    },
    {
      "epoch": 1.8363636363636364,
      "grad_norm": 0.060001444071531296,
      "learning_rate": 5e-05,
      "loss": 0.0347,
      "step": 1515
    },
    {
      "epoch": 1.8424242424242425,
      "grad_norm": 0.05073675513267517,
      "learning_rate": 5e-05,
      "loss": 0.0327,
      "step": 1520
    },
    {
      "epoch": 1.8484848484848484,
      "grad_norm": 0.0669025331735611,
      "learning_rate": 5e-05,
      "loss": 0.0336,
      "step": 1525
    },
    {
      "epoch": 1.8545454545454545,
      "grad_norm": 0.04290784150362015,
      "learning_rate": 5e-05,
      "loss": 0.0346,
      "step": 1530
    },
    {
      "epoch": 1.8606060606060606,
      "grad_norm": 0.04918001592159271,
      "learning_rate": 5e-05,
      "loss": 0.0344,
      "step": 1535
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.04472113773226738,
      "learning_rate": 5e-05,
      "loss": 0.0332,
      "step": 1540
    },
    {
      "epoch": 1.8727272727272728,
      "grad_norm": 0.052523721009492874,
      "learning_rate": 5e-05,
      "loss": 0.0318,
      "step": 1545
    },
    {
      "epoch": 1.878787878787879,
      "grad_norm": 0.08973512798547745,
      "learning_rate": 5e-05,
      "loss": 0.0321,
      "step": 1550
    },
    {
      "epoch": 1.8848484848484848,
      "grad_norm": 0.04704463481903076,
      "learning_rate": 5e-05,
      "loss": 0.0367,
      "step": 1555
    },
    {
      "epoch": 1.8909090909090909,
      "grad_norm": 0.050860077142715454,
      "learning_rate": 5e-05,
      "loss": 0.0324,
      "step": 1560
    },
    {
      "epoch": 1.896969696969697,
      "grad_norm": 0.05936155095696449,
      "learning_rate": 5e-05,
      "loss": 0.0333,
      "step": 1565
    },
    {
      "epoch": 1.903030303030303,
      "grad_norm": 0.05070656165480614,
      "learning_rate": 5e-05,
      "loss": 0.0336,
      "step": 1570
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 0.056354206055402756,
      "learning_rate": 5e-05,
      "loss": 0.0327,
      "step": 1575
    },
    {
      "epoch": 1.915151515151515,
      "grad_norm": 0.05858565866947174,
      "learning_rate": 5e-05,
      "loss": 0.0355,
      "step": 1580
    },
    {
      "epoch": 1.9212121212121211,
      "grad_norm": 0.04813086614012718,
      "learning_rate": 5e-05,
      "loss": 0.0338,
      "step": 1585
    },
    {
      "epoch": 1.9272727272727272,
      "grad_norm": 0.0753236636519432,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 1590
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.06351776421070099,
      "learning_rate": 5e-05,
      "loss": 0.0348,
      "step": 1595
    },
    {
      "epoch": 1.9393939393939394,
      "grad_norm": 0.055220331996679306,
      "learning_rate": 5e-05,
      "loss": 0.0339,
      "step": 1600
    },
    {
      "epoch": 1.9454545454545455,
      "grad_norm": 0.0633152648806572,
      "learning_rate": 5e-05,
      "loss": 0.0331,
      "step": 1605
    },
    {
      "epoch": 1.9515151515151514,
      "grad_norm": 0.04389311373233795,
      "learning_rate": 5e-05,
      "loss": 0.0337,
      "step": 1610
    },
    {
      "epoch": 1.9575757575757575,
      "grad_norm": 0.04735228791832924,
      "learning_rate": 5e-05,
      "loss": 0.0332,
      "step": 1615
    },
    {
      "epoch": 1.9636363636363636,
      "grad_norm": 0.04047701880335808,
      "learning_rate": 5e-05,
      "loss": 0.0325,
      "step": 1620
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 0.04750264436006546,
      "learning_rate": 5e-05,
      "loss": 0.0329,
      "step": 1625
    },
    {
      "epoch": 1.9757575757575758,
      "grad_norm": 0.04866630956530571,
      "learning_rate": 5e-05,
      "loss": 0.0323,
      "step": 1630
    },
    {
      "epoch": 1.981818181818182,
      "grad_norm": 0.043967120349407196,
      "learning_rate": 5e-05,
      "loss": 0.0322,
      "step": 1635
    },
    {
      "epoch": 1.9878787878787878,
      "grad_norm": 0.044381216168403625,
      "learning_rate": 5e-05,
      "loss": 0.033,
      "step": 1640
    },
    {
      "epoch": 1.993939393939394,
      "grad_norm": 0.04057879000902176,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 1645
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.05267352610826492,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 1650
    },
    {
      "epoch": 2.0,
      "eval_average": 0.6128951532168693,
      "eval_crossner_ai": 0.586036671318191,
      "eval_crossner_literature": 0.5482815056783416,
      "eval_crossner_music": 0.7389240505828821,
      "eval_crossner_politics": 0.6592927246734585,
      "eval_crossner_science": 0.6907123534216418,
      "eval_mit-movie": 0.5819411703921676,
      "eval_mit-restaurant": 0.4850775964514026,
      "eval_runtime": 605.2166,
      "eval_samples_per_second": 10.054,
      "eval_steps_per_second": 0.159,
      "step": 1650
    },
    {
      "epoch": 2.006060606060606,
      "grad_norm": 0.06151062622666359,
      "learning_rate": 5e-05,
      "loss": 0.0328,
      "step": 1655
    },
    {
      "epoch": 2.012121212121212,
      "grad_norm": 0.058824777603149414,
      "learning_rate": 5e-05,
      "loss": 0.0306,
      "step": 1660
    },
    {
      "epoch": 2.018181818181818,
      "grad_norm": 0.047803595662117004,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 1665
    },
    {
      "epoch": 2.0242424242424244,
      "grad_norm": 0.05537231266498566,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 1670
    },
    {
      "epoch": 2.0303030303030303,
      "grad_norm": 0.05965382605791092,
      "learning_rate": 5e-05,
      "loss": 0.0328,
      "step": 1675
    },
    {
      "epoch": 2.036363636363636,
      "grad_norm": 0.05255914479494095,
      "learning_rate": 5e-05,
      "loss": 0.0332,
      "step": 1680
    },
    {
      "epoch": 2.0424242424242425,
      "grad_norm": 0.046549540013074875,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 1685
    },
    {
      "epoch": 2.0484848484848484,
      "grad_norm": 0.04661072790622711,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 1690
    },
    {
      "epoch": 2.0545454545454547,
      "grad_norm": 0.042320434004068375,
      "learning_rate": 5e-05,
      "loss": 0.0321,
      "step": 1695
    },
    {
      "epoch": 2.0606060606060606,
      "grad_norm": 0.050901055335998535,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 1700
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.04859345406293869,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 1705
    },
    {
      "epoch": 2.0727272727272728,
      "grad_norm": 0.06658387184143066,
      "learning_rate": 5e-05,
      "loss": 0.0321,
      "step": 1710
    },
    {
      "epoch": 2.0787878787878786,
      "grad_norm": 0.04425247758626938,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 1715
    },
    {
      "epoch": 2.084848484848485,
      "grad_norm": 0.04081323370337486,
      "learning_rate": 5e-05,
      "loss": 0.0305,
      "step": 1720
    },
    {
      "epoch": 2.090909090909091,
      "grad_norm": 0.05987773835659027,
      "learning_rate": 5e-05,
      "loss": 0.0316,
      "step": 1725
    },
    {
      "epoch": 2.096969696969697,
      "grad_norm": 0.046439774334430695,
      "learning_rate": 5e-05,
      "loss": 0.0323,
      "step": 1730
    },
    {
      "epoch": 2.103030303030303,
      "grad_norm": 0.039646271616220474,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 1735
    },
    {
      "epoch": 2.109090909090909,
      "grad_norm": 0.07345104217529297,
      "learning_rate": 5e-05,
      "loss": 0.0311,
      "step": 1740
    },
    {
      "epoch": 2.1151515151515152,
      "grad_norm": 0.06688502430915833,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 1745
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 0.05484945699572563,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 1750
    },
    {
      "epoch": 2.1272727272727274,
      "grad_norm": 0.05491612106561661,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 1755
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.0612156055867672,
      "learning_rate": 5e-05,
      "loss": 0.0309,
      "step": 1760
    },
    {
      "epoch": 2.139393939393939,
      "grad_norm": 0.03820618614554405,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 1765
    },
    {
      "epoch": 2.1454545454545455,
      "grad_norm": 0.048043008893728256,
      "learning_rate": 5e-05,
      "loss": 0.0318,
      "step": 1770
    },
    {
      "epoch": 2.1515151515151514,
      "grad_norm": 0.045766767114400864,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 1775
    },
    {
      "epoch": 2.1575757575757577,
      "grad_norm": 0.04954197257757187,
      "learning_rate": 5e-05,
      "loss": 0.0318,
      "step": 1780
    },
    {
      "epoch": 2.1636363636363636,
      "grad_norm": 0.04767274856567383,
      "learning_rate": 5e-05,
      "loss": 0.0323,
      "step": 1785
    },
    {
      "epoch": 2.16969696969697,
      "grad_norm": 0.040674567222595215,
      "learning_rate": 5e-05,
      "loss": 0.0305,
      "step": 1790
    },
    {
      "epoch": 2.175757575757576,
      "grad_norm": 0.05696846917271614,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 1795
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 0.049947384744882584,
      "learning_rate": 5e-05,
      "loss": 0.0307,
      "step": 1800
    },
    {
      "epoch": 2.187878787878788,
      "grad_norm": 0.055800944566726685,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 1805
    },
    {
      "epoch": 2.193939393939394,
      "grad_norm": 0.04166347533464432,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 1810
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.03969123214483261,
      "learning_rate": 5e-05,
      "loss": 0.0311,
      "step": 1815
    },
    {
      "epoch": 2.206060606060606,
      "grad_norm": 0.049315281212329865,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 1820
    },
    {
      "epoch": 2.212121212121212,
      "grad_norm": 0.046311166137456894,
      "learning_rate": 5e-05,
      "loss": 0.0327,
      "step": 1825
    },
    {
      "epoch": 2.2181818181818183,
      "grad_norm": 0.039723485708236694,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 1830
    },
    {
      "epoch": 2.224242424242424,
      "grad_norm": 0.06615424156188965,
      "learning_rate": 5e-05,
      "loss": 0.0297,
      "step": 1835
    },
    {
      "epoch": 2.2303030303030305,
      "grad_norm": 0.044731441885232925,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 1840
    },
    {
      "epoch": 2.2363636363636363,
      "grad_norm": 0.039907731115818024,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 1845
    },
    {
      "epoch": 2.242424242424242,
      "grad_norm": 0.05147165805101395,
      "learning_rate": 5e-05,
      "loss": 0.0326,
      "step": 1850
    },
    {
      "epoch": 2.2484848484848485,
      "grad_norm": 0.04808584600687027,
      "learning_rate": 5e-05,
      "loss": 0.0318,
      "step": 1855
    },
    {
      "epoch": 2.2545454545454544,
      "grad_norm": 0.04493199288845062,
      "learning_rate": 5e-05,
      "loss": 0.0316,
      "step": 1860
    },
    {
      "epoch": 2.2606060606060607,
      "grad_norm": 0.03892715647816658,
      "learning_rate": 5e-05,
      "loss": 0.0321,
      "step": 1865
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.04593859240412712,
      "learning_rate": 5e-05,
      "loss": 0.0297,
      "step": 1870
    },
    {
      "epoch": 2.272727272727273,
      "grad_norm": 0.05826153978705406,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 1875
    },
    {
      "epoch": 2.278787878787879,
      "grad_norm": 0.044886913150548935,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 1880
    },
    {
      "epoch": 2.2848484848484847,
      "grad_norm": 0.054926786571741104,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 1885
    },
    {
      "epoch": 2.290909090909091,
      "grad_norm": 0.050195157527923584,
      "learning_rate": 5e-05,
      "loss": 0.0322,
      "step": 1890
    },
    {
      "epoch": 2.296969696969697,
      "grad_norm": 0.03867163881659508,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 1895
    },
    {
      "epoch": 2.303030303030303,
      "grad_norm": 0.04655490443110466,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 1900
    },
    {
      "epoch": 2.309090909090909,
      "grad_norm": 0.04781629517674446,
      "learning_rate": 5e-05,
      "loss": 0.0328,
      "step": 1905
    },
    {
      "epoch": 2.315151515151515,
      "grad_norm": 0.05583782121539116,
      "learning_rate": 5e-05,
      "loss": 0.0302,
      "step": 1910
    },
    {
      "epoch": 2.3212121212121213,
      "grad_norm": 0.04437439143657684,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 1915
    },
    {
      "epoch": 2.327272727272727,
      "grad_norm": 0.052671417593955994,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 1920
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.042088959366083145,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 1925
    },
    {
      "epoch": 2.3393939393939394,
      "grad_norm": 0.07228727638721466,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 1930
    },
    {
      "epoch": 2.3454545454545452,
      "grad_norm": 0.045577455312013626,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 1935
    },
    {
      "epoch": 2.3515151515151516,
      "grad_norm": 0.04241783171892166,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 1940
    },
    {
      "epoch": 2.3575757575757574,
      "grad_norm": 0.03725886344909668,
      "learning_rate": 5e-05,
      "loss": 0.0305,
      "step": 1945
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 0.0494803786277771,
      "learning_rate": 5e-05,
      "loss": 0.0341,
      "step": 1950
    },
    {
      "epoch": 2.3696969696969696,
      "grad_norm": 0.04060351848602295,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 1955
    },
    {
      "epoch": 2.375757575757576,
      "grad_norm": 0.05367689207196236,
      "learning_rate": 5e-05,
      "loss": 0.0319,
      "step": 1960
    },
    {
      "epoch": 2.381818181818182,
      "grad_norm": 0.04030327871441841,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 1965
    },
    {
      "epoch": 2.3878787878787877,
      "grad_norm": 0.05422325059771538,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 1970
    },
    {
      "epoch": 2.393939393939394,
      "grad_norm": 0.05661623179912567,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 1975
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.06293974816799164,
      "learning_rate": 5e-05,
      "loss": 0.0328,
      "step": 1980
    },
    {
      "epoch": 2.4060606060606062,
      "grad_norm": 0.04714273288846016,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 1985
    },
    {
      "epoch": 2.412121212121212,
      "grad_norm": 0.04438882693648338,
      "learning_rate": 5e-05,
      "loss": 0.0333,
      "step": 1990
    },
    {
      "epoch": 2.418181818181818,
      "grad_norm": 0.040730468928813934,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 1995
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 0.039338432252407074,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 2000
    },
    {
      "epoch": 2.43030303030303,
      "grad_norm": 0.04418187215924263,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 2005
    },
    {
      "epoch": 2.4363636363636365,
      "grad_norm": 0.04645801708102226,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 2010
    },
    {
      "epoch": 2.4424242424242424,
      "grad_norm": 0.03647175058722496,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 2015
    },
    {
      "epoch": 2.4484848484848483,
      "grad_norm": 0.047627780586481094,
      "learning_rate": 5e-05,
      "loss": 0.0313,
      "step": 2020
    },
    {
      "epoch": 2.4545454545454546,
      "grad_norm": 0.04942844808101654,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 2025
    },
    {
      "epoch": 2.4606060606060605,
      "grad_norm": 0.07455330342054367,
      "learning_rate": 5e-05,
      "loss": 0.0321,
      "step": 2030
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.057274624705314636,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 2035
    },
    {
      "epoch": 2.4727272727272727,
      "grad_norm": 0.04733812436461449,
      "learning_rate": 5e-05,
      "loss": 0.0311,
      "step": 2040
    },
    {
      "epoch": 2.478787878787879,
      "grad_norm": 0.043880242854356766,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 2045
    },
    {
      "epoch": 2.484848484848485,
      "grad_norm": 0.04730875790119171,
      "learning_rate": 5e-05,
      "loss": 0.0306,
      "step": 2050
    },
    {
      "epoch": 2.4909090909090907,
      "grad_norm": 0.043111544102430344,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 2055
    },
    {
      "epoch": 2.496969696969697,
      "grad_norm": 0.0342271663248539,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 2060
    },
    {
      "epoch": 2.4993939393939395,
      "eval_average": 0.6379020668133034,
      "eval_crossner_ai": 0.6171958615269003,
      "eval_crossner_literature": 0.5926336194460745,
      "eval_crossner_music": 0.7748607795840194,
      "eval_crossner_politics": 0.6908752327246529,
      "eval_crossner_science": 0.7178432715972487,
      "eval_mit-movie": 0.5843689020763665,
      "eval_mit-restaurant": 0.48753680073786104,
      "eval_runtime": 610.4042,
      "eval_samples_per_second": 9.969,
      "eval_steps_per_second": 0.157,
      "step": 2062
    },
    {
      "epoch": 2.503030303030303,
      "grad_norm": 0.05372614040970802,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 2065
    },
    {
      "epoch": 2.5090909090909093,
      "grad_norm": 0.07061748206615448,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 2070
    },
    {
      "epoch": 2.515151515151515,
      "grad_norm": 0.06444719433784485,
      "learning_rate": 5e-05,
      "loss": 0.0317,
      "step": 2075
    },
    {
      "epoch": 2.521212121212121,
      "grad_norm": 0.05272315815091133,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 2080
    },
    {
      "epoch": 2.5272727272727273,
      "grad_norm": 0.06052810698747635,
      "learning_rate": 5e-05,
      "loss": 0.0302,
      "step": 2085
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.04444526508450508,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 2090
    },
    {
      "epoch": 2.5393939393939395,
      "grad_norm": 0.060236986726522446,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 2095
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 0.042165543884038925,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 2100
    },
    {
      "epoch": 2.5515151515151517,
      "grad_norm": 0.038601405918598175,
      "learning_rate": 5e-05,
      "loss": 0.0308,
      "step": 2105
    },
    {
      "epoch": 2.5575757575757576,
      "grad_norm": 0.05023554340004921,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 2110
    },
    {
      "epoch": 2.5636363636363635,
      "grad_norm": 0.04048175364732742,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 2115
    },
    {
      "epoch": 2.56969696969697,
      "grad_norm": 0.05063512921333313,
      "learning_rate": 5e-05,
      "loss": 0.0316,
      "step": 2120
    },
    {
      "epoch": 2.5757575757575757,
      "grad_norm": 0.06379430741071701,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 2125
    },
    {
      "epoch": 2.581818181818182,
      "grad_norm": 0.06509973853826523,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 2130
    },
    {
      "epoch": 2.587878787878788,
      "grad_norm": 0.04953153058886528,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 2135
    },
    {
      "epoch": 2.5939393939393938,
      "grad_norm": 0.06324736773967743,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 2140
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.07059264928102493,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 2145
    },
    {
      "epoch": 2.606060606060606,
      "grad_norm": 0.04541000351309776,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 2150
    },
    {
      "epoch": 2.6121212121212123,
      "grad_norm": 0.05597423389554024,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 2155
    },
    {
      "epoch": 2.618181818181818,
      "grad_norm": 0.05401655659079552,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 2160
    },
    {
      "epoch": 2.624242424242424,
      "grad_norm": 0.06905027478933334,
      "learning_rate": 5e-05,
      "loss": 0.032,
      "step": 2165
    },
    {
      "epoch": 2.6303030303030304,
      "grad_norm": 0.05907181277871132,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 2170
    },
    {
      "epoch": 2.6363636363636362,
      "grad_norm": 0.06471002101898193,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 2175
    },
    {
      "epoch": 2.6424242424242426,
      "grad_norm": 0.04287099093198776,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 2180
    },
    {
      "epoch": 2.6484848484848484,
      "grad_norm": 0.043180495500564575,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 2185
    },
    {
      "epoch": 2.6545454545454548,
      "grad_norm": 0.037595320492982864,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 2190
    },
    {
      "epoch": 2.6606060606060606,
      "grad_norm": 0.07802839577198029,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 2195
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.05832613632082939,
      "learning_rate": 5e-05,
      "loss": 0.0314,
      "step": 2200
    },
    {
      "epoch": 2.672727272727273,
      "grad_norm": 0.0484803207218647,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 2205
    },
    {
      "epoch": 2.6787878787878787,
      "grad_norm": 0.04343926161527634,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 2210
    },
    {
      "epoch": 2.684848484848485,
      "grad_norm": 0.03524085134267807,
      "learning_rate": 5e-05,
      "loss": 0.0327,
      "step": 2215
    },
    {
      "epoch": 2.690909090909091,
      "grad_norm": 0.062253184616565704,
      "learning_rate": 5e-05,
      "loss": 0.0302,
      "step": 2220
    },
    {
      "epoch": 2.696969696969697,
      "grad_norm": 0.04657703638076782,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 2225
    },
    {
      "epoch": 2.703030303030303,
      "grad_norm": 0.05622091144323349,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 2230
    },
    {
      "epoch": 2.709090909090909,
      "grad_norm": 0.03769258037209511,
      "learning_rate": 5e-05,
      "loss": 0.0311,
      "step": 2235
    },
    {
      "epoch": 2.7151515151515153,
      "grad_norm": 0.038313351571559906,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 2240
    },
    {
      "epoch": 2.721212121212121,
      "grad_norm": 0.042932409793138504,
      "learning_rate": 5e-05,
      "loss": 0.0321,
      "step": 2245
    },
    {
      "epoch": 2.727272727272727,
      "grad_norm": 0.0396142303943634,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 2250
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.062911257147789,
      "learning_rate": 5e-05,
      "loss": 0.0308,
      "step": 2255
    },
    {
      "epoch": 2.7393939393939393,
      "grad_norm": 0.03643081709742546,
      "learning_rate": 5e-05,
      "loss": 0.0297,
      "step": 2260
    },
    {
      "epoch": 2.7454545454545456,
      "grad_norm": 0.048007648438215256,
      "learning_rate": 5e-05,
      "loss": 0.0287,
      "step": 2265
    },
    {
      "epoch": 2.7515151515151515,
      "grad_norm": 0.05368134751915932,
      "learning_rate": 5e-05,
      "loss": 0.031,
      "step": 2270
    },
    {
      "epoch": 2.757575757575758,
      "grad_norm": 0.04905089735984802,
      "learning_rate": 5e-05,
      "loss": 0.0306,
      "step": 2275
    },
    {
      "epoch": 2.7636363636363637,
      "grad_norm": 0.0482952743768692,
      "learning_rate": 5e-05,
      "loss": 0.0305,
      "step": 2280
    },
    {
      "epoch": 2.7696969696969695,
      "grad_norm": 0.053626395761966705,
      "learning_rate": 5e-05,
      "loss": 0.0305,
      "step": 2285
    },
    {
      "epoch": 2.775757575757576,
      "grad_norm": 0.04028894379734993,
      "learning_rate": 5e-05,
      "loss": 0.0297,
      "step": 2290
    },
    {
      "epoch": 2.7818181818181817,
      "grad_norm": 0.05571451783180237,
      "learning_rate": 5e-05,
      "loss": 0.0308,
      "step": 2295
    },
    {
      "epoch": 2.787878787878788,
      "grad_norm": 0.0479196272790432,
      "learning_rate": 5e-05,
      "loss": 0.0304,
      "step": 2300
    },
    {
      "epoch": 2.793939393939394,
      "grad_norm": 0.05851449444890022,
      "learning_rate": 5e-05,
      "loss": 0.0297,
      "step": 2305
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.04345862939953804,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 2310
    },
    {
      "epoch": 2.806060606060606,
      "grad_norm": 0.04341760650277138,
      "learning_rate": 5e-05,
      "loss": 0.0312,
      "step": 2315
    },
    {
      "epoch": 2.812121212121212,
      "grad_norm": 0.04732973501086235,
      "learning_rate": 5e-05,
      "loss": 0.0315,
      "step": 2320
    },
    {
      "epoch": 2.8181818181818183,
      "grad_norm": 0.046325284987688065,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 2325
    },
    {
      "epoch": 2.824242424242424,
      "grad_norm": 0.03932766243815422,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 2330
    },
    {
      "epoch": 2.83030303030303,
      "grad_norm": 0.043588005006313324,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 2335
    },
    {
      "epoch": 2.8363636363636364,
      "grad_norm": 0.058124735951423645,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 2340
    },
    {
      "epoch": 2.8424242424242423,
      "grad_norm": 0.052231620997190475,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 2345
    },
    {
      "epoch": 2.8484848484848486,
      "grad_norm": 0.04333264008164406,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 2350
    },
    {
      "epoch": 2.8545454545454545,
      "grad_norm": 0.04349289834499359,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 2355
    },
    {
      "epoch": 2.860606060606061,
      "grad_norm": 0.04412107914686203,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 2360
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.039459697902202606,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 2365
    },
    {
      "epoch": 2.8727272727272726,
      "grad_norm": 0.04867342859506607,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 2370
    },
    {
      "epoch": 2.878787878787879,
      "grad_norm": 0.04465062543749809,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 2375
    },
    {
      "epoch": 2.8848484848484848,
      "grad_norm": 0.05725083127617836,
      "learning_rate": 5e-05,
      "loss": 0.0295,
      "step": 2380
    },
    {
      "epoch": 2.890909090909091,
      "grad_norm": 0.04091992601752281,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 2385
    },
    {
      "epoch": 2.896969696969697,
      "grad_norm": 0.061108704656362534,
      "learning_rate": 5e-05,
      "loss": 0.0316,
      "step": 2390
    },
    {
      "epoch": 2.903030303030303,
      "grad_norm": 0.04623361676931381,
      "learning_rate": 5e-05,
      "loss": 0.0316,
      "step": 2395
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 0.04039705917239189,
      "learning_rate": 5e-05,
      "loss": 0.0301,
      "step": 2400
    },
    {
      "epoch": 2.915151515151515,
      "grad_norm": 0.04167446866631508,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 2405
    },
    {
      "epoch": 2.9212121212121214,
      "grad_norm": 0.04547049105167389,
      "learning_rate": 5e-05,
      "loss": 0.0297,
      "step": 2410
    },
    {
      "epoch": 2.9272727272727272,
      "grad_norm": 0.057736486196517944,
      "learning_rate": 5e-05,
      "loss": 0.0307,
      "step": 2415
    },
    {
      "epoch": 2.933333333333333,
      "grad_norm": 0.043314944952726364,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 2420
    },
    {
      "epoch": 2.9393939393939394,
      "grad_norm": 0.052022580057382584,
      "learning_rate": 5e-05,
      "loss": 0.0307,
      "step": 2425
    },
    {
      "epoch": 2.9454545454545453,
      "grad_norm": 0.04903764650225639,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 2430
    },
    {
      "epoch": 2.9515151515151516,
      "grad_norm": 0.04512345790863037,
      "learning_rate": 5e-05,
      "loss": 0.0306,
      "step": 2435
    },
    {
      "epoch": 2.9575757575757575,
      "grad_norm": 0.04762592539191246,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 2440
    },
    {
      "epoch": 2.963636363636364,
      "grad_norm": 0.04601770639419556,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 2445
    },
    {
      "epoch": 2.9696969696969697,
      "grad_norm": 0.038316160440444946,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 2450
    },
    {
      "epoch": 2.9757575757575756,
      "grad_norm": 0.03856251761317253,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 2455
    },
    {
      "epoch": 2.981818181818182,
      "grad_norm": 0.043273963034152985,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 2460
    },
    {
      "epoch": 2.987878787878788,
      "grad_norm": 0.03665269911289215,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 2465
    },
    {
      "epoch": 2.993939393939394,
      "grad_norm": 0.03151369467377663,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 2470
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.039404336363077164,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 2475
    },
    {
      "epoch": 3.0,
      "eval_average": 0.6428253381551591,
      "eval_crossner_ai": 0.616487455147136,
      "eval_crossner_literature": 0.601973684160569,
      "eval_crossner_music": 0.7764332473215225,
      "eval_crossner_politics": 0.6766682293608285,
      "eval_crossner_science": 0.719187358866518,
      "eval_mit-movie": 0.6117575014813156,
      "eval_mit-restaurant": 0.49726989074822414,
      "eval_runtime": 603.7062,
      "eval_samples_per_second": 10.079,
      "eval_steps_per_second": 0.159,
      "step": 2475
    },
    {
      "epoch": 3.006060606060606,
      "grad_norm": 0.03921062499284744,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 2480
    },
    {
      "epoch": 3.012121212121212,
      "grad_norm": 0.039839472621679306,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 2485
    },
    {
      "epoch": 3.018181818181818,
      "grad_norm": 0.036989904940128326,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 2490
    },
    {
      "epoch": 3.0242424242424244,
      "grad_norm": 0.07926663756370544,
      "learning_rate": 5e-05,
      "loss": 0.0285,
      "step": 2495
    },
    {
      "epoch": 3.0303030303030303,
      "grad_norm": 0.04322892054915428,
      "learning_rate": 5e-05,
      "loss": 0.03,
      "step": 2500
    },
    {
      "epoch": 3.036363636363636,
      "grad_norm": 0.07271397858858109,
      "learning_rate": 5e-05,
      "loss": 0.0281,
      "step": 2505
    },
    {
      "epoch": 3.0424242424242425,
      "grad_norm": 0.03526373207569122,
      "learning_rate": 5e-05,
      "loss": 0.0291,
      "step": 2510
    },
    {
      "epoch": 3.0484848484848484,
      "grad_norm": 0.03360647335648537,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 2515
    },
    {
      "epoch": 3.0545454545454547,
      "grad_norm": 0.0460897758603096,
      "learning_rate": 5e-05,
      "loss": 0.0303,
      "step": 2520
    },
    {
      "epoch": 3.0606060606060606,
      "grad_norm": 0.06532470881938934,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 2525
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.06243832781910896,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 2530
    },
    {
      "epoch": 3.0727272727272728,
      "grad_norm": 0.041783880442380905,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 2535
    },
    {
      "epoch": 3.0787878787878786,
      "grad_norm": 0.04445856437087059,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 2540
    },
    {
      "epoch": 3.084848484848485,
      "grad_norm": 0.05658264458179474,
      "learning_rate": 5e-05,
      "loss": 0.0267,
      "step": 2545
    },
    {
      "epoch": 3.090909090909091,
      "grad_norm": 0.03701433166861534,
      "learning_rate": 5e-05,
      "loss": 0.0265,
      "step": 2550
    },
    {
      "epoch": 3.096969696969697,
      "grad_norm": 0.06396092474460602,
      "learning_rate": 5e-05,
      "loss": 0.0307,
      "step": 2555
    },
    {
      "epoch": 3.103030303030303,
      "grad_norm": 0.049497153609991074,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 2560
    },
    {
      "epoch": 3.109090909090909,
      "grad_norm": 0.059118546545505524,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 2565
    },
    {
      "epoch": 3.1151515151515152,
      "grad_norm": 0.056557830423116684,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 2570
    },
    {
      "epoch": 3.121212121212121,
      "grad_norm": 0.042770687490701675,
      "learning_rate": 5e-05,
      "loss": 0.0299,
      "step": 2575
    },
    {
      "epoch": 3.1272727272727274,
      "grad_norm": 0.04688771814107895,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 2580
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.064878448843956,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 2585
    },
    {
      "epoch": 3.139393939393939,
      "grad_norm": 0.0408673994243145,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 2590
    },
    {
      "epoch": 3.1454545454545455,
      "grad_norm": 0.04743499308824539,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 2595
    },
    {
      "epoch": 3.1515151515151514,
      "grad_norm": 0.03627030551433563,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 2600
    },
    {
      "epoch": 3.1575757575757577,
      "grad_norm": 0.04150136560201645,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 2605
    },
    {
      "epoch": 3.1636363636363636,
      "grad_norm": 0.05047564208507538,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 2610
    },
    {
      "epoch": 3.16969696969697,
      "grad_norm": 0.04717445746064186,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 2615
    },
    {
      "epoch": 3.175757575757576,
      "grad_norm": 0.04982085898518562,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 2620
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 0.03973981365561485,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 2625
    },
    {
      "epoch": 3.187878787878788,
      "grad_norm": 0.040387995541095734,
      "learning_rate": 5e-05,
      "loss": 0.0291,
      "step": 2630
    },
    {
      "epoch": 3.193939393939394,
      "grad_norm": 0.03660165145993233,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 2635
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.04102002829313278,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 2640
    },
    {
      "epoch": 3.206060606060606,
      "grad_norm": 0.04768123850226402,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 2645
    },
    {
      "epoch": 3.212121212121212,
      "grad_norm": 0.04671141877770424,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 2650
    },
    {
      "epoch": 3.2181818181818183,
      "grad_norm": 0.03978985175490379,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 2655
    },
    {
      "epoch": 3.224242424242424,
      "grad_norm": 0.04427233710885048,
      "learning_rate": 5e-05,
      "loss": 0.0285,
      "step": 2660
    },
    {
      "epoch": 3.2303030303030305,
      "grad_norm": 0.044488970190286636,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 2665
    },
    {
      "epoch": 3.2363636363636363,
      "grad_norm": 0.05478399246931076,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 2670
    },
    {
      "epoch": 3.242424242424242,
      "grad_norm": 0.05086371675133705,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 2675
    },
    {
      "epoch": 3.2484848484848485,
      "grad_norm": 0.059403784573078156,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 2680
    },
    {
      "epoch": 3.2545454545454544,
      "grad_norm": 0.04183360934257507,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 2685
    },
    {
      "epoch": 3.2606060606060607,
      "grad_norm": 0.041125502437353134,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 2690
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.06092136725783348,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 2695
    },
    {
      "epoch": 3.272727272727273,
      "grad_norm": 0.04186001792550087,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 2700
    },
    {
      "epoch": 3.278787878787879,
      "grad_norm": 0.04238676652312279,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 2705
    },
    {
      "epoch": 3.2848484848484847,
      "grad_norm": 0.049277640879154205,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 2710
    },
    {
      "epoch": 3.290909090909091,
      "grad_norm": 0.04271101579070091,
      "learning_rate": 5e-05,
      "loss": 0.0272,
      "step": 2715
    },
    {
      "epoch": 3.296969696969697,
      "grad_norm": 0.041214097291231155,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 2720
    },
    {
      "epoch": 3.303030303030303,
      "grad_norm": 0.05011172965168953,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 2725
    },
    {
      "epoch": 3.309090909090909,
      "grad_norm": 0.03391839191317558,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 2730
    },
    {
      "epoch": 3.315151515151515,
      "grad_norm": 0.07081297785043716,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 2735
    },
    {
      "epoch": 3.3212121212121213,
      "grad_norm": 0.0390753448009491,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 2740
    },
    {
      "epoch": 3.327272727272727,
      "grad_norm": 0.05001889541745186,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 2745
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.03348173573613167,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 2750
    },
    {
      "epoch": 3.3393939393939394,
      "grad_norm": 0.038648106157779694,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 2755
    },
    {
      "epoch": 3.3454545454545452,
      "grad_norm": 0.03982597216963768,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 2760
    },
    {
      "epoch": 3.3515151515151516,
      "grad_norm": 0.03769079968333244,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 2765
    },
    {
      "epoch": 3.3575757575757574,
      "grad_norm": 0.04458944499492645,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 2770
    },
    {
      "epoch": 3.3636363636363638,
      "grad_norm": 0.03742107003927231,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 2775
    },
    {
      "epoch": 3.3696969696969696,
      "grad_norm": 0.0485738143324852,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 2780
    },
    {
      "epoch": 3.375757575757576,
      "grad_norm": 0.04001298546791077,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 2785
    },
    {
      "epoch": 3.381818181818182,
      "grad_norm": 0.041646841913461685,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 2790
    },
    {
      "epoch": 3.3878787878787877,
      "grad_norm": 0.037036191672086716,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 2795
    },
    {
      "epoch": 3.393939393939394,
      "grad_norm": 0.052974965423345566,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 2800
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.036026012152433395,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 2805
    },
    {
      "epoch": 3.4060606060606062,
      "grad_norm": 0.03692179545760155,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 2810
    },
    {
      "epoch": 3.412121212121212,
      "grad_norm": 0.040503110736608505,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 2815
    },
    {
      "epoch": 3.418181818181818,
      "grad_norm": 0.04705323278903961,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 2820
    },
    {
      "epoch": 3.4242424242424243,
      "grad_norm": 0.05133044719696045,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 2825
    },
    {
      "epoch": 3.43030303030303,
      "grad_norm": 0.05422220006585121,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 2830
    },
    {
      "epoch": 3.4363636363636365,
      "grad_norm": 0.047176312655210495,
      "learning_rate": 5e-05,
      "loss": 0.0294,
      "step": 2835
    },
    {
      "epoch": 3.4424242424242424,
      "grad_norm": 0.04107426851987839,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 2840
    },
    {
      "epoch": 3.4484848484848483,
      "grad_norm": 0.038816384971141815,
      "learning_rate": 5e-05,
      "loss": 0.0296,
      "step": 2845
    },
    {
      "epoch": 3.4545454545454546,
      "grad_norm": 0.05025389418005943,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 2850
    },
    {
      "epoch": 3.4606060606060605,
      "grad_norm": 0.041109599173069,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 2855
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.062458477914333344,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 2860
    },
    {
      "epoch": 3.4727272727272727,
      "grad_norm": 0.06171433627605438,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 2865
    },
    {
      "epoch": 3.478787878787879,
      "grad_norm": 0.04837648198008537,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 2870
    },
    {
      "epoch": 3.484848484848485,
      "grad_norm": 0.043963946402072906,
      "learning_rate": 5e-05,
      "loss": 0.0272,
      "step": 2875
    },
    {
      "epoch": 3.4909090909090907,
      "grad_norm": 0.04530396685004234,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 2880
    },
    {
      "epoch": 3.496969696969697,
      "grad_norm": 0.038436274975538254,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 2885
    },
    {
      "epoch": 3.5006060606060605,
      "eval_average": 0.6282022651162043,
      "eval_crossner_ai": 0.591859185868552,
      "eval_crossner_literature": 0.5556170447757657,
      "eval_crossner_music": 0.7725646122760138,
      "eval_crossner_politics": 0.6735010872441702,
      "eval_crossner_science": 0.704425590410897,
      "eval_mit-movie": 0.6061412486709926,
      "eval_mit-restaurant": 0.49330708656703764,
      "eval_runtime": 608.0631,
      "eval_samples_per_second": 10.007,
      "eval_steps_per_second": 0.158,
      "step": 2888
    },
    {
      "epoch": 3.503030303030303,
      "grad_norm": 0.041046708822250366,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 2890
    },
    {
      "epoch": 3.5090909090909093,
      "grad_norm": 0.05540839582681656,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 2895
    },
    {
      "epoch": 3.515151515151515,
      "grad_norm": 0.05723531171679497,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 2900
    },
    {
      "epoch": 3.521212121212121,
      "grad_norm": 0.045173872262239456,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 2905
    },
    {
      "epoch": 3.5272727272727273,
      "grad_norm": 0.043023139238357544,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 2910
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.05037825182080269,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 2915
    },
    {
      "epoch": 3.5393939393939395,
      "grad_norm": 0.05216532573103905,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 2920
    },
    {
      "epoch": 3.5454545454545454,
      "grad_norm": 0.03893985599279404,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 2925
    },
    {
      "epoch": 3.5515151515151517,
      "grad_norm": 0.03712200000882149,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 2930
    },
    {
      "epoch": 3.5575757575757576,
      "grad_norm": 0.033536169677972794,
      "learning_rate": 5e-05,
      "loss": 0.0287,
      "step": 2935
    },
    {
      "epoch": 3.5636363636363635,
      "grad_norm": 0.041627850383520126,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 2940
    },
    {
      "epoch": 3.56969696969697,
      "grad_norm": 0.047564797103405,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 2945
    },
    {
      "epoch": 3.5757575757575757,
      "grad_norm": 0.05165176838636398,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 2950
    },
    {
      "epoch": 3.581818181818182,
      "grad_norm": 0.039918866008520126,
      "learning_rate": 5e-05,
      "loss": 0.0281,
      "step": 2955
    },
    {
      "epoch": 3.587878787878788,
      "grad_norm": 0.043777257204055786,
      "learning_rate": 5e-05,
      "loss": 0.0281,
      "step": 2960
    },
    {
      "epoch": 3.5939393939393938,
      "grad_norm": 0.058113910257816315,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 2965
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.04668101668357849,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 2970
    },
    {
      "epoch": 3.606060606060606,
      "grad_norm": 0.040997497737407684,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 2975
    },
    {
      "epoch": 3.6121212121212123,
      "grad_norm": 0.03591303527355194,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 2980
    },
    {
      "epoch": 3.618181818181818,
      "grad_norm": 0.050950787961483,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 2985
    },
    {
      "epoch": 3.624242424242424,
      "grad_norm": 0.03706828132271767,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 2990
    },
    {
      "epoch": 3.6303030303030304,
      "grad_norm": 0.16603918373584747,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 2995
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 0.040306802839040756,
      "learning_rate": 5e-05,
      "loss": 0.0281,
      "step": 3000
    },
    {
      "epoch": 3.6424242424242426,
      "grad_norm": 0.05742175877094269,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 3005
    },
    {
      "epoch": 3.6484848484848484,
      "grad_norm": 0.046968184411525726,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 3010
    },
    {
      "epoch": 3.6545454545454548,
      "grad_norm": 0.057790447026491165,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 3015
    },
    {
      "epoch": 3.6606060606060606,
      "grad_norm": 0.06445077061653137,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 3020
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.03706223517656326,
      "learning_rate": 5e-05,
      "loss": 0.0288,
      "step": 3025
    },
    {
      "epoch": 3.672727272727273,
      "grad_norm": 0.04849424958229065,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 3030
    },
    {
      "epoch": 3.6787878787878787,
      "grad_norm": 0.041173651814460754,
      "learning_rate": 5e-05,
      "loss": 0.0265,
      "step": 3035
    },
    {
      "epoch": 3.684848484848485,
      "grad_norm": 0.038033340126276016,
      "learning_rate": 5e-05,
      "loss": 0.0263,
      "step": 3040
    },
    {
      "epoch": 3.690909090909091,
      "grad_norm": 0.035329703241586685,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 3045
    },
    {
      "epoch": 3.696969696969697,
      "grad_norm": 0.04799158498644829,
      "learning_rate": 5e-05,
      "loss": 0.0284,
      "step": 3050
    },
    {
      "epoch": 3.703030303030303,
      "grad_norm": 0.04823992773890495,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 3055
    },
    {
      "epoch": 3.709090909090909,
      "grad_norm": 0.04262559115886688,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 3060
    },
    {
      "epoch": 3.7151515151515153,
      "grad_norm": 0.04726352542638779,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 3065
    },
    {
      "epoch": 3.721212121212121,
      "grad_norm": 0.3389405906200409,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 3070
    },
    {
      "epoch": 3.727272727272727,
      "grad_norm": 0.043569885194301605,
      "learning_rate": 5e-05,
      "loss": 0.0307,
      "step": 3075
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.041828181594610214,
      "learning_rate": 5e-05,
      "loss": 0.029,
      "step": 3080
    },
    {
      "epoch": 3.7393939393939393,
      "grad_norm": 0.07474999874830246,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 3085
    },
    {
      "epoch": 3.7454545454545456,
      "grad_norm": 0.04850531369447708,
      "learning_rate": 5e-05,
      "loss": 0.0298,
      "step": 3090
    },
    {
      "epoch": 3.7515151515151515,
      "grad_norm": 0.04038536548614502,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 3095
    },
    {
      "epoch": 3.757575757575758,
      "grad_norm": 0.03403542563319206,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 3100
    },
    {
      "epoch": 3.7636363636363637,
      "grad_norm": 0.05190834775567055,
      "learning_rate": 5e-05,
      "loss": 0.0291,
      "step": 3105
    },
    {
      "epoch": 3.7696969696969695,
      "grad_norm": 0.044142432510852814,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 3110
    },
    {
      "epoch": 3.775757575757576,
      "grad_norm": 0.04286090284585953,
      "learning_rate": 5e-05,
      "loss": 0.0281,
      "step": 3115
    },
    {
      "epoch": 3.7818181818181817,
      "grad_norm": 0.04146591201424599,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 3120
    },
    {
      "epoch": 3.787878787878788,
      "grad_norm": 0.04504074901342392,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 3125
    },
    {
      "epoch": 3.793939393939394,
      "grad_norm": 0.047032926231622696,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 3130
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.04363613948225975,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 3135
    },
    {
      "epoch": 3.806060606060606,
      "grad_norm": 0.04276721179485321,
      "learning_rate": 5e-05,
      "loss": 0.026,
      "step": 3140
    },
    {
      "epoch": 3.812121212121212,
      "grad_norm": 0.03801874443888664,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 3145
    },
    {
      "epoch": 3.8181818181818183,
      "grad_norm": 0.039671555161476135,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 3150
    },
    {
      "epoch": 3.824242424242424,
      "grad_norm": 0.03848567605018616,
      "learning_rate": 5e-05,
      "loss": 0.0263,
      "step": 3155
    },
    {
      "epoch": 3.83030303030303,
      "grad_norm": 0.04666701331734657,
      "learning_rate": 5e-05,
      "loss": 0.0293,
      "step": 3160
    },
    {
      "epoch": 3.8363636363636364,
      "grad_norm": 0.05390109121799469,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 3165
    },
    {
      "epoch": 3.8424242424242423,
      "grad_norm": 0.0422060452401638,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 3170
    },
    {
      "epoch": 3.8484848484848486,
      "grad_norm": 0.05485120043158531,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 3175
    },
    {
      "epoch": 3.8545454545454545,
      "grad_norm": 0.043971918523311615,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 3180
    },
    {
      "epoch": 3.860606060606061,
      "grad_norm": 0.042199596762657166,
      "learning_rate": 5e-05,
      "loss": 0.0281,
      "step": 3185
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.061121802777051926,
      "learning_rate": 5e-05,
      "loss": 0.0282,
      "step": 3190
    },
    {
      "epoch": 3.8727272727272726,
      "grad_norm": 0.04296249523758888,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 3195
    },
    {
      "epoch": 3.878787878787879,
      "grad_norm": 0.039399437606334686,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 3200
    },
    {
      "epoch": 3.8848484848484848,
      "grad_norm": 0.05838954076170921,
      "learning_rate": 5e-05,
      "loss": 0.0272,
      "step": 3205
    },
    {
      "epoch": 3.890909090909091,
      "grad_norm": 0.05072599649429321,
      "learning_rate": 5e-05,
      "loss": 0.0283,
      "step": 3210
    },
    {
      "epoch": 3.896969696969697,
      "grad_norm": 0.034792132675647736,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 3215
    },
    {
      "epoch": 3.903030303030303,
      "grad_norm": 0.04738497734069824,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 3220
    },
    {
      "epoch": 3.909090909090909,
      "grad_norm": 0.032784171402454376,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 3225
    },
    {
      "epoch": 3.915151515151515,
      "grad_norm": 0.046856239438056946,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 3230
    },
    {
      "epoch": 3.9212121212121214,
      "grad_norm": 0.05000331997871399,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 3235
    },
    {
      "epoch": 3.9272727272727272,
      "grad_norm": 0.048305075615644455,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 3240
    },
    {
      "epoch": 3.933333333333333,
      "grad_norm": 0.05900001898407936,
      "learning_rate": 5e-05,
      "loss": 0.0276,
      "step": 3245
    },
    {
      "epoch": 3.9393939393939394,
      "grad_norm": 0.03971203789114952,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 3250
    },
    {
      "epoch": 3.9454545454545453,
      "grad_norm": 0.05721418187022209,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 3255
    },
    {
      "epoch": 3.9515151515151516,
      "grad_norm": 0.03401621803641319,
      "learning_rate": 5e-05,
      "loss": 0.0259,
      "step": 3260
    },
    {
      "epoch": 3.9575757575757575,
      "grad_norm": 0.0517246313393116,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 3265
    },
    {
      "epoch": 3.963636363636364,
      "grad_norm": 0.06799126416444778,
      "learning_rate": 5e-05,
      "loss": 0.0266,
      "step": 3270
    },
    {
      "epoch": 3.9696969696969697,
      "grad_norm": 0.051690053194761276,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 3275
    },
    {
      "epoch": 3.9757575757575756,
      "grad_norm": 0.04346253722906113,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 3280
    },
    {
      "epoch": 3.981818181818182,
      "grad_norm": 0.035644013434648514,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 3285
    },
    {
      "epoch": 3.987878787878788,
      "grad_norm": 0.06812318414449692,
      "learning_rate": 5e-05,
      "loss": 0.0278,
      "step": 3290
    },
    {
      "epoch": 3.993939393939394,
      "grad_norm": 0.03542844578623772,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 3295
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.03634639084339142,
      "learning_rate": 5e-05,
      "loss": 0.0266,
      "step": 3300
    },
    {
      "epoch": 4.0,
      "eval_average": 0.6263262649063251,
      "eval_crossner_ai": 0.6046010064201612,
      "eval_crossner_literature": 0.5766768716450805,
      "eval_crossner_music": 0.7549721558767808,
      "eval_crossner_politics": 0.6197051977777545,
      "eval_crossner_science": 0.7100591715476461,
      "eval_mit-movie": 0.6286412710850215,
      "eval_mit-restaurant": 0.4896281799918313,
      "eval_runtime": 605.281,
      "eval_samples_per_second": 10.053,
      "eval_steps_per_second": 0.159,
      "step": 3300
    },
    {
      "epoch": 4.006060606060606,
      "grad_norm": 0.03810608386993408,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 3305
    },
    {
      "epoch": 4.012121212121212,
      "grad_norm": 0.03934602811932564,
      "learning_rate": 5e-05,
      "loss": 0.0267,
      "step": 3310
    },
    {
      "epoch": 4.0181818181818185,
      "grad_norm": 0.03239506855607033,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 3315
    },
    {
      "epoch": 4.024242424242424,
      "grad_norm": 0.03455152362585068,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 3320
    },
    {
      "epoch": 4.03030303030303,
      "grad_norm": 0.05954733118414879,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 3325
    },
    {
      "epoch": 4.036363636363636,
      "grad_norm": 0.03334681689739227,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 3330
    },
    {
      "epoch": 4.042424242424242,
      "grad_norm": 0.043733689934015274,
      "learning_rate": 5e-05,
      "loss": 0.0267,
      "step": 3335
    },
    {
      "epoch": 4.048484848484849,
      "grad_norm": 0.041138336062431335,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 3340
    },
    {
      "epoch": 4.054545454545455,
      "grad_norm": 0.05041959136724472,
      "learning_rate": 5e-05,
      "loss": 0.0263,
      "step": 3345
    },
    {
      "epoch": 4.0606060606060606,
      "grad_norm": 0.053458426147699356,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 3350
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 0.03786378353834152,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 3355
    },
    {
      "epoch": 4.072727272727272,
      "grad_norm": 0.05564917251467705,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 3360
    },
    {
      "epoch": 4.078787878787879,
      "grad_norm": 0.034660205245018005,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 3365
    },
    {
      "epoch": 4.084848484848485,
      "grad_norm": 0.04120485112071037,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 3370
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 0.05298149585723877,
      "learning_rate": 5e-05,
      "loss": 0.0254,
      "step": 3375
    },
    {
      "epoch": 4.096969696969697,
      "grad_norm": 0.03281589597463608,
      "learning_rate": 5e-05,
      "loss": 0.026,
      "step": 3380
    },
    {
      "epoch": 4.1030303030303035,
      "grad_norm": 0.03626403585076332,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 3385
    },
    {
      "epoch": 4.109090909090909,
      "grad_norm": 0.05014646425843239,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 3390
    },
    {
      "epoch": 4.115151515151515,
      "grad_norm": 0.039391010999679565,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 3395
    },
    {
      "epoch": 4.121212121212121,
      "grad_norm": 0.7047333121299744,
      "learning_rate": 5e-05,
      "loss": 0.0252,
      "step": 3400
    },
    {
      "epoch": 4.127272727272727,
      "grad_norm": 0.04334111884236336,
      "learning_rate": 5e-05,
      "loss": 0.0267,
      "step": 3405
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 0.039480168372392654,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 3410
    },
    {
      "epoch": 4.13939393939394,
      "grad_norm": 0.03894781321287155,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 3415
    },
    {
      "epoch": 4.1454545454545455,
      "grad_norm": 0.038656268268823624,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 3420
    },
    {
      "epoch": 4.151515151515151,
      "grad_norm": 0.03274125978350639,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 3425
    },
    {
      "epoch": 4.157575757575757,
      "grad_norm": 0.04409566521644592,
      "learning_rate": 5e-05,
      "loss": 0.0266,
      "step": 3430
    },
    {
      "epoch": 4.163636363636364,
      "grad_norm": 0.05420565605163574,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 3435
    },
    {
      "epoch": 4.16969696969697,
      "grad_norm": 0.047980714589357376,
      "learning_rate": 5e-05,
      "loss": 0.026,
      "step": 3440
    },
    {
      "epoch": 4.175757575757576,
      "grad_norm": 0.04455847665667534,
      "learning_rate": 5e-05,
      "loss": 0.0252,
      "step": 3445
    },
    {
      "epoch": 4.181818181818182,
      "grad_norm": 0.040039997547864914,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 3450
    },
    {
      "epoch": 4.1878787878787875,
      "grad_norm": 0.03567077964544296,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 3455
    },
    {
      "epoch": 4.193939393939394,
      "grad_norm": 0.037781599909067154,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 3460
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.05766141787171364,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 3465
    },
    {
      "epoch": 4.206060606060606,
      "grad_norm": 0.04031475633382797,
      "learning_rate": 5e-05,
      "loss": 0.0274,
      "step": 3470
    },
    {
      "epoch": 4.212121212121212,
      "grad_norm": 0.03854406252503395,
      "learning_rate": 5e-05,
      "loss": 0.0292,
      "step": 3475
    },
    {
      "epoch": 4.218181818181818,
      "grad_norm": 0.03543729707598686,
      "learning_rate": 5e-05,
      "loss": 0.026,
      "step": 3480
    },
    {
      "epoch": 4.224242424242425,
      "grad_norm": 0.03834791108965874,
      "learning_rate": 5e-05,
      "loss": 0.025,
      "step": 3485
    },
    {
      "epoch": 4.2303030303030305,
      "grad_norm": 0.04208280146121979,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 3490
    },
    {
      "epoch": 4.236363636363636,
      "grad_norm": 0.04713437333703041,
      "learning_rate": 5e-05,
      "loss": 0.0265,
      "step": 3495
    },
    {
      "epoch": 4.242424242424242,
      "grad_norm": 0.0379447340965271,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 3500
    },
    {
      "epoch": 4.248484848484848,
      "grad_norm": 0.03953339904546738,
      "learning_rate": 5e-05,
      "loss": 0.0265,
      "step": 3505
    },
    {
      "epoch": 4.254545454545455,
      "grad_norm": 0.04397277161478996,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 3510
    },
    {
      "epoch": 4.260606060606061,
      "grad_norm": 0.05002659931778908,
      "learning_rate": 5e-05,
      "loss": 0.0263,
      "step": 3515
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.037922460585832596,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 3520
    },
    {
      "epoch": 4.2727272727272725,
      "grad_norm": 0.04622384160757065,
      "learning_rate": 5e-05,
      "loss": 0.026,
      "step": 3525
    },
    {
      "epoch": 4.278787878787878,
      "grad_norm": 0.04811681807041168,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 3530
    },
    {
      "epoch": 4.284848484848485,
      "grad_norm": 0.03967113792896271,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 3535
    },
    {
      "epoch": 4.290909090909091,
      "grad_norm": 0.05361161008477211,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 3540
    },
    {
      "epoch": 4.296969696969697,
      "grad_norm": 0.03665781766176224,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 3545
    },
    {
      "epoch": 4.303030303030303,
      "grad_norm": 0.05417073518037796,
      "learning_rate": 5e-05,
      "loss": 0.0244,
      "step": 3550
    },
    {
      "epoch": 4.3090909090909095,
      "grad_norm": 0.06050070747733116,
      "learning_rate": 5e-05,
      "loss": 0.0277,
      "step": 3555
    },
    {
      "epoch": 4.315151515151515,
      "grad_norm": 0.034327346831560135,
      "learning_rate": 5e-05,
      "loss": 0.0266,
      "step": 3560
    },
    {
      "epoch": 4.321212121212121,
      "grad_norm": 0.07929076254367828,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 3565
    },
    {
      "epoch": 4.327272727272727,
      "grad_norm": 0.03803977742791176,
      "learning_rate": 5e-05,
      "loss": 0.0259,
      "step": 3570
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.042077142745256424,
      "learning_rate": 5e-05,
      "loss": 0.0267,
      "step": 3575
    },
    {
      "epoch": 4.33939393939394,
      "grad_norm": 0.03524099662899971,
      "learning_rate": 5e-05,
      "loss": 0.0263,
      "step": 3580
    },
    {
      "epoch": 4.345454545454546,
      "grad_norm": 0.042601291090250015,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 3585
    },
    {
      "epoch": 4.351515151515152,
      "grad_norm": 0.05168283358216286,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 3590
    },
    {
      "epoch": 4.357575757575757,
      "grad_norm": 0.04463369399309158,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 3595
    },
    {
      "epoch": 4.363636363636363,
      "grad_norm": 0.049201373010873795,
      "learning_rate": 5e-05,
      "loss": 0.0259,
      "step": 3600
    },
    {
      "epoch": 4.36969696969697,
      "grad_norm": 0.04212230443954468,
      "learning_rate": 5e-05,
      "loss": 0.0254,
      "step": 3605
    },
    {
      "epoch": 4.375757575757576,
      "grad_norm": 0.05787612497806549,
      "learning_rate": 5e-05,
      "loss": 0.0255,
      "step": 3610
    },
    {
      "epoch": 4.381818181818182,
      "grad_norm": 0.060874201357364655,
      "learning_rate": 5e-05,
      "loss": 0.0263,
      "step": 3615
    },
    {
      "epoch": 4.387878787878788,
      "grad_norm": 0.04765818640589714,
      "learning_rate": 5e-05,
      "loss": 0.0267,
      "step": 3620
    },
    {
      "epoch": 4.393939393939394,
      "grad_norm": 0.03872821107506752,
      "learning_rate": 5e-05,
      "loss": 0.0266,
      "step": 3625
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.052176907658576965,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 3630
    },
    {
      "epoch": 4.406060606060606,
      "grad_norm": 0.036805566400289536,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 3635
    },
    {
      "epoch": 4.412121212121212,
      "grad_norm": 0.03934288024902344,
      "learning_rate": 5e-05,
      "loss": 0.025,
      "step": 3640
    },
    {
      "epoch": 4.418181818181818,
      "grad_norm": 0.044915154576301575,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 3645
    },
    {
      "epoch": 4.424242424242424,
      "grad_norm": 0.0368279404938221,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 3650
    },
    {
      "epoch": 4.430303030303031,
      "grad_norm": 0.06251546740531921,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 3655
    },
    {
      "epoch": 4.4363636363636365,
      "grad_norm": 0.040247540920972824,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 3660
    },
    {
      "epoch": 4.442424242424242,
      "grad_norm": 0.043563444167375565,
      "learning_rate": 5e-05,
      "loss": 0.0265,
      "step": 3665
    },
    {
      "epoch": 4.448484848484848,
      "grad_norm": 0.03478359058499336,
      "learning_rate": 5e-05,
      "loss": 0.0255,
      "step": 3670
    },
    {
      "epoch": 4.454545454545454,
      "grad_norm": 0.03507029265165329,
      "learning_rate": 5e-05,
      "loss": 0.0252,
      "step": 3675
    },
    {
      "epoch": 4.460606060606061,
      "grad_norm": 0.05672943964600563,
      "learning_rate": 5e-05,
      "loss": 0.0255,
      "step": 3680
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 0.03214805945754051,
      "learning_rate": 5e-05,
      "loss": 0.0259,
      "step": 3685
    },
    {
      "epoch": 4.472727272727273,
      "grad_norm": 0.051666438579559326,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 3690
    },
    {
      "epoch": 4.4787878787878785,
      "grad_norm": 0.05610233172774315,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 3695
    },
    {
      "epoch": 4.484848484848484,
      "grad_norm": 0.040486544370651245,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 3700
    },
    {
      "epoch": 4.490909090909091,
      "grad_norm": 0.040968459099531174,
      "learning_rate": 5e-05,
      "loss": 0.0286,
      "step": 3705
    },
    {
      "epoch": 4.496969696969697,
      "grad_norm": 0.09052527695894241,
      "learning_rate": 5e-05,
      "loss": 0.0263,
      "step": 3710
    },
    {
      "epoch": 4.499393939393939,
      "eval_average": 0.6367236217895433,
      "eval_crossner_ai": 0.5939524837512885,
      "eval_crossner_literature": 0.6028153463480856,
      "eval_crossner_music": 0.7919409534708169,
      "eval_crossner_politics": 0.6621371216300294,
      "eval_crossner_science": 0.7086649988128759,
      "eval_mit-movie": 0.61045394798387,
      "eval_mit-restaurant": 0.4871005005298379,
      "eval_runtime": 613.8663,
      "eval_samples_per_second": 9.913,
      "eval_steps_per_second": 0.156,
      "step": 3712
    },
    {
      "epoch": 4.503030303030303,
      "grad_norm": 0.04912066459655762,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 3715
    },
    {
      "epoch": 4.509090909090909,
      "grad_norm": 0.05733504518866539,
      "learning_rate": 5e-05,
      "loss": 0.0265,
      "step": 3720
    },
    {
      "epoch": 4.515151515151516,
      "grad_norm": 0.05564863979816437,
      "learning_rate": 5e-05,
      "loss": 0.0272,
      "step": 3725
    },
    {
      "epoch": 4.5212121212121215,
      "grad_norm": 0.05030566081404686,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 3730
    },
    {
      "epoch": 4.527272727272727,
      "grad_norm": 0.037260375916957855,
      "learning_rate": 5e-05,
      "loss": 0.0267,
      "step": 3735
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.04261510819196701,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 3740
    },
    {
      "epoch": 4.539393939393939,
      "grad_norm": 0.039714496582746506,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 3745
    },
    {
      "epoch": 4.545454545454546,
      "grad_norm": 0.051546353846788406,
      "learning_rate": 5e-05,
      "loss": 0.0244,
      "step": 3750
    },
    {
      "epoch": 4.551515151515152,
      "grad_norm": 0.050556790083646774,
      "learning_rate": 5e-05,
      "loss": 0.0263,
      "step": 3755
    },
    {
      "epoch": 4.557575757575758,
      "grad_norm": 0.03775591030716896,
      "learning_rate": 5e-05,
      "loss": 0.026,
      "step": 3760
    },
    {
      "epoch": 4.5636363636363635,
      "grad_norm": 0.04762694984674454,
      "learning_rate": 5e-05,
      "loss": 0.0272,
      "step": 3765
    },
    {
      "epoch": 4.569696969696969,
      "grad_norm": 0.04274764284491539,
      "learning_rate": 5e-05,
      "loss": 0.0266,
      "step": 3770
    },
    {
      "epoch": 4.575757575757576,
      "grad_norm": 0.04587399214506149,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 3775
    },
    {
      "epoch": 4.581818181818182,
      "grad_norm": 0.04873040318489075,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 3780
    },
    {
      "epoch": 4.587878787878788,
      "grad_norm": 0.03724747896194458,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 3785
    },
    {
      "epoch": 4.593939393939394,
      "grad_norm": 0.040666185319423676,
      "learning_rate": 5e-05,
      "loss": 0.0273,
      "step": 3790
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.08387401700019836,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 3795
    },
    {
      "epoch": 4.606060606060606,
      "grad_norm": 0.03879247233271599,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 3800
    },
    {
      "epoch": 4.612121212121212,
      "grad_norm": 0.04572800546884537,
      "learning_rate": 5e-05,
      "loss": 0.0263,
      "step": 3805
    },
    {
      "epoch": 4.618181818181818,
      "grad_norm": 0.036782484501600266,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 3810
    },
    {
      "epoch": 4.624242424242424,
      "grad_norm": 0.06451129168272018,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 3815
    },
    {
      "epoch": 4.63030303030303,
      "grad_norm": 0.043258167803287506,
      "learning_rate": 5e-05,
      "loss": 0.0266,
      "step": 3820
    },
    {
      "epoch": 4.636363636363637,
      "grad_norm": 0.12166634202003479,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 3825
    },
    {
      "epoch": 4.642424242424243,
      "grad_norm": 0.05121571570634842,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 3830
    },
    {
      "epoch": 4.648484848484848,
      "grad_norm": 0.06830551475286484,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 3835
    },
    {
      "epoch": 4.654545454545454,
      "grad_norm": 0.038497444242239,
      "learning_rate": 5e-05,
      "loss": 0.0259,
      "step": 3840
    },
    {
      "epoch": 4.66060606060606,
      "grad_norm": 0.0776275023818016,
      "learning_rate": 5e-05,
      "loss": 0.0252,
      "step": 3845
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.03657933324575424,
      "learning_rate": 5e-05,
      "loss": 0.0259,
      "step": 3850
    },
    {
      "epoch": 4.672727272727273,
      "grad_norm": 0.044915322214365005,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 3855
    },
    {
      "epoch": 4.678787878787879,
      "grad_norm": 0.04071296006441116,
      "learning_rate": 5e-05,
      "loss": 0.0252,
      "step": 3860
    },
    {
      "epoch": 4.684848484848485,
      "grad_norm": 0.0617741122841835,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 3865
    },
    {
      "epoch": 4.6909090909090905,
      "grad_norm": 0.051733653992414474,
      "learning_rate": 5e-05,
      "loss": 0.0255,
      "step": 3870
    },
    {
      "epoch": 4.696969696969697,
      "grad_norm": 0.05518839880824089,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 3875
    },
    {
      "epoch": 4.703030303030303,
      "grad_norm": 0.03842488303780556,
      "learning_rate": 5e-05,
      "loss": 0.0254,
      "step": 3880
    },
    {
      "epoch": 4.709090909090909,
      "grad_norm": 0.11912485212087631,
      "learning_rate": 5e-05,
      "loss": 0.0265,
      "step": 3885
    },
    {
      "epoch": 4.715151515151515,
      "grad_norm": 0.03343971073627472,
      "learning_rate": 5e-05,
      "loss": 0.0249,
      "step": 3890
    },
    {
      "epoch": 4.721212121212122,
      "grad_norm": 0.06125941872596741,
      "learning_rate": 5e-05,
      "loss": 0.0279,
      "step": 3895
    },
    {
      "epoch": 4.7272727272727275,
      "grad_norm": 0.03640797361731529,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 3900
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 0.03359464555978775,
      "learning_rate": 5e-05,
      "loss": 0.0254,
      "step": 3905
    },
    {
      "epoch": 4.739393939393939,
      "grad_norm": 0.04606229439377785,
      "learning_rate": 5e-05,
      "loss": 0.0269,
      "step": 3910
    },
    {
      "epoch": 4.745454545454545,
      "grad_norm": 0.04379754140973091,
      "learning_rate": 5e-05,
      "loss": 0.026,
      "step": 3915
    },
    {
      "epoch": 4.751515151515152,
      "grad_norm": 0.04866625368595123,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 3920
    },
    {
      "epoch": 4.757575757575758,
      "grad_norm": 0.047014471143484116,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 3925
    },
    {
      "epoch": 4.763636363636364,
      "grad_norm": 0.0567447692155838,
      "learning_rate": 5e-05,
      "loss": 0.0255,
      "step": 3930
    },
    {
      "epoch": 4.7696969696969695,
      "grad_norm": 0.03922777995467186,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 3935
    },
    {
      "epoch": 4.775757575757575,
      "grad_norm": 0.041812650859355927,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 3940
    },
    {
      "epoch": 4.781818181818182,
      "grad_norm": 0.03326275572180748,
      "learning_rate": 5e-05,
      "loss": 0.0275,
      "step": 3945
    },
    {
      "epoch": 4.787878787878788,
      "grad_norm": 0.04007016867399216,
      "learning_rate": 5e-05,
      "loss": 0.025,
      "step": 3950
    },
    {
      "epoch": 4.793939393939394,
      "grad_norm": 0.04889184236526489,
      "learning_rate": 5e-05,
      "loss": 0.026,
      "step": 3955
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.04559795558452606,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 3960
    },
    {
      "epoch": 4.806060606060606,
      "grad_norm": 0.039132941514253616,
      "learning_rate": 5e-05,
      "loss": 0.0263,
      "step": 3965
    },
    {
      "epoch": 4.8121212121212125,
      "grad_norm": 0.04972178488969803,
      "learning_rate": 5e-05,
      "loss": 0.0253,
      "step": 3970
    },
    {
      "epoch": 4.818181818181818,
      "grad_norm": 0.044501155614852905,
      "learning_rate": 5e-05,
      "loss": 0.0267,
      "step": 3975
    },
    {
      "epoch": 4.824242424242424,
      "grad_norm": 0.04731348901987076,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 3980
    },
    {
      "epoch": 4.83030303030303,
      "grad_norm": 0.04570474103093147,
      "learning_rate": 5e-05,
      "loss": 0.0263,
      "step": 3985
    },
    {
      "epoch": 4.836363636363636,
      "grad_norm": 0.0648391917347908,
      "learning_rate": 5e-05,
      "loss": 0.026,
      "step": 3990
    },
    {
      "epoch": 4.842424242424243,
      "grad_norm": 0.04503064230084419,
      "learning_rate": 5e-05,
      "loss": 0.0252,
      "step": 3995
    },
    {
      "epoch": 4.848484848484849,
      "grad_norm": 0.03704683110117912,
      "learning_rate": 5e-05,
      "loss": 0.026,
      "step": 4000
    },
    {
      "epoch": 4.8545454545454545,
      "grad_norm": 0.03941870108246803,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 4005
    },
    {
      "epoch": 4.86060606060606,
      "grad_norm": 0.05591484531760216,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 4010
    },
    {
      "epoch": 4.866666666666666,
      "grad_norm": 0.03892013058066368,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 4015
    },
    {
      "epoch": 4.872727272727273,
      "grad_norm": 0.05925643444061279,
      "learning_rate": 5e-05,
      "loss": 0.0266,
      "step": 4020
    },
    {
      "epoch": 4.878787878787879,
      "grad_norm": 0.07979008555412292,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 4025
    },
    {
      "epoch": 4.884848484848485,
      "grad_norm": 0.03981857746839523,
      "learning_rate": 5e-05,
      "loss": 0.0244,
      "step": 4030
    },
    {
      "epoch": 4.890909090909091,
      "grad_norm": 0.036082230508327484,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 4035
    },
    {
      "epoch": 4.8969696969696965,
      "grad_norm": 0.049530960619449615,
      "learning_rate": 5e-05,
      "loss": 0.0266,
      "step": 4040
    },
    {
      "epoch": 4.903030303030303,
      "grad_norm": 0.04193180054426193,
      "learning_rate": 5e-05,
      "loss": 0.0252,
      "step": 4045
    },
    {
      "epoch": 4.909090909090909,
      "grad_norm": 0.05061601474881172,
      "learning_rate": 5e-05,
      "loss": 0.028,
      "step": 4050
    },
    {
      "epoch": 4.915151515151515,
      "grad_norm": 0.03596682474017143,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 4055
    },
    {
      "epoch": 4.921212121212121,
      "grad_norm": 0.04152793064713478,
      "learning_rate": 5e-05,
      "loss": 0.0249,
      "step": 4060
    },
    {
      "epoch": 4.927272727272728,
      "grad_norm": 0.03664882853627205,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 4065
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 0.03643488883972168,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 4070
    },
    {
      "epoch": 4.9393939393939394,
      "grad_norm": 0.03682177513837814,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 4075
    },
    {
      "epoch": 4.945454545454545,
      "grad_norm": 0.04672197252511978,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 4080
    },
    {
      "epoch": 4.951515151515151,
      "grad_norm": 0.037280093878507614,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 4085
    },
    {
      "epoch": 4.957575757575758,
      "grad_norm": 0.031921010464429855,
      "learning_rate": 5e-05,
      "loss": 0.0263,
      "step": 4090
    },
    {
      "epoch": 4.963636363636364,
      "grad_norm": 0.028620708733797073,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 4095
    },
    {
      "epoch": 4.96969696969697,
      "grad_norm": 0.06813168525695801,
      "learning_rate": 5e-05,
      "loss": 0.0266,
      "step": 4100
    },
    {
      "epoch": 4.975757575757576,
      "grad_norm": 0.060750529170036316,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 4105
    },
    {
      "epoch": 4.9818181818181815,
      "grad_norm": 0.03731861710548401,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 4110
    },
    {
      "epoch": 4.987878787878788,
      "grad_norm": 0.035364486277103424,
      "learning_rate": 5e-05,
      "loss": 0.026,
      "step": 4115
    },
    {
      "epoch": 4.993939393939394,
      "grad_norm": 0.04607643559575081,
      "learning_rate": 5e-05,
      "loss": 0.0289,
      "step": 4120
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.04214770346879959,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 4125
    },
    {
      "epoch": 5.0,
      "eval_average": 0.6418643799271629,
      "eval_crossner_ai": 0.622336517216672,
      "eval_crossner_literature": 0.5989932885407496,
      "eval_crossner_music": 0.7802419354338559,
      "eval_crossner_politics": 0.6550123761876036,
      "eval_crossner_science": 0.716693604198421,
      "eval_mit-movie": 0.6265011194292965,
      "eval_mit-restaurant": 0.4932718184835427,
      "eval_runtime": 604.7169,
      "eval_samples_per_second": 10.063,
      "eval_steps_per_second": 0.159,
      "step": 4125
    },
    {
      "epoch": 5.006060606060606,
      "grad_norm": 0.036722324788570404,
      "learning_rate": 5e-05,
      "loss": 0.0267,
      "step": 4130
    },
    {
      "epoch": 5.012121212121212,
      "grad_norm": 0.043384358286857605,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 4135
    },
    {
      "epoch": 5.0181818181818185,
      "grad_norm": 0.03619667887687683,
      "learning_rate": 5e-05,
      "loss": 0.0255,
      "step": 4140
    },
    {
      "epoch": 5.024242424242424,
      "grad_norm": 0.05281084403395653,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 4145
    },
    {
      "epoch": 5.03030303030303,
      "grad_norm": 0.06380736082792282,
      "learning_rate": 5e-05,
      "loss": 0.0241,
      "step": 4150
    },
    {
      "epoch": 5.036363636363636,
      "grad_norm": 0.03922020643949509,
      "learning_rate": 5e-05,
      "loss": 0.0252,
      "step": 4155
    },
    {
      "epoch": 5.042424242424242,
      "grad_norm": 0.0809616819024086,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 4160
    },
    {
      "epoch": 5.048484848484849,
      "grad_norm": 0.032731667160987854,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 4165
    },
    {
      "epoch": 5.054545454545455,
      "grad_norm": 0.04991290718317032,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 4170
    },
    {
      "epoch": 5.0606060606060606,
      "grad_norm": 0.046229805797338486,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 4175
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 0.03525558486580849,
      "learning_rate": 5e-05,
      "loss": 0.0255,
      "step": 4180
    },
    {
      "epoch": 5.072727272727272,
      "grad_norm": 0.039175160229206085,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 4185
    },
    {
      "epoch": 5.078787878787879,
      "grad_norm": 0.03374418616294861,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 4190
    },
    {
      "epoch": 5.084848484848485,
      "grad_norm": 0.05939358100295067,
      "learning_rate": 5e-05,
      "loss": 0.024,
      "step": 4195
    },
    {
      "epoch": 5.090909090909091,
      "grad_norm": 0.042834267020225525,
      "learning_rate": 5e-05,
      "loss": 0.0261,
      "step": 4200
    },
    {
      "epoch": 5.096969696969697,
      "grad_norm": 0.053791433572769165,
      "learning_rate": 5e-05,
      "loss": 0.0253,
      "step": 4205
    },
    {
      "epoch": 5.1030303030303035,
      "grad_norm": 0.04389406368136406,
      "learning_rate": 5e-05,
      "loss": 0.0243,
      "step": 4210
    },
    {
      "epoch": 5.109090909090909,
      "grad_norm": 0.04955391213297844,
      "learning_rate": 5e-05,
      "loss": 0.0255,
      "step": 4215
    },
    {
      "epoch": 5.115151515151515,
      "grad_norm": 0.044726498425006866,
      "learning_rate": 5e-05,
      "loss": 0.0254,
      "step": 4220
    },
    {
      "epoch": 5.121212121212121,
      "grad_norm": 0.03727960214018822,
      "learning_rate": 5e-05,
      "loss": 0.0237,
      "step": 4225
    },
    {
      "epoch": 5.127272727272727,
      "grad_norm": 0.041343316435813904,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 4230
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 0.036392901092767715,
      "learning_rate": 5e-05,
      "loss": 0.025,
      "step": 4235
    },
    {
      "epoch": 5.13939393939394,
      "grad_norm": 0.033059459179639816,
      "learning_rate": 5e-05,
      "loss": 0.0254,
      "step": 4240
    },
    {
      "epoch": 5.1454545454545455,
      "grad_norm": 0.036590296775102615,
      "learning_rate": 5e-05,
      "loss": 0.0239,
      "step": 4245
    },
    {
      "epoch": 5.151515151515151,
      "grad_norm": 0.038959257304668427,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 4250
    },
    {
      "epoch": 5.157575757575757,
      "grad_norm": 0.683249831199646,
      "learning_rate": 5e-05,
      "loss": 0.0264,
      "step": 4255
    },
    {
      "epoch": 5.163636363636364,
      "grad_norm": 0.051935289055109024,
      "learning_rate": 5e-05,
      "loss": 0.0268,
      "step": 4260
    },
    {
      "epoch": 5.16969696969697,
      "grad_norm": 0.0342310331761837,
      "learning_rate": 5e-05,
      "loss": 0.0259,
      "step": 4265
    },
    {
      "epoch": 5.175757575757576,
      "grad_norm": 0.04303242266178131,
      "learning_rate": 5e-05,
      "loss": 0.0243,
      "step": 4270
    },
    {
      "epoch": 5.181818181818182,
      "grad_norm": 0.04987228289246559,
      "learning_rate": 5e-05,
      "loss": 0.0266,
      "step": 4275
    },
    {
      "epoch": 5.1878787878787875,
      "grad_norm": 0.04984753206372261,
      "learning_rate": 5e-05,
      "loss": 0.0252,
      "step": 4280
    },
    {
      "epoch": 5.193939393939394,
      "grad_norm": 0.0453135184943676,
      "learning_rate": 5e-05,
      "loss": 0.0271,
      "step": 4285
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.049315765500068665,
      "learning_rate": 5e-05,
      "loss": 0.0249,
      "step": 4290
    },
    {
      "epoch": 5.206060606060606,
      "grad_norm": 0.033086538314819336,
      "learning_rate": 5e-05,
      "loss": 0.025,
      "step": 4295
    },
    {
      "epoch": 5.212121212121212,
      "grad_norm": 0.04844564571976662,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 4300
    },
    {
      "epoch": 5.218181818181818,
      "grad_norm": 0.04696636646986008,
      "learning_rate": 5e-05,
      "loss": 0.0246,
      "step": 4305
    },
    {
      "epoch": 5.224242424242425,
      "grad_norm": 0.04978501424193382,
      "learning_rate": 5e-05,
      "loss": 0.0254,
      "step": 4310
    },
    {
      "epoch": 5.2303030303030305,
      "grad_norm": 0.04200909659266472,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 4315
    },
    {
      "epoch": 5.236363636363636,
      "grad_norm": 0.050466492772102356,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 4320
    },
    {
      "epoch": 5.242424242424242,
      "grad_norm": 0.05156884342432022,
      "learning_rate": 5e-05,
      "loss": 0.0246,
      "step": 4325
    },
    {
      "epoch": 5.248484848484848,
      "grad_norm": 0.08291295170783997,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 4330
    },
    {
      "epoch": 5.254545454545455,
      "grad_norm": 0.052472081035375595,
      "learning_rate": 5e-05,
      "loss": 0.0246,
      "step": 4335
    },
    {
      "epoch": 5.260606060606061,
      "grad_norm": 0.029644431546330452,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 4340
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 0.04739336296916008,
      "learning_rate": 5e-05,
      "loss": 0.0258,
      "step": 4345
    },
    {
      "epoch": 5.2727272727272725,
      "grad_norm": 0.031260523945093155,
      "learning_rate": 5e-05,
      "loss": 0.025,
      "step": 4350
    },
    {
      "epoch": 5.278787878787878,
      "grad_norm": 0.04418927803635597,
      "learning_rate": 5e-05,
      "loss": 0.0253,
      "step": 4355
    },
    {
      "epoch": 5.284848484848485,
      "grad_norm": 0.038507092744112015,
      "learning_rate": 5e-05,
      "loss": 0.0246,
      "step": 4360
    },
    {
      "epoch": 5.290909090909091,
      "grad_norm": 0.03852144628763199,
      "learning_rate": 5e-05,
      "loss": 0.0255,
      "step": 4365
    },
    {
      "epoch": 5.296969696969697,
      "grad_norm": 0.03913373500108719,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 4370
    },
    {
      "epoch": 5.303030303030303,
      "grad_norm": 0.04548708349466324,
      "learning_rate": 5e-05,
      "loss": 0.0246,
      "step": 4375
    },
    {
      "epoch": 5.3090909090909095,
      "grad_norm": 0.035120293498039246,
      "learning_rate": 5e-05,
      "loss": 0.0242,
      "step": 4380
    },
    {
      "epoch": 5.315151515151515,
      "grad_norm": 0.03680972009897232,
      "learning_rate": 5e-05,
      "loss": 0.0254,
      "step": 4385
    },
    {
      "epoch": 5.321212121212121,
      "grad_norm": 0.03995387628674507,
      "learning_rate": 5e-05,
      "loss": 0.0252,
      "step": 4390
    },
    {
      "epoch": 5.327272727272727,
      "grad_norm": 0.037195365875959396,
      "learning_rate": 5e-05,
      "loss": 0.0242,
      "step": 4395
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.040989432483911514,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 4400
    },
    {
      "epoch": 5.33939393939394,
      "grad_norm": 0.035656239837408066,
      "learning_rate": 5e-05,
      "loss": 0.0246,
      "step": 4405
    },
    {
      "epoch": 5.345454545454546,
      "grad_norm": 0.030128145590424538,
      "learning_rate": 5e-05,
      "loss": 0.0236,
      "step": 4410
    },
    {
      "epoch": 5.351515151515152,
      "grad_norm": 0.06130426749587059,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 4415
    },
    {
      "epoch": 5.357575757575757,
      "grad_norm": 0.04279307276010513,
      "learning_rate": 5e-05,
      "loss": 0.0253,
      "step": 4420
    },
    {
      "epoch": 5.363636363636363,
      "grad_norm": 0.03422312065958977,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 4425
    },
    {
      "epoch": 5.36969696969697,
      "grad_norm": 0.03441784530878067,
      "learning_rate": 5e-05,
      "loss": 0.026,
      "step": 4430
    },
    {
      "epoch": 5.375757575757576,
      "grad_norm": 0.13084770739078522,
      "learning_rate": 5e-05,
      "loss": 0.0244,
      "step": 4435
    },
    {
      "epoch": 5.381818181818182,
      "grad_norm": 0.037005715072155,
      "learning_rate": 5e-05,
      "loss": 0.0249,
      "step": 4440
    },
    {
      "epoch": 5.387878787878788,
      "grad_norm": 1.3544213771820068,
      "learning_rate": 5e-05,
      "loss": 0.0262,
      "step": 4445
    },
    {
      "epoch": 5.393939393939394,
      "grad_norm": 0.03465539216995239,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 4450
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.054189786314964294,
      "learning_rate": 5e-05,
      "loss": 0.0244,
      "step": 4455
    },
    {
      "epoch": 5.406060606060606,
      "grad_norm": 0.04427013173699379,
      "learning_rate": 5e-05,
      "loss": 0.0237,
      "step": 4460
    },
    {
      "epoch": 5.412121212121212,
      "grad_norm": 0.04099365323781967,
      "learning_rate": 5e-05,
      "loss": 0.0238,
      "step": 4465
    },
    {
      "epoch": 5.418181818181818,
      "grad_norm": 0.05101871117949486,
      "learning_rate": 5e-05,
      "loss": 0.0256,
      "step": 4470
    },
    {
      "epoch": 5.424242424242424,
      "grad_norm": 0.030444689095020294,
      "learning_rate": 5e-05,
      "loss": 0.024,
      "step": 4475
    },
    {
      "epoch": 5.430303030303031,
      "grad_norm": 0.03866606950759888,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 4480
    },
    {
      "epoch": 5.4363636363636365,
      "grad_norm": 0.04088766872882843,
      "learning_rate": 5e-05,
      "loss": 0.025,
      "step": 4485
    },
    {
      "epoch": 5.442424242424242,
      "grad_norm": 0.034151069819927216,
      "learning_rate": 5e-05,
      "loss": 0.024,
      "step": 4490
    },
    {
      "epoch": 5.448484848484848,
      "grad_norm": 0.04824439063668251,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 4495
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 0.037062495946884155,
      "learning_rate": 5e-05,
      "loss": 0.0257,
      "step": 4500
    },
    {
      "epoch": 5.460606060606061,
      "grad_norm": 0.15399344265460968,
      "learning_rate": 5e-05,
      "loss": 0.0249,
      "step": 4505
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 0.03325450420379639,
      "learning_rate": 5e-05,
      "loss": 0.0242,
      "step": 4510
    },
    {
      "epoch": 5.472727272727273,
      "grad_norm": 0.052063267678022385,
      "learning_rate": 5e-05,
      "loss": 0.0239,
      "step": 4515
    },
    {
      "epoch": 5.4787878787878785,
      "grad_norm": 0.06682277470827103,
      "learning_rate": 5e-05,
      "loss": 0.024,
      "step": 4520
    },
    {
      "epoch": 5.484848484848484,
      "grad_norm": 0.04393066093325615,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 4525
    },
    {
      "epoch": 5.490909090909091,
      "grad_norm": 0.04844316467642784,
      "learning_rate": 5e-05,
      "loss": 0.0234,
      "step": 4530
    },
    {
      "epoch": 5.496969696969697,
      "grad_norm": 0.03436494618654251,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 4535
    },
    {
      "epoch": 5.500606060606061,
      "eval_average": 0.6284970534027369,
      "eval_crossner_ai": 0.5906328208295267,
      "eval_crossner_literature": 0.5911357340221136,
      "eval_crossner_music": 0.7757046446502474,
      "eval_crossner_politics": 0.6312384472697601,
      "eval_crossner_science": 0.7123974475343297,
      "eval_mit-movie": 0.6162205674688802,
      "eval_mit-restaurant": 0.4821497120443005,
      "eval_runtime": 615.6297,
      "eval_samples_per_second": 9.884,
      "eval_steps_per_second": 0.156,
      "step": 4538
    },
    {
      "epoch": 5.503030303030303,
      "grad_norm": 0.06637916713953018,
      "learning_rate": 5e-05,
      "loss": 0.027,
      "step": 4540
    },
    {
      "epoch": 5.509090909090909,
      "grad_norm": 0.049372926354408264,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 4545
    },
    {
      "epoch": 5.515151515151516,
      "grad_norm": 0.03298494964838028,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 4550
    },
    {
      "epoch": 5.5212121212121215,
      "grad_norm": 0.04819168522953987,
      "learning_rate": 5e-05,
      "loss": 0.0244,
      "step": 4555
    },
    {
      "epoch": 5.527272727272727,
      "grad_norm": 0.043296001851558685,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 4560
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 0.0397389680147171,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 4565
    },
    {
      "epoch": 5.539393939393939,
      "grad_norm": 0.03963429853320122,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 4570
    },
    {
      "epoch": 5.545454545454546,
      "grad_norm": 0.04214467480778694,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 4575
    },
    {
      "epoch": 5.551515151515152,
      "grad_norm": 0.04000036045908928,
      "learning_rate": 5e-05,
      "loss": 0.0236,
      "step": 4580
    },
    {
      "epoch": 5.557575757575758,
      "grad_norm": 0.03299699351191521,
      "learning_rate": 5e-05,
      "loss": 0.025,
      "step": 4585
    },
    {
      "epoch": 5.5636363636363635,
      "grad_norm": 0.0397377610206604,
      "learning_rate": 5e-05,
      "loss": 0.0234,
      "step": 4590
    },
    {
      "epoch": 5.569696969696969,
      "grad_norm": 0.040206946432590485,
      "learning_rate": 5e-05,
      "loss": 0.0234,
      "step": 4595
    },
    {
      "epoch": 5.575757575757576,
      "grad_norm": 0.030709151178598404,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 4600
    },
    {
      "epoch": 5.581818181818182,
      "grad_norm": 0.04629269987344742,
      "learning_rate": 5e-05,
      "loss": 0.0234,
      "step": 4605
    },
    {
      "epoch": 5.587878787878788,
      "grad_norm": 0.04481707513332367,
      "learning_rate": 5e-05,
      "loss": 0.024,
      "step": 4610
    },
    {
      "epoch": 5.593939393939394,
      "grad_norm": 0.0385398343205452,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 4615
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.03676397725939751,
      "learning_rate": 5e-05,
      "loss": 0.0255,
      "step": 4620
    },
    {
      "epoch": 5.606060606060606,
      "grad_norm": 0.04125125706195831,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 4625
    },
    {
      "epoch": 5.612121212121212,
      "grad_norm": 0.03577420860528946,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 4630
    },
    {
      "epoch": 5.618181818181818,
      "grad_norm": 0.2214587926864624,
      "learning_rate": 5e-05,
      "loss": 0.0254,
      "step": 4635
    },
    {
      "epoch": 5.624242424242424,
      "grad_norm": 0.035042017698287964,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 4640
    },
    {
      "epoch": 5.63030303030303,
      "grad_norm": 0.03330445662140846,
      "learning_rate": 5e-05,
      "loss": 0.0241,
      "step": 4645
    },
    {
      "epoch": 5.636363636363637,
      "grad_norm": 0.04479708522558212,
      "learning_rate": 5e-05,
      "loss": 0.0254,
      "step": 4650
    },
    {
      "epoch": 5.642424242424243,
      "grad_norm": 0.052424050867557526,
      "learning_rate": 5e-05,
      "loss": 0.0244,
      "step": 4655
    },
    {
      "epoch": 5.648484848484848,
      "grad_norm": 0.04977964982390404,
      "learning_rate": 5e-05,
      "loss": 0.0243,
      "step": 4660
    },
    {
      "epoch": 5.654545454545454,
      "grad_norm": 0.04167182371020317,
      "learning_rate": 5e-05,
      "loss": 0.0243,
      "step": 4665
    },
    {
      "epoch": 5.66060606060606,
      "grad_norm": 0.032566096633672714,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 4670
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 0.03592539206147194,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 4675
    },
    {
      "epoch": 5.672727272727273,
      "grad_norm": 0.030059175565838814,
      "learning_rate": 5e-05,
      "loss": 0.0246,
      "step": 4680
    },
    {
      "epoch": 5.678787878787879,
      "grad_norm": 0.06916933506727219,
      "learning_rate": 5e-05,
      "loss": 0.0236,
      "step": 4685
    },
    {
      "epoch": 5.684848484848485,
      "grad_norm": 0.06057480722665787,
      "learning_rate": 5e-05,
      "loss": 0.0233,
      "step": 4690
    },
    {
      "epoch": 5.6909090909090905,
      "grad_norm": 0.06055600568652153,
      "learning_rate": 5e-05,
      "loss": 0.0246,
      "step": 4695
    },
    {
      "epoch": 5.696969696969697,
      "grad_norm": 0.037131961435079575,
      "learning_rate": 5e-05,
      "loss": 0.0237,
      "step": 4700
    },
    {
      "epoch": 5.703030303030303,
      "grad_norm": 0.030604610219597816,
      "learning_rate": 5e-05,
      "loss": 0.0234,
      "step": 4705
    },
    {
      "epoch": 5.709090909090909,
      "grad_norm": 0.0406193733215332,
      "learning_rate": 5e-05,
      "loss": 0.0241,
      "step": 4710
    },
    {
      "epoch": 5.715151515151515,
      "grad_norm": 0.10031086951494217,
      "learning_rate": 5e-05,
      "loss": 0.0243,
      "step": 4715
    },
    {
      "epoch": 5.721212121212122,
      "grad_norm": 0.04109171777963638,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 4720
    },
    {
      "epoch": 5.7272727272727275,
      "grad_norm": 0.037136442959308624,
      "learning_rate": 5e-05,
      "loss": 0.0246,
      "step": 4725
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 0.031140554696321487,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 4730
    },
    {
      "epoch": 5.739393939393939,
      "grad_norm": 0.040217332541942596,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 4735
    },
    {
      "epoch": 5.745454545454545,
      "grad_norm": 0.04498113691806793,
      "learning_rate": 5e-05,
      "loss": 0.0234,
      "step": 4740
    },
    {
      "epoch": 5.751515151515152,
      "grad_norm": 0.03106071799993515,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 4745
    },
    {
      "epoch": 5.757575757575758,
      "grad_norm": 0.05438012629747391,
      "learning_rate": 5e-05,
      "loss": 0.0237,
      "step": 4750
    },
    {
      "epoch": 5.763636363636364,
      "grad_norm": 0.044218190014362335,
      "learning_rate": 5e-05,
      "loss": 0.0238,
      "step": 4755
    },
    {
      "epoch": 5.7696969696969695,
      "grad_norm": 0.03991316258907318,
      "learning_rate": 5e-05,
      "loss": 0.0243,
      "step": 4760
    },
    {
      "epoch": 5.775757575757575,
      "grad_norm": 0.030073752626776695,
      "learning_rate": 5e-05,
      "loss": 0.0234,
      "step": 4765
    },
    {
      "epoch": 5.781818181818182,
      "grad_norm": 0.0261353999376297,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 4770
    },
    {
      "epoch": 5.787878787878788,
      "grad_norm": 0.031996943056583405,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 4775
    },
    {
      "epoch": 5.793939393939394,
      "grad_norm": 0.047766514122486115,
      "learning_rate": 5e-05,
      "loss": 0.024,
      "step": 4780
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.030206069350242615,
      "learning_rate": 5e-05,
      "loss": 0.0244,
      "step": 4785
    },
    {
      "epoch": 5.806060606060606,
      "grad_norm": 0.04486441984772682,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 4790
    },
    {
      "epoch": 5.8121212121212125,
      "grad_norm": 0.03529003635048866,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 4795
    },
    {
      "epoch": 5.818181818181818,
      "grad_norm": 0.036092743277549744,
      "learning_rate": 5e-05,
      "loss": 0.0241,
      "step": 4800
    },
    {
      "epoch": 5.824242424242424,
      "grad_norm": 0.03587324172258377,
      "learning_rate": 5e-05,
      "loss": 0.0248,
      "step": 4805
    },
    {
      "epoch": 5.83030303030303,
      "grad_norm": 0.02998632937669754,
      "learning_rate": 5e-05,
      "loss": 0.0244,
      "step": 4810
    },
    {
      "epoch": 5.836363636363636,
      "grad_norm": 0.04313105344772339,
      "learning_rate": 5e-05,
      "loss": 0.0243,
      "step": 4815
    },
    {
      "epoch": 5.842424242424243,
      "grad_norm": 0.04225587472319603,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 4820
    },
    {
      "epoch": 5.848484848484849,
      "grad_norm": 0.21142058074474335,
      "learning_rate": 5e-05,
      "loss": 0.0238,
      "step": 4825
    },
    {
      "epoch": 5.8545454545454545,
      "grad_norm": 0.03432782366871834,
      "learning_rate": 5e-05,
      "loss": 0.0242,
      "step": 4830
    },
    {
      "epoch": 5.86060606060606,
      "grad_norm": 0.0577109158039093,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 4835
    },
    {
      "epoch": 5.866666666666666,
      "grad_norm": 0.04129607602953911,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 4840
    },
    {
      "epoch": 5.872727272727273,
      "grad_norm": 0.04930323734879494,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 4845
    },
    {
      "epoch": 5.878787878787879,
      "grad_norm": 0.055211879312992096,
      "learning_rate": 5e-05,
      "loss": 0.0233,
      "step": 4850
    },
    {
      "epoch": 5.884848484848485,
      "grad_norm": 0.041990552097558975,
      "learning_rate": 5e-05,
      "loss": 0.0238,
      "step": 4855
    },
    {
      "epoch": 5.890909090909091,
      "grad_norm": 0.04597470164299011,
      "learning_rate": 5e-05,
      "loss": 0.0233,
      "step": 4860
    },
    {
      "epoch": 5.8969696969696965,
      "grad_norm": 0.03999422490596771,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 4865
    },
    {
      "epoch": 5.903030303030303,
      "grad_norm": 0.03644870966672897,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 4870
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 0.036632124334573746,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 4875
    },
    {
      "epoch": 5.915151515151515,
      "grad_norm": 0.05589691549539566,
      "learning_rate": 5e-05,
      "loss": 0.0239,
      "step": 4880
    },
    {
      "epoch": 5.921212121212121,
      "grad_norm": 0.06069694086909294,
      "learning_rate": 5e-05,
      "loss": 0.0241,
      "step": 4885
    },
    {
      "epoch": 5.927272727272728,
      "grad_norm": 0.0683228150010109,
      "learning_rate": 5e-05,
      "loss": 0.0238,
      "step": 4890
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 0.04255877807736397,
      "learning_rate": 5e-05,
      "loss": 0.0245,
      "step": 4895
    },
    {
      "epoch": 5.9393939393939394,
      "grad_norm": 0.03498656675219536,
      "learning_rate": 5e-05,
      "loss": 0.024,
      "step": 4900
    },
    {
      "epoch": 5.945454545454545,
      "grad_norm": 0.05036737397313118,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 4905
    },
    {
      "epoch": 5.951515151515151,
      "grad_norm": 0.0472494512796402,
      "learning_rate": 5e-05,
      "loss": 0.0253,
      "step": 4910
    },
    {
      "epoch": 5.957575757575758,
      "grad_norm": 0.040694963186979294,
      "learning_rate": 5e-05,
      "loss": 0.0242,
      "step": 4915
    },
    {
      "epoch": 5.963636363636364,
      "grad_norm": 0.043246325105428696,
      "learning_rate": 5e-05,
      "loss": 0.0225,
      "step": 4920
    },
    {
      "epoch": 5.96969696969697,
      "grad_norm": 0.04720492660999298,
      "learning_rate": 5e-05,
      "loss": 0.0249,
      "step": 4925
    },
    {
      "epoch": 5.975757575757576,
      "grad_norm": 0.047425996512174606,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 4930
    },
    {
      "epoch": 5.9818181818181815,
      "grad_norm": 0.05185554549098015,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 4935
    },
    {
      "epoch": 5.987878787878788,
      "grad_norm": 0.03044125996530056,
      "learning_rate": 5e-05,
      "loss": 0.0244,
      "step": 4940
    },
    {
      "epoch": 5.993939393939394,
      "grad_norm": 0.03137073665857315,
      "learning_rate": 5e-05,
      "loss": 0.0233,
      "step": 4945
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.06878063827753067,
      "learning_rate": 5e-05,
      "loss": 0.0237,
      "step": 4950
    },
    {
      "epoch": 6.0,
      "eval_average": 0.6292560112662907,
      "eval_crossner_ai": 0.6222715500794397,
      "eval_crossner_literature": 0.5942474168727844,
      "eval_crossner_music": 0.7691997580632687,
      "eval_crossner_politics": 0.6186269951460137,
      "eval_crossner_science": 0.6868782567003273,
      "eval_mit-movie": 0.623748211681447,
      "eval_mit-restaurant": 0.48981989032075374,
      "eval_runtime": 602.3966,
      "eval_samples_per_second": 10.101,
      "eval_steps_per_second": 0.159,
      "step": 4950
    },
    {
      "epoch": 6.006060606060606,
      "grad_norm": 0.025823570787906647,
      "learning_rate": 5e-05,
      "loss": 0.0242,
      "step": 4955
    },
    {
      "epoch": 6.012121212121212,
      "grad_norm": 0.03377290070056915,
      "learning_rate": 5e-05,
      "loss": 0.0239,
      "step": 4960
    },
    {
      "epoch": 6.0181818181818185,
      "grad_norm": 0.04030439630150795,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 4965
    },
    {
      "epoch": 6.024242424242424,
      "grad_norm": 0.03400493413209915,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 4970
    },
    {
      "epoch": 6.03030303030303,
      "grad_norm": 0.05579778179526329,
      "learning_rate": 5e-05,
      "loss": 0.0233,
      "step": 4975
    },
    {
      "epoch": 6.036363636363636,
      "grad_norm": 0.03866611048579216,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 4980
    },
    {
      "epoch": 6.042424242424242,
      "grad_norm": 0.05393664166331291,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 4985
    },
    {
      "epoch": 6.048484848484849,
      "grad_norm": 0.0503978431224823,
      "learning_rate": 5e-05,
      "loss": 0.0236,
      "step": 4990
    },
    {
      "epoch": 6.054545454545455,
      "grad_norm": 0.03131993114948273,
      "learning_rate": 5e-05,
      "loss": 0.0214,
      "step": 4995
    },
    {
      "epoch": 6.0606060606060606,
      "grad_norm": 0.047618620097637177,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5000
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 0.02891930751502514,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 5005
    },
    {
      "epoch": 6.072727272727272,
      "grad_norm": 0.05583135783672333,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 5010
    },
    {
      "epoch": 6.078787878787879,
      "grad_norm": 0.03380438685417175,
      "learning_rate": 5e-05,
      "loss": 0.0233,
      "step": 5015
    },
    {
      "epoch": 6.084848484848485,
      "grad_norm": 0.0279101449996233,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 5020
    },
    {
      "epoch": 6.090909090909091,
      "grad_norm": 0.04146373271942139,
      "learning_rate": 5e-05,
      "loss": 0.0228,
      "step": 5025
    },
    {
      "epoch": 6.096969696969697,
      "grad_norm": 0.050093285739421844,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 5030
    },
    {
      "epoch": 6.1030303030303035,
      "grad_norm": 0.05020187050104141,
      "learning_rate": 5e-05,
      "loss": 0.0243,
      "step": 5035
    },
    {
      "epoch": 6.109090909090909,
      "grad_norm": 0.053222257643938065,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 5040
    },
    {
      "epoch": 6.115151515151515,
      "grad_norm": 0.04996950179338455,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 5045
    },
    {
      "epoch": 6.121212121212121,
      "grad_norm": 0.04915186017751694,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 5050
    },
    {
      "epoch": 6.127272727272727,
      "grad_norm": 0.051093343645334244,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 5055
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 0.03988043963909149,
      "learning_rate": 5e-05,
      "loss": 0.0225,
      "step": 5060
    },
    {
      "epoch": 6.13939393939394,
      "grad_norm": 0.0369848869740963,
      "learning_rate": 5e-05,
      "loss": 0.0243,
      "step": 5065
    },
    {
      "epoch": 6.1454545454545455,
      "grad_norm": 0.03401264548301697,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 5070
    },
    {
      "epoch": 6.151515151515151,
      "grad_norm": 0.03996976092457771,
      "learning_rate": 5e-05,
      "loss": 0.0238,
      "step": 5075
    },
    {
      "epoch": 6.157575757575757,
      "grad_norm": 0.033281393349170685,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 5080
    },
    {
      "epoch": 6.163636363636364,
      "grad_norm": 0.032616592943668365,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 5085
    },
    {
      "epoch": 6.16969696969697,
      "grad_norm": 0.038187697529792786,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5090
    },
    {
      "epoch": 6.175757575757576,
      "grad_norm": 0.029854020103812218,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 5095
    },
    {
      "epoch": 6.181818181818182,
      "grad_norm": 0.03955798223614693,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 5100
    },
    {
      "epoch": 6.1878787878787875,
      "grad_norm": 0.04383544251322746,
      "learning_rate": 5e-05,
      "loss": 0.0234,
      "step": 5105
    },
    {
      "epoch": 6.193939393939394,
      "grad_norm": 0.04198103025555611,
      "learning_rate": 5e-05,
      "loss": 0.0228,
      "step": 5110
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.04035251960158348,
      "learning_rate": 5e-05,
      "loss": 0.025,
      "step": 5115
    },
    {
      "epoch": 6.206060606060606,
      "grad_norm": 0.04458486661314964,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 5120
    },
    {
      "epoch": 6.212121212121212,
      "grad_norm": 0.04994836449623108,
      "learning_rate": 5e-05,
      "loss": 0.0223,
      "step": 5125
    },
    {
      "epoch": 6.218181818181818,
      "grad_norm": 0.0462811104953289,
      "learning_rate": 5e-05,
      "loss": 0.0239,
      "step": 5130
    },
    {
      "epoch": 6.224242424242425,
      "grad_norm": 0.03698590397834778,
      "learning_rate": 5e-05,
      "loss": 0.0226,
      "step": 5135
    },
    {
      "epoch": 6.2303030303030305,
      "grad_norm": 0.06253299862146378,
      "learning_rate": 5e-05,
      "loss": 0.0225,
      "step": 5140
    },
    {
      "epoch": 6.236363636363636,
      "grad_norm": 0.058317482471466064,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 5145
    },
    {
      "epoch": 6.242424242424242,
      "grad_norm": 0.030127214267849922,
      "learning_rate": 5e-05,
      "loss": 0.0225,
      "step": 5150
    },
    {
      "epoch": 6.248484848484848,
      "grad_norm": 0.031119296327233315,
      "learning_rate": 5e-05,
      "loss": 0.0237,
      "step": 5155
    },
    {
      "epoch": 6.254545454545455,
      "grad_norm": 0.053826071321964264,
      "learning_rate": 5e-05,
      "loss": 0.0237,
      "step": 5160
    },
    {
      "epoch": 6.260606060606061,
      "grad_norm": 0.03480933979153633,
      "learning_rate": 5e-05,
      "loss": 0.0217,
      "step": 5165
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 0.04832719266414642,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 5170
    },
    {
      "epoch": 6.2727272727272725,
      "grad_norm": 0.0716027319431305,
      "learning_rate": 5e-05,
      "loss": 0.0237,
      "step": 5175
    },
    {
      "epoch": 6.278787878787878,
      "grad_norm": 0.040200673043727875,
      "learning_rate": 5e-05,
      "loss": 0.0241,
      "step": 5180
    },
    {
      "epoch": 6.284848484848485,
      "grad_norm": 0.031239835545420647,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 5185
    },
    {
      "epoch": 6.290909090909091,
      "grad_norm": 0.03427533432841301,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 5190
    },
    {
      "epoch": 6.296969696969697,
      "grad_norm": 0.03294594958424568,
      "learning_rate": 5e-05,
      "loss": 0.0226,
      "step": 5195
    },
    {
      "epoch": 6.303030303030303,
      "grad_norm": 0.06112398952245712,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 5200
    },
    {
      "epoch": 6.3090909090909095,
      "grad_norm": 0.027500297874212265,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 5205
    },
    {
      "epoch": 6.315151515151515,
      "grad_norm": 0.03422011062502861,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 5210
    },
    {
      "epoch": 6.321212121212121,
      "grad_norm": 0.03467288613319397,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 5215
    },
    {
      "epoch": 6.327272727272727,
      "grad_norm": 0.052317116409540176,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 5220
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 0.04434657096862793,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 5225
    },
    {
      "epoch": 6.33939393939394,
      "grad_norm": 0.038810182362794876,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 5230
    },
    {
      "epoch": 6.345454545454546,
      "grad_norm": 0.04263545572757721,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 5235
    },
    {
      "epoch": 6.351515151515152,
      "grad_norm": 0.033717043697834015,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5240
    },
    {
      "epoch": 6.357575757575757,
      "grad_norm": 0.04482770338654518,
      "learning_rate": 5e-05,
      "loss": 0.0221,
      "step": 5245
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 0.033339064568281174,
      "learning_rate": 5e-05,
      "loss": 0.024,
      "step": 5250
    },
    {
      "epoch": 6.36969696969697,
      "grad_norm": 0.038749851286411285,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 5255
    },
    {
      "epoch": 6.375757575757576,
      "grad_norm": 0.03668541461229324,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 5260
    },
    {
      "epoch": 6.381818181818182,
      "grad_norm": 0.040073320269584656,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 5265
    },
    {
      "epoch": 6.387878787878788,
      "grad_norm": 0.039210058748722076,
      "learning_rate": 5e-05,
      "loss": 0.0234,
      "step": 5270
    },
    {
      "epoch": 6.393939393939394,
      "grad_norm": 0.03880517929792404,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 5275
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.03742845356464386,
      "learning_rate": 5e-05,
      "loss": 0.0243,
      "step": 5280
    },
    {
      "epoch": 6.406060606060606,
      "grad_norm": 0.03137956187129021,
      "learning_rate": 5e-05,
      "loss": 0.0239,
      "step": 5285
    },
    {
      "epoch": 6.412121212121212,
      "grad_norm": 0.04118181765079498,
      "learning_rate": 5e-05,
      "loss": 0.0225,
      "step": 5290
    },
    {
      "epoch": 6.418181818181818,
      "grad_norm": 0.030874673277139664,
      "learning_rate": 5e-05,
      "loss": 0.0228,
      "step": 5295
    },
    {
      "epoch": 6.424242424242424,
      "grad_norm": 0.029253918677568436,
      "learning_rate": 5e-05,
      "loss": 0.0225,
      "step": 5300
    },
    {
      "epoch": 6.430303030303031,
      "grad_norm": 0.04759635403752327,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 5305
    },
    {
      "epoch": 6.4363636363636365,
      "grad_norm": 0.037839069962501526,
      "learning_rate": 5e-05,
      "loss": 0.024,
      "step": 5310
    },
    {
      "epoch": 6.442424242424242,
      "grad_norm": 0.05642589181661606,
      "learning_rate": 5e-05,
      "loss": 0.0212,
      "step": 5315
    },
    {
      "epoch": 6.448484848484848,
      "grad_norm": 0.042485255748033524,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 5320
    },
    {
      "epoch": 6.454545454545454,
      "grad_norm": 0.03360029309988022,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5325
    },
    {
      "epoch": 6.460606060606061,
      "grad_norm": 0.04505953937768936,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 5330
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 0.02797822467982769,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 5335
    },
    {
      "epoch": 6.472727272727273,
      "grad_norm": 0.042451608926057816,
      "learning_rate": 5e-05,
      "loss": 0.0221,
      "step": 5340
    },
    {
      "epoch": 6.4787878787878785,
      "grad_norm": 0.04907147213816643,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 5345
    },
    {
      "epoch": 6.484848484848484,
      "grad_norm": 0.03582075983285904,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 5350
    },
    {
      "epoch": 6.490909090909091,
      "grad_norm": 0.04138000309467316,
      "learning_rate": 5e-05,
      "loss": 0.024,
      "step": 5355
    },
    {
      "epoch": 6.496969696969697,
      "grad_norm": 0.04448524862527847,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 5360
    },
    {
      "epoch": 6.499393939393939,
      "eval_average": 0.6323020251785006,
      "eval_crossner_ai": 0.6073911452115784,
      "eval_crossner_literature": 0.5920177383092963,
      "eval_crossner_music": 0.7738310254366326,
      "eval_crossner_politics": 0.6451712433933624,
      "eval_crossner_science": 0.7124740603604038,
      "eval_mit-movie": 0.605956932934769,
      "eval_mit-restaurant": 0.4892720306034625,
      "eval_runtime": 604.9274,
      "eval_samples_per_second": 10.059,
      "eval_steps_per_second": 0.159,
      "step": 5362
    },
    {
      "epoch": 6.503030303030303,
      "grad_norm": 0.03024453856050968,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 5365
    },
    {
      "epoch": 6.509090909090909,
      "grad_norm": 0.046045925468206406,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 5370
    },
    {
      "epoch": 6.515151515151516,
      "grad_norm": 0.04022037982940674,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5375
    },
    {
      "epoch": 6.5212121212121215,
      "grad_norm": 0.05105359107255936,
      "learning_rate": 5e-05,
      "loss": 0.0223,
      "step": 5380
    },
    {
      "epoch": 6.527272727272727,
      "grad_norm": 0.06293677538633347,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 5385
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 0.042019449174404144,
      "learning_rate": 5e-05,
      "loss": 0.0228,
      "step": 5390
    },
    {
      "epoch": 6.539393939393939,
      "grad_norm": 0.044339217245578766,
      "learning_rate": 5e-05,
      "loss": 0.0225,
      "step": 5395
    },
    {
      "epoch": 6.545454545454546,
      "grad_norm": 0.024878356605768204,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 5400
    },
    {
      "epoch": 6.551515151515152,
      "grad_norm": 0.047537073493003845,
      "learning_rate": 5e-05,
      "loss": 0.0226,
      "step": 5405
    },
    {
      "epoch": 6.557575757575758,
      "grad_norm": 0.0369623638689518,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 5410
    },
    {
      "epoch": 6.5636363636363635,
      "grad_norm": 0.05781250074505806,
      "learning_rate": 5e-05,
      "loss": 0.0226,
      "step": 5415
    },
    {
      "epoch": 6.569696969696969,
      "grad_norm": 0.0445278100669384,
      "learning_rate": 5e-05,
      "loss": 0.0236,
      "step": 5420
    },
    {
      "epoch": 6.575757575757576,
      "grad_norm": 0.04003453254699707,
      "learning_rate": 5e-05,
      "loss": 0.0225,
      "step": 5425
    },
    {
      "epoch": 6.581818181818182,
      "grad_norm": 0.035290610045194626,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 5430
    },
    {
      "epoch": 6.587878787878788,
      "grad_norm": 0.030457014217972755,
      "learning_rate": 5e-05,
      "loss": 0.0215,
      "step": 5435
    },
    {
      "epoch": 6.593939393939394,
      "grad_norm": 0.03671363741159439,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 5440
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.05032859742641449,
      "learning_rate": 5e-05,
      "loss": 0.0241,
      "step": 5445
    },
    {
      "epoch": 6.606060606060606,
      "grad_norm": 0.032951515167951584,
      "learning_rate": 5e-05,
      "loss": 0.0247,
      "step": 5450
    },
    {
      "epoch": 6.612121212121212,
      "grad_norm": 0.037229686975479126,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 5455
    },
    {
      "epoch": 6.618181818181818,
      "grad_norm": 0.051831334829330444,
      "learning_rate": 5e-05,
      "loss": 0.0226,
      "step": 5460
    },
    {
      "epoch": 6.624242424242424,
      "grad_norm": 0.03642982244491577,
      "learning_rate": 5e-05,
      "loss": 0.0226,
      "step": 5465
    },
    {
      "epoch": 6.63030303030303,
      "grad_norm": 0.03402450308203697,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 5470
    },
    {
      "epoch": 6.636363636363637,
      "grad_norm": 0.03683866932988167,
      "learning_rate": 5e-05,
      "loss": 0.0221,
      "step": 5475
    },
    {
      "epoch": 6.642424242424243,
      "grad_norm": 0.030404694378376007,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 5480
    },
    {
      "epoch": 6.648484848484848,
      "grad_norm": 0.0510781928896904,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 5485
    },
    {
      "epoch": 6.654545454545454,
      "grad_norm": 0.035895153880119324,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 5490
    },
    {
      "epoch": 6.66060606060606,
      "grad_norm": 0.028813138604164124,
      "learning_rate": 5e-05,
      "loss": 0.0236,
      "step": 5495
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.03421507775783539,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 5500
    },
    {
      "epoch": 6.672727272727273,
      "grad_norm": 0.042716994881629944,
      "learning_rate": 5e-05,
      "loss": 0.024,
      "step": 5505
    },
    {
      "epoch": 6.678787878787879,
      "grad_norm": 0.036884844303131104,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 5510
    },
    {
      "epoch": 6.684848484848485,
      "grad_norm": 0.04365573450922966,
      "learning_rate": 5e-05,
      "loss": 0.0241,
      "step": 5515
    },
    {
      "epoch": 6.6909090909090905,
      "grad_norm": 0.03946178779006004,
      "learning_rate": 5e-05,
      "loss": 0.0226,
      "step": 5520
    },
    {
      "epoch": 6.696969696969697,
      "grad_norm": 0.03550593554973602,
      "learning_rate": 5e-05,
      "loss": 0.024,
      "step": 5525
    },
    {
      "epoch": 6.703030303030303,
      "grad_norm": 0.029865065589547157,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 5530
    },
    {
      "epoch": 6.709090909090909,
      "grad_norm": 0.04643668234348297,
      "learning_rate": 5e-05,
      "loss": 0.0223,
      "step": 5535
    },
    {
      "epoch": 6.715151515151515,
      "grad_norm": 0.037629082798957825,
      "learning_rate": 5e-05,
      "loss": 0.0223,
      "step": 5540
    },
    {
      "epoch": 6.721212121212122,
      "grad_norm": 0.04283478856086731,
      "learning_rate": 5e-05,
      "loss": 0.0221,
      "step": 5545
    },
    {
      "epoch": 6.7272727272727275,
      "grad_norm": 0.06966075301170349,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 5550
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 0.03506382927298546,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 5555
    },
    {
      "epoch": 6.739393939393939,
      "grad_norm": 0.033555563539266586,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 5560
    },
    {
      "epoch": 6.745454545454545,
      "grad_norm": 0.0392502099275589,
      "learning_rate": 5e-05,
      "loss": 0.0236,
      "step": 5565
    },
    {
      "epoch": 6.751515151515152,
      "grad_norm": 0.035255931317806244,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 5570
    },
    {
      "epoch": 6.757575757575758,
      "grad_norm": 0.03635290637612343,
      "learning_rate": 5e-05,
      "loss": 0.0239,
      "step": 5575
    },
    {
      "epoch": 6.763636363636364,
      "grad_norm": 0.03873997554183006,
      "learning_rate": 5e-05,
      "loss": 0.0217,
      "step": 5580
    },
    {
      "epoch": 6.7696969696969695,
      "grad_norm": 0.04637407511472702,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 5585
    },
    {
      "epoch": 6.775757575757575,
      "grad_norm": 0.03896280378103256,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 5590
    },
    {
      "epoch": 6.781818181818182,
      "grad_norm": 0.04225950688123703,
      "learning_rate": 5e-05,
      "loss": 0.0209,
      "step": 5595
    },
    {
      "epoch": 6.787878787878788,
      "grad_norm": 0.02829851023852825,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 5600
    },
    {
      "epoch": 6.793939393939394,
      "grad_norm": 0.04042356088757515,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 5605
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.039382219314575195,
      "learning_rate": 5e-05,
      "loss": 0.0238,
      "step": 5610
    },
    {
      "epoch": 6.806060606060606,
      "grad_norm": 0.0320318341255188,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5615
    },
    {
      "epoch": 6.8121212121212125,
      "grad_norm": 0.03149804100394249,
      "learning_rate": 5e-05,
      "loss": 0.0218,
      "step": 5620
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 0.04935714229941368,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 5625
    },
    {
      "epoch": 6.824242424242424,
      "grad_norm": 0.06505697220563889,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 5630
    },
    {
      "epoch": 6.83030303030303,
      "grad_norm": 0.038327980786561966,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 5635
    },
    {
      "epoch": 6.836363636363636,
      "grad_norm": 0.04506022855639458,
      "learning_rate": 5e-05,
      "loss": 0.0242,
      "step": 5640
    },
    {
      "epoch": 6.842424242424243,
      "grad_norm": 0.03191790357232094,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 5645
    },
    {
      "epoch": 6.848484848484849,
      "grad_norm": 0.03665788471698761,
      "learning_rate": 5e-05,
      "loss": 0.0228,
      "step": 5650
    },
    {
      "epoch": 6.8545454545454545,
      "grad_norm": 0.054028552025556564,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 5655
    },
    {
      "epoch": 6.86060606060606,
      "grad_norm": 0.04015377536416054,
      "learning_rate": 5e-05,
      "loss": 0.0234,
      "step": 5660
    },
    {
      "epoch": 6.866666666666666,
      "grad_norm": 0.052709996700286865,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 5665
    },
    {
      "epoch": 6.872727272727273,
      "grad_norm": 0.06626269966363907,
      "learning_rate": 5e-05,
      "loss": 0.0237,
      "step": 5670
    },
    {
      "epoch": 6.878787878787879,
      "grad_norm": 0.04529676213860512,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 5675
    },
    {
      "epoch": 6.884848484848485,
      "grad_norm": 0.030927447602152824,
      "learning_rate": 5e-05,
      "loss": 0.0232,
      "step": 5680
    },
    {
      "epoch": 6.890909090909091,
      "grad_norm": 0.03429831191897392,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5685
    },
    {
      "epoch": 6.8969696969696965,
      "grad_norm": 0.04907563328742981,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 5690
    },
    {
      "epoch": 6.903030303030303,
      "grad_norm": 0.030987635254859924,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 5695
    },
    {
      "epoch": 6.909090909090909,
      "grad_norm": 0.04393631964921951,
      "learning_rate": 5e-05,
      "loss": 0.0228,
      "step": 5700
    },
    {
      "epoch": 6.915151515151515,
      "grad_norm": 0.0405719056725502,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 5705
    },
    {
      "epoch": 6.921212121212121,
      "grad_norm": 0.056112539023160934,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 5710
    },
    {
      "epoch": 6.927272727272728,
      "grad_norm": 0.049579717218875885,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 5715
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 0.06507901102304459,
      "learning_rate": 5e-05,
      "loss": 0.0228,
      "step": 5720
    },
    {
      "epoch": 6.9393939393939394,
      "grad_norm": 0.06459630280733109,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 5725
    },
    {
      "epoch": 6.945454545454545,
      "grad_norm": 0.07845195382833481,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5730
    },
    {
      "epoch": 6.951515151515151,
      "grad_norm": 0.04563499242067337,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 5735
    },
    {
      "epoch": 6.957575757575758,
      "grad_norm": 0.03691524639725685,
      "learning_rate": 5e-05,
      "loss": 0.0233,
      "step": 5740
    },
    {
      "epoch": 6.963636363636364,
      "grad_norm": 0.03468068316578865,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 5745
    },
    {
      "epoch": 6.96969696969697,
      "grad_norm": 0.03550529107451439,
      "learning_rate": 5e-05,
      "loss": 0.0226,
      "step": 5750
    },
    {
      "epoch": 6.975757575757576,
      "grad_norm": 0.04908468946814537,
      "learning_rate": 5e-05,
      "loss": 0.0218,
      "step": 5755
    },
    {
      "epoch": 6.9818181818181815,
      "grad_norm": 0.04492495208978653,
      "learning_rate": 5e-05,
      "loss": 0.0223,
      "step": 5760
    },
    {
      "epoch": 6.987878787878788,
      "grad_norm": 0.04579722508788109,
      "learning_rate": 5e-05,
      "loss": 0.023,
      "step": 5765
    },
    {
      "epoch": 6.993939393939394,
      "grad_norm": 0.04569530487060547,
      "learning_rate": 5e-05,
      "loss": 0.0231,
      "step": 5770
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.04468743875622749,
      "learning_rate": 5e-05,
      "loss": 0.0225,
      "step": 5775
    },
    {
      "epoch": 7.0,
      "eval_average": 0.6322591466340552,
      "eval_crossner_ai": 0.6103660746147093,
      "eval_crossner_literature": 0.6137184115024502,
      "eval_crossner_music": 0.7877697841226347,
      "eval_crossner_politics": 0.6186075168939134,
      "eval_crossner_science": 0.7095271371500627,
      "eval_mit-movie": 0.5964877069246545,
      "eval_mit-restaurant": 0.48933739522996156,
      "eval_runtime": 619.0763,
      "eval_samples_per_second": 9.829,
      "eval_steps_per_second": 0.155,
      "step": 5775
    },
    {
      "epoch": 7.006060606060606,
      "grad_norm": 0.04092913493514061,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 5780
    },
    {
      "epoch": 7.012121212121212,
      "grad_norm": 0.03362756222486496,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 5785
    },
    {
      "epoch": 7.0181818181818185,
      "grad_norm": 0.04348708689212799,
      "learning_rate": 5e-05,
      "loss": 0.0237,
      "step": 5790
    },
    {
      "epoch": 7.024242424242424,
      "grad_norm": 0.03295361250638962,
      "learning_rate": 5e-05,
      "loss": 0.0212,
      "step": 5795
    },
    {
      "epoch": 7.03030303030303,
      "grad_norm": 0.030986908823251724,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5800
    },
    {
      "epoch": 7.036363636363636,
      "grad_norm": 0.031977251172065735,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 5805
    },
    {
      "epoch": 7.042424242424242,
      "grad_norm": 0.033138103783130646,
      "learning_rate": 5e-05,
      "loss": 0.0223,
      "step": 5810
    },
    {
      "epoch": 7.048484848484849,
      "grad_norm": 0.0437137633562088,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 5815
    },
    {
      "epoch": 7.054545454545455,
      "grad_norm": 0.054445408284664154,
      "learning_rate": 5e-05,
      "loss": 0.0212,
      "step": 5820
    },
    {
      "epoch": 7.0606060606060606,
      "grad_norm": 0.0334361232817173,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 5825
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 0.03551406040787697,
      "learning_rate": 5e-05,
      "loss": 0.0226,
      "step": 5830
    },
    {
      "epoch": 7.072727272727272,
      "grad_norm": 0.04352130368351936,
      "learning_rate": 5e-05,
      "loss": 0.0217,
      "step": 5835
    },
    {
      "epoch": 7.078787878787879,
      "grad_norm": 0.04300972446799278,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5840
    },
    {
      "epoch": 7.084848484848485,
      "grad_norm": 0.08021088689565659,
      "learning_rate": 5e-05,
      "loss": 0.0217,
      "step": 5845
    },
    {
      "epoch": 7.090909090909091,
      "grad_norm": 0.04200869798660278,
      "learning_rate": 5e-05,
      "loss": 0.0209,
      "step": 5850
    },
    {
      "epoch": 7.096969696969697,
      "grad_norm": 0.03685295954346657,
      "learning_rate": 5e-05,
      "loss": 0.0215,
      "step": 5855
    },
    {
      "epoch": 7.1030303030303035,
      "grad_norm": 0.038492172956466675,
      "learning_rate": 5e-05,
      "loss": 0.0214,
      "step": 5860
    },
    {
      "epoch": 7.109090909090909,
      "grad_norm": 0.04138908535242081,
      "learning_rate": 5e-05,
      "loss": 0.0206,
      "step": 5865
    },
    {
      "epoch": 7.115151515151515,
      "grad_norm": 0.043056342750787735,
      "learning_rate": 5e-05,
      "loss": 0.0207,
      "step": 5870
    },
    {
      "epoch": 7.121212121212121,
      "grad_norm": 0.039815910160541534,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5875
    },
    {
      "epoch": 7.127272727272727,
      "grad_norm": 0.04309098795056343,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 5880
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 0.0427989698946476,
      "learning_rate": 5e-05,
      "loss": 0.0216,
      "step": 5885
    },
    {
      "epoch": 7.13939393939394,
      "grad_norm": 0.032435692846775055,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5890
    },
    {
      "epoch": 7.1454545454545455,
      "grad_norm": 0.03879780322313309,
      "learning_rate": 5e-05,
      "loss": 0.0218,
      "step": 5895
    },
    {
      "epoch": 7.151515151515151,
      "grad_norm": 0.040325190871953964,
      "learning_rate": 5e-05,
      "loss": 0.0225,
      "step": 5900
    },
    {
      "epoch": 7.157575757575757,
      "grad_norm": 0.035131439566612244,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 5905
    },
    {
      "epoch": 7.163636363636364,
      "grad_norm": 0.03529278188943863,
      "learning_rate": 5e-05,
      "loss": 0.0217,
      "step": 5910
    },
    {
      "epoch": 7.16969696969697,
      "grad_norm": 0.03194297477602959,
      "learning_rate": 5e-05,
      "loss": 0.0217,
      "step": 5915
    },
    {
      "epoch": 7.175757575757576,
      "grad_norm": 0.04129665344953537,
      "learning_rate": 5e-05,
      "loss": 0.0228,
      "step": 5920
    },
    {
      "epoch": 7.181818181818182,
      "grad_norm": 0.03661396726965904,
      "learning_rate": 5e-05,
      "loss": 0.0209,
      "step": 5925
    },
    {
      "epoch": 7.1878787878787875,
      "grad_norm": 0.036278706043958664,
      "learning_rate": 5e-05,
      "loss": 0.0212,
      "step": 5930
    },
    {
      "epoch": 7.193939393939394,
      "grad_norm": 0.04110967740416527,
      "learning_rate": 5e-05,
      "loss": 0.0215,
      "step": 5935
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.0414452850818634,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 5940
    },
    {
      "epoch": 7.206060606060606,
      "grad_norm": 0.05806460976600647,
      "learning_rate": 5e-05,
      "loss": 0.0225,
      "step": 5945
    },
    {
      "epoch": 7.212121212121212,
      "grad_norm": 0.07822374254465103,
      "learning_rate": 5e-05,
      "loss": 0.0207,
      "step": 5950
    },
    {
      "epoch": 7.218181818181818,
      "grad_norm": 0.058976639062166214,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5955
    },
    {
      "epoch": 7.224242424242425,
      "grad_norm": 0.04809923097491264,
      "learning_rate": 5e-05,
      "loss": 0.0201,
      "step": 5960
    },
    {
      "epoch": 7.2303030303030305,
      "grad_norm": 0.0454961322247982,
      "learning_rate": 5e-05,
      "loss": 0.0215,
      "step": 5965
    },
    {
      "epoch": 7.236363636363636,
      "grad_norm": 0.03896050527691841,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 5970
    },
    {
      "epoch": 7.242424242424242,
      "grad_norm": 0.042614202946424484,
      "learning_rate": 5e-05,
      "loss": 0.0214,
      "step": 5975
    },
    {
      "epoch": 7.248484848484848,
      "grad_norm": 0.03943157196044922,
      "learning_rate": 5e-05,
      "loss": 0.0216,
      "step": 5980
    },
    {
      "epoch": 7.254545454545455,
      "grad_norm": 0.039226192981004715,
      "learning_rate": 5e-05,
      "loss": 0.0226,
      "step": 5985
    },
    {
      "epoch": 7.260606060606061,
      "grad_norm": 0.031284675002098083,
      "learning_rate": 5e-05,
      "loss": 0.0216,
      "step": 5990
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 0.036902640014886856,
      "learning_rate": 5e-05,
      "loss": 0.0235,
      "step": 5995
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 0.05406607687473297,
      "learning_rate": 5e-05,
      "loss": 0.0213,
      "step": 6000
    },
    {
      "epoch": 7.278787878787878,
      "grad_norm": 0.055550165474414825,
      "learning_rate": 5e-05,
      "loss": 0.021,
      "step": 6005
    },
    {
      "epoch": 7.284848484848485,
      "grad_norm": 0.03486628085374832,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 6010
    },
    {
      "epoch": 7.290909090909091,
      "grad_norm": 0.035359304398298264,
      "learning_rate": 5e-05,
      "loss": 0.0218,
      "step": 6015
    },
    {
      "epoch": 7.296969696969697,
      "grad_norm": 0.04344772547483444,
      "learning_rate": 5e-05,
      "loss": 0.0214,
      "step": 6020
    },
    {
      "epoch": 7.303030303030303,
      "grad_norm": 0.04125402122735977,
      "learning_rate": 5e-05,
      "loss": 0.0198,
      "step": 6025
    },
    {
      "epoch": 7.3090909090909095,
      "grad_norm": 0.03538498282432556,
      "learning_rate": 5e-05,
      "loss": 0.0206,
      "step": 6030
    },
    {
      "epoch": 7.315151515151515,
      "grad_norm": 0.04153091087937355,
      "learning_rate": 5e-05,
      "loss": 0.021,
      "step": 6035
    },
    {
      "epoch": 7.321212121212121,
      "grad_norm": 0.042601753026247025,
      "learning_rate": 5e-05,
      "loss": 0.0221,
      "step": 6040
    },
    {
      "epoch": 7.327272727272727,
      "grad_norm": 0.044296327978372574,
      "learning_rate": 5e-05,
      "loss": 0.0214,
      "step": 6045
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 0.05313859134912491,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 6050
    },
    {
      "epoch": 7.33939393939394,
      "grad_norm": 0.03247370943427086,
      "learning_rate": 5e-05,
      "loss": 0.0221,
      "step": 6055
    },
    {
      "epoch": 7.345454545454546,
      "grad_norm": 0.05260825529694557,
      "learning_rate": 5e-05,
      "loss": 0.0225,
      "step": 6060
    },
    {
      "epoch": 7.351515151515152,
      "grad_norm": 0.0887979045510292,
      "learning_rate": 5e-05,
      "loss": 0.0219,
      "step": 6065
    },
    {
      "epoch": 7.357575757575757,
      "grad_norm": 0.10991904139518738,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 6070
    },
    {
      "epoch": 7.363636363636363,
      "grad_norm": 0.04387820139527321,
      "learning_rate": 5e-05,
      "loss": 0.0221,
      "step": 6075
    },
    {
      "epoch": 7.36969696969697,
      "grad_norm": 0.030923889949917793,
      "learning_rate": 5e-05,
      "loss": 0.0217,
      "step": 6080
    },
    {
      "epoch": 7.375757575757576,
      "grad_norm": 0.038198210299015045,
      "learning_rate": 5e-05,
      "loss": 0.0223,
      "step": 6085
    },
    {
      "epoch": 7.381818181818182,
      "grad_norm": 0.03924135863780975,
      "learning_rate": 5e-05,
      "loss": 0.0226,
      "step": 6090
    },
    {
      "epoch": 7.387878787878788,
      "grad_norm": 0.06184989959001541,
      "learning_rate": 5e-05,
      "loss": 0.0227,
      "step": 6095
    },
    {
      "epoch": 7.393939393939394,
      "grad_norm": 0.040304120630025864,
      "learning_rate": 5e-05,
      "loss": 0.0213,
      "step": 6100
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.04610481113195419,
      "learning_rate": 5e-05,
      "loss": 0.0223,
      "step": 6105
    },
    {
      "epoch": 7.406060606060606,
      "grad_norm": 0.03284385800361633,
      "learning_rate": 5e-05,
      "loss": 0.021,
      "step": 6110
    },
    {
      "epoch": 7.412121212121212,
      "grad_norm": 0.0472518652677536,
      "learning_rate": 5e-05,
      "loss": 0.0214,
      "step": 6115
    },
    {
      "epoch": 7.418181818181818,
      "grad_norm": 0.044741395860910416,
      "learning_rate": 5e-05,
      "loss": 0.0229,
      "step": 6120
    },
    {
      "epoch": 7.424242424242424,
      "grad_norm": 0.03872781991958618,
      "learning_rate": 5e-05,
      "loss": 0.0205,
      "step": 6125
    },
    {
      "epoch": 7.430303030303031,
      "grad_norm": 0.03433758392930031,
      "learning_rate": 5e-05,
      "loss": 0.0211,
      "step": 6130
    },
    {
      "epoch": 7.4363636363636365,
      "grad_norm": 0.040726855397224426,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 6135
    },
    {
      "epoch": 7.442424242424242,
      "grad_norm": 0.027523770928382874,
      "learning_rate": 5e-05,
      "loss": 0.0209,
      "step": 6140
    },
    {
      "epoch": 7.448484848484848,
      "grad_norm": 0.025933850556612015,
      "learning_rate": 5e-05,
      "loss": 0.0216,
      "step": 6145
    },
    {
      "epoch": 7.454545454545454,
      "grad_norm": 0.03207048773765564,
      "learning_rate": 5e-05,
      "loss": 0.0218,
      "step": 6150
    },
    {
      "epoch": 7.460606060606061,
      "grad_norm": 0.03574753552675247,
      "learning_rate": 5e-05,
      "loss": 0.0218,
      "step": 6155
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 0.05977269262075424,
      "learning_rate": 5e-05,
      "loss": 0.0222,
      "step": 6160
    },
    {
      "epoch": 7.472727272727273,
      "grad_norm": 0.06830301880836487,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 6165
    },
    {
      "epoch": 7.4787878787878785,
      "grad_norm": 0.047804221510887146,
      "learning_rate": 5e-05,
      "loss": 0.0221,
      "step": 6170
    },
    {
      "epoch": 7.484848484848484,
      "grad_norm": 0.0429578460752964,
      "learning_rate": 5e-05,
      "loss": 0.0224,
      "step": 6175
    },
    {
      "epoch": 7.490909090909091,
      "grad_norm": 0.029759686440229416,
      "learning_rate": 5e-05,
      "loss": 0.022,
      "step": 6180
    },
    {
      "epoch": 7.496969696969697,
      "grad_norm": 0.029290813952684402,
      "learning_rate": 5e-05,
      "loss": 0.0218,
      "step": 6185
    },
    {
      "epoch": 7.500606060606061,
      "eval_average": 0.64513710636598,
      "eval_crossner_ai": 0.6220414200682972,
      "eval_crossner_literature": 0.6245103524963144,
      "eval_crossner_music": 0.790316126400554,
      "eval_crossner_politics": 0.6491446344756416,
      "eval_crossner_science": 0.736570490215037,
      "eval_mit-movie": 0.6040882741291427,
      "eval_mit-restaurant": 0.4892884467768731,
      "eval_runtime": 605.6933,
      "eval_samples_per_second": 10.046,
      "eval_steps_per_second": 0.158,
      "step": 6188
    }
  ],
  "logging_steps": 5,
  "max_steps": 8250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 9223372036854775807,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.198517129616294e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
