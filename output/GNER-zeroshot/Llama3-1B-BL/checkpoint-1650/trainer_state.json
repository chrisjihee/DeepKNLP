{
  "best_metric": 0.5840201620937151,
  "best_model_checkpoint": "output/GNER-zeroshot/Llama3-1B-BL/checkpoint-1650",
  "epoch": 2.0,
  "eval_steps": 9223372036854775807,
  "global_step": 1650,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006060606060606061,
      "grad_norm": 8.63490104675293,
      "learning_rate": 5.381462829997033e-06,
      "loss": 0.5825,
      "step": 5
    },
    {
      "epoch": 0.012121212121212121,
      "grad_norm": 4.322505474090576,
      "learning_rate": 7.699132719020058e-06,
      "loss": 0.3051,
      "step": 10
    },
    {
      "epoch": 0.01818181818181818,
      "grad_norm": 3.8407723903656006,
      "learning_rate": 9.05488269314909e-06,
      "loss": 0.15,
      "step": 15
    },
    {
      "epoch": 0.024242424242424242,
      "grad_norm": 3.2588558197021484,
      "learning_rate": 1.001680260804308e-05,
      "loss": 0.1129,
      "step": 20
    },
    {
      "epoch": 0.030303030303030304,
      "grad_norm": 1.7190009355545044,
      "learning_rate": 1.0762925659994066e-05,
      "loss": 0.0809,
      "step": 25
    },
    {
      "epoch": 0.03636363636363636,
      "grad_norm": 2.713549852371216,
      "learning_rate": 1.1372552582172113e-05,
      "loss": 0.0729,
      "step": 30
    },
    {
      "epoch": 0.04242424242424243,
      "grad_norm": 1.786845088005066,
      "learning_rate": 1.1887984800650519e-05,
      "loss": 0.065,
      "step": 35
    },
    {
      "epoch": 0.048484848484848485,
      "grad_norm": 0.8609691262245178,
      "learning_rate": 1.2334472497066103e-05,
      "loss": 0.0594,
      "step": 40
    },
    {
      "epoch": 0.05454545454545454,
      "grad_norm": 0.5927493572235107,
      "learning_rate": 1.2728302556301145e-05,
      "loss": 0.0542,
      "step": 45
    },
    {
      "epoch": 0.06060606060606061,
      "grad_norm": 0.5668410062789917,
      "learning_rate": 1.308059554901709e-05,
      "loss": 0.0535,
      "step": 50
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.7819388508796692,
      "learning_rate": 1.3399283325646878e-05,
      "loss": 0.0504,
      "step": 55
    },
    {
      "epoch": 0.07272727272727272,
      "grad_norm": 0.4519084393978119,
      "learning_rate": 1.3690222471195137e-05,
      "loss": 0.049,
      "step": 60
    },
    {
      "epoch": 0.07878787878787878,
      "grad_norm": 0.45400285720825195,
      "learning_rate": 1.3957860540877486e-05,
      "loss": 0.0488,
      "step": 65
    },
    {
      "epoch": 0.08484848484848485,
      "grad_norm": 0.5067660212516785,
      "learning_rate": 1.4205654689673544e-05,
      "loss": 0.0474,
      "step": 70
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.4362829923629761,
      "learning_rate": 1.4436345523146123e-05,
      "loss": 0.0466,
      "step": 75
    },
    {
      "epoch": 0.09696969696969697,
      "grad_norm": 0.5346341133117676,
      "learning_rate": 1.4652142386089125e-05,
      "loss": 0.0438,
      "step": 80
    },
    {
      "epoch": 0.10303030303030303,
      "grad_norm": 0.7718464732170105,
      "learning_rate": 1.4854852379663441e-05,
      "loss": 0.0463,
      "step": 85
    },
    {
      "epoch": 0.10909090909090909,
      "grad_norm": 0.8955128192901611,
      "learning_rate": 1.5045972445324168e-05,
      "loss": 0.0445,
      "step": 90
    },
    {
      "epoch": 0.11515151515151516,
      "grad_norm": 1.0805050134658813,
      "learning_rate": 1.5226756518657677e-05,
      "loss": 0.0438,
      "step": 95
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 0.6605945825576782,
      "learning_rate": 1.5398265438040116e-05,
      "loss": 0.0451,
      "step": 100
    },
    {
      "epoch": 0.12727272727272726,
      "grad_norm": 0.3935721516609192,
      "learning_rate": 1.5561404663802574e-05,
      "loss": 0.0414,
      "step": 105
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.34878817200660706,
      "learning_rate": 1.5716953214669903e-05,
      "loss": 0.0423,
      "step": 110
    },
    {
      "epoch": 0.1393939393939394,
      "grad_norm": 0.38836702704429626,
      "learning_rate": 1.5865586166680462e-05,
      "loss": 0.041,
      "step": 115
    },
    {
      "epoch": 0.14545454545454545,
      "grad_norm": 0.5763735771179199,
      "learning_rate": 1.600789236021816e-05,
      "loss": 0.0404,
      "step": 120
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 0.6142232418060303,
      "learning_rate": 1.6144388489991102e-05,
      "loss": 0.0399,
      "step": 125
    },
    {
      "epoch": 0.15757575757575756,
      "grad_norm": 0.457793265581131,
      "learning_rate": 1.627553042990051e-05,
      "loss": 0.0375,
      "step": 130
    },
    {
      "epoch": 0.16363636363636364,
      "grad_norm": 0.44654425978660583,
      "learning_rate": 1.6401722419453202e-05,
      "loss": 0.0376,
      "step": 135
    },
    {
      "epoch": 0.1696969696969697,
      "grad_norm": 0.4559710621833801,
      "learning_rate": 1.6523324578696566e-05,
      "loss": 0.04,
      "step": 140
    },
    {
      "epoch": 0.17575757575757575,
      "grad_norm": 0.5468865036964417,
      "learning_rate": 1.664065910385031e-05,
      "loss": 0.0379,
      "step": 145
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.3406648635864258,
      "learning_rate": 1.6754015412169146e-05,
      "loss": 0.0378,
      "step": 150
    },
    {
      "epoch": 0.18787878787878787,
      "grad_norm": 0.305560439825058,
      "learning_rate": 1.6863654442889655e-05,
      "loss": 0.0393,
      "step": 155
    },
    {
      "epoch": 0.19393939393939394,
      "grad_norm": 0.709934651851654,
      "learning_rate": 1.696981227511215e-05,
      "loss": 0.0389,
      "step": 160
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.4553469121456146,
      "learning_rate": 1.7072703188798933e-05,
      "loss": 0.038,
      "step": 165
    },
    {
      "epoch": 0.20606060606060606,
      "grad_norm": 0.4402814507484436,
      "learning_rate": 1.7172522268686466e-05,
      "loss": 0.0386,
      "step": 170
    },
    {
      "epoch": 0.21212121212121213,
      "grad_norm": 0.4144954979419708,
      "learning_rate": 1.7269447630647555e-05,
      "loss": 0.0369,
      "step": 175
    },
    {
      "epoch": 0.21818181818181817,
      "grad_norm": 0.35709768533706665,
      "learning_rate": 1.7363642334347193e-05,
      "loss": 0.0379,
      "step": 180
    },
    {
      "epoch": 0.22424242424242424,
      "grad_norm": 0.4880967140197754,
      "learning_rate": 1.7455256033784897e-05,
      "loss": 0.0367,
      "step": 185
    },
    {
      "epoch": 0.23030303030303031,
      "grad_norm": 0.35466328263282776,
      "learning_rate": 1.75444264076807e-05,
      "loss": 0.0358,
      "step": 190
    },
    {
      "epoch": 0.23636363636363636,
      "grad_norm": 0.31103697419166565,
      "learning_rate": 1.7631280404029543e-05,
      "loss": 0.036,
      "step": 195
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 0.4138535261154175,
      "learning_rate": 1.7715935327063138e-05,
      "loss": 0.0361,
      "step": 200
    },
    {
      "epoch": 0.24848484848484848,
      "grad_norm": 0.6836885809898376,
      "learning_rate": 1.7798499789975307e-05,
      "loss": 0.0378,
      "step": 205
    },
    {
      "epoch": 0.2545454545454545,
      "grad_norm": 0.5516701936721802,
      "learning_rate": 1.78790745528256e-05,
      "loss": 0.0369,
      "step": 210
    },
    {
      "epoch": 0.2606060606060606,
      "grad_norm": 0.39796844124794006,
      "learning_rate": 1.7957753261836987e-05,
      "loss": 0.0385,
      "step": 215
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.44762420654296875,
      "learning_rate": 1.8034623103692924e-05,
      "loss": 0.0366,
      "step": 220
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.35374191403388977,
      "learning_rate": 1.810976538629818e-05,
      "loss": 0.0372,
      "step": 225
    },
    {
      "epoch": 0.2787878787878788,
      "grad_norm": 0.29907068610191345,
      "learning_rate": 1.8183256055703488e-05,
      "loss": 0.0365,
      "step": 230
    },
    {
      "epoch": 0.28484848484848485,
      "grad_norm": 0.6014831066131592,
      "learning_rate": 1.8255166157433265e-05,
      "loss": 0.0362,
      "step": 235
    },
    {
      "epoch": 0.2909090909090909,
      "grad_norm": 0.5096117258071899,
      "learning_rate": 1.8325562249241182e-05,
      "loss": 0.034,
      "step": 240
    },
    {
      "epoch": 0.296969696969697,
      "grad_norm": 0.5617567300796509,
      "learning_rate": 1.839450677130401e-05,
      "loss": 0.0347,
      "step": 245
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 0.3558465540409088,
      "learning_rate": 1.8462058379014124e-05,
      "loss": 0.0351,
      "step": 250
    },
    {
      "epoch": 0.3090909090909091,
      "grad_norm": 0.71063631772995,
      "learning_rate": 1.8528272242815496e-05,
      "loss": 0.0366,
      "step": 255
    },
    {
      "epoch": 0.3151515151515151,
      "grad_norm": 0.5253021717071533,
      "learning_rate": 1.8593200318923534e-05,
      "loss": 0.0366,
      "step": 260
    },
    {
      "epoch": 0.3212121212121212,
      "grad_norm": 0.40601053833961487,
      "learning_rate": 1.8656891594257228e-05,
      "loss": 0.0345,
      "step": 265
    },
    {
      "epoch": 0.32727272727272727,
      "grad_norm": 0.5746368169784546,
      "learning_rate": 1.8719392308476227e-05,
      "loss": 0.0365,
      "step": 270
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.5711379647254944,
      "learning_rate": 1.878074615564391e-05,
      "loss": 0.0361,
      "step": 275
    },
    {
      "epoch": 0.3393939393939394,
      "grad_norm": 0.6803319454193115,
      "learning_rate": 1.8840994467719588e-05,
      "loss": 0.0353,
      "step": 280
    },
    {
      "epoch": 0.34545454545454546,
      "grad_norm": 0.3054657280445099,
      "learning_rate": 1.8900176381809734e-05,
      "loss": 0.0361,
      "step": 285
    },
    {
      "epoch": 0.3515151515151515,
      "grad_norm": 0.2711184620857239,
      "learning_rate": 1.8958328992873333e-05,
      "loss": 0.0339,
      "step": 290
    },
    {
      "epoch": 0.3575757575757576,
      "grad_norm": 0.31115850806236267,
      "learning_rate": 1.9015487493373553e-05,
      "loss": 0.0348,
      "step": 295
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.33295345306396484,
      "learning_rate": 1.9071685301192168e-05,
      "loss": 0.035,
      "step": 300
    },
    {
      "epoch": 0.3696969696969697,
      "grad_norm": 0.3145270049571991,
      "learning_rate": 1.912695417697111e-05,
      "loss": 0.0349,
      "step": 305
    },
    {
      "epoch": 0.37575757575757573,
      "grad_norm": 0.4976232051849365,
      "learning_rate": 1.9181324331912677e-05,
      "loss": 0.037,
      "step": 310
    },
    {
      "epoch": 0.38181818181818183,
      "grad_norm": 0.545088529586792,
      "learning_rate": 1.9234824526954633e-05,
      "loss": 0.0343,
      "step": 315
    },
    {
      "epoch": 0.3878787878787879,
      "grad_norm": 0.3774203956127167,
      "learning_rate": 1.9287482164135173e-05,
      "loss": 0.0329,
      "step": 320
    },
    {
      "epoch": 0.3939393939393939,
      "grad_norm": 0.3258519470691681,
      "learning_rate": 1.933932337087452e-05,
      "loss": 0.0336,
      "step": 325
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.31861215829849243,
      "learning_rate": 1.9390373077821954e-05,
      "loss": 0.0354,
      "step": 330
    },
    {
      "epoch": 0.40606060606060607,
      "grad_norm": 0.2723672389984131,
      "learning_rate": 1.944065509084906e-05,
      "loss": 0.0339,
      "step": 335
    },
    {
      "epoch": 0.4121212121212121,
      "grad_norm": 0.48070028424263,
      "learning_rate": 1.9490192157709488e-05,
      "loss": 0.0333,
      "step": 340
    },
    {
      "epoch": 0.41818181818181815,
      "grad_norm": 0.459739625453949,
      "learning_rate": 1.9539006029832518e-05,
      "loss": 0.0343,
      "step": 345
    },
    {
      "epoch": 0.42424242424242425,
      "grad_norm": 0.3008536398410797,
      "learning_rate": 1.9587117519670574e-05,
      "loss": 0.0353,
      "step": 350
    },
    {
      "epoch": 0.4303030303030303,
      "grad_norm": 0.38889071345329285,
      "learning_rate": 1.963454655397911e-05,
      "loss": 0.0341,
      "step": 355
    },
    {
      "epoch": 0.43636363636363634,
      "grad_norm": 0.4915270209312439,
      "learning_rate": 1.968131222337022e-05,
      "loss": 0.0353,
      "step": 360
    },
    {
      "epoch": 0.44242424242424244,
      "grad_norm": 0.4057636559009552,
      "learning_rate": 1.9727432828448466e-05,
      "loss": 0.0345,
      "step": 365
    },
    {
      "epoch": 0.4484848484848485,
      "grad_norm": 0.32680678367614746,
      "learning_rate": 1.977292592280792e-05,
      "loss": 0.0332,
      "step": 370
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.2668668329715729,
      "learning_rate": 1.9817808353143154e-05,
      "loss": 0.0315,
      "step": 375
    },
    {
      "epoch": 0.46060606060606063,
      "grad_norm": 0.26270952820777893,
      "learning_rate": 1.9862096296703722e-05,
      "loss": 0.0338,
      "step": 380
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.2758481800556183,
      "learning_rate": 1.9905805296300364e-05,
      "loss": 0.0331,
      "step": 385
    },
    {
      "epoch": 0.4727272727272727,
      "grad_norm": 0.2486269325017929,
      "learning_rate": 1.9948950293052568e-05,
      "loss": 0.0336,
      "step": 390
    },
    {
      "epoch": 0.47878787878787876,
      "grad_norm": 0.43292659521102905,
      "learning_rate": 1.999154565705013e-05,
      "loss": 0.0336,
      "step": 395
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 0.32967817783355713,
      "learning_rate": 1.999368686868687e-05,
      "loss": 0.0333,
      "step": 400
    },
    {
      "epoch": 0.4909090909090909,
      "grad_norm": 0.29001298546791077,
      "learning_rate": 1.9983164983164986e-05,
      "loss": 0.0321,
      "step": 405
    },
    {
      "epoch": 0.49696969696969695,
      "grad_norm": 0.32173898816108704,
      "learning_rate": 1.99726430976431e-05,
      "loss": 0.0325,
      "step": 410
    },
    {
      "epoch": 0.4993939393939394,
      "eval_average": 0.5791917974219782,
      "eval_crossner_ai": 0.520146520096587,
      "eval_crossner_literature": 0.5606407322153182,
      "eval_crossner_music": 0.7170349251102842,
      "eval_crossner_politics": 0.6584967319760383,
      "eval_crossner_science": 0.6681034482257635,
      "eval_mit-movie": 0.5561497325704369,
      "eval_mit-restaurant": 0.3737704917594195,
      "eval_runtime": 20.8165,
      "eval_samples_per_second": 33.627,
      "eval_steps_per_second": 0.336,
      "step": 412
    },
    {
      "epoch": 0.503030303030303,
      "grad_norm": 0.35715827345848083,
      "learning_rate": 1.996212121212121e-05,
      "loss": 0.0324,
      "step": 415
    },
    {
      "epoch": 0.509090909090909,
      "grad_norm": 0.2450762689113617,
      "learning_rate": 1.9951599326599327e-05,
      "loss": 0.0323,
      "step": 420
    },
    {
      "epoch": 0.5151515151515151,
      "grad_norm": 0.29769468307495117,
      "learning_rate": 1.9941077441077444e-05,
      "loss": 0.0327,
      "step": 425
    },
    {
      "epoch": 0.5212121212121212,
      "grad_norm": 0.3731805384159088,
      "learning_rate": 1.9930555555555556e-05,
      "loss": 0.0333,
      "step": 430
    },
    {
      "epoch": 0.5272727272727272,
      "grad_norm": 0.4432564377784729,
      "learning_rate": 1.9920033670033672e-05,
      "loss": 0.0333,
      "step": 435
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.31727364659309387,
      "learning_rate": 1.9909511784511785e-05,
      "loss": 0.0332,
      "step": 440
    },
    {
      "epoch": 0.5393939393939394,
      "grad_norm": 0.5506933331489563,
      "learning_rate": 1.98989898989899e-05,
      "loss": 0.0339,
      "step": 445
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.2608805000782013,
      "learning_rate": 1.9888468013468014e-05,
      "loss": 0.0314,
      "step": 450
    },
    {
      "epoch": 0.5515151515151515,
      "grad_norm": 0.28970998525619507,
      "learning_rate": 1.987794612794613e-05,
      "loss": 0.0324,
      "step": 455
    },
    {
      "epoch": 0.5575757575757576,
      "grad_norm": 0.2809615731239319,
      "learning_rate": 1.9867424242424246e-05,
      "loss": 0.0304,
      "step": 460
    },
    {
      "epoch": 0.5636363636363636,
      "grad_norm": 0.3160002827644348,
      "learning_rate": 1.985690235690236e-05,
      "loss": 0.0298,
      "step": 465
    },
    {
      "epoch": 0.5696969696969697,
      "grad_norm": 0.5111832022666931,
      "learning_rate": 1.9846380471380475e-05,
      "loss": 0.034,
      "step": 470
    },
    {
      "epoch": 0.5757575757575758,
      "grad_norm": 0.326456755399704,
      "learning_rate": 1.9835858585858587e-05,
      "loss": 0.0325,
      "step": 475
    },
    {
      "epoch": 0.5818181818181818,
      "grad_norm": 0.42159873247146606,
      "learning_rate": 1.98253367003367e-05,
      "loss": 0.0322,
      "step": 480
    },
    {
      "epoch": 0.5878787878787879,
      "grad_norm": 0.29852959513664246,
      "learning_rate": 1.9814814814814816e-05,
      "loss": 0.0322,
      "step": 485
    },
    {
      "epoch": 0.593939393939394,
      "grad_norm": 0.2802867889404297,
      "learning_rate": 1.9804292929292932e-05,
      "loss": 0.0331,
      "step": 490
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.2572622001171112,
      "learning_rate": 1.9793771043771045e-05,
      "loss": 0.0313,
      "step": 495
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 0.26064616441726685,
      "learning_rate": 1.978324915824916e-05,
      "loss": 0.033,
      "step": 500
    },
    {
      "epoch": 0.6121212121212121,
      "grad_norm": 0.3742619752883911,
      "learning_rate": 1.9772727272727274e-05,
      "loss": 0.0312,
      "step": 505
    },
    {
      "epoch": 0.6181818181818182,
      "grad_norm": 0.5433458089828491,
      "learning_rate": 1.976220538720539e-05,
      "loss": 0.0328,
      "step": 510
    },
    {
      "epoch": 0.6242424242424243,
      "grad_norm": 0.26670128107070923,
      "learning_rate": 1.9751683501683503e-05,
      "loss": 0.0303,
      "step": 515
    },
    {
      "epoch": 0.6303030303030303,
      "grad_norm": 0.23925013840198517,
      "learning_rate": 1.974116161616162e-05,
      "loss": 0.0306,
      "step": 520
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 0.3795340061187744,
      "learning_rate": 1.973063973063973e-05,
      "loss": 0.0306,
      "step": 525
    },
    {
      "epoch": 0.6424242424242425,
      "grad_norm": 0.407589852809906,
      "learning_rate": 1.9720117845117847e-05,
      "loss": 0.0309,
      "step": 530
    },
    {
      "epoch": 0.6484848484848484,
      "grad_norm": 0.4038148820400238,
      "learning_rate": 1.9709595959595963e-05,
      "loss": 0.032,
      "step": 535
    },
    {
      "epoch": 0.6545454545454545,
      "grad_norm": 0.27322643995285034,
      "learning_rate": 1.9699074074074076e-05,
      "loss": 0.0305,
      "step": 540
    },
    {
      "epoch": 0.6606060606060606,
      "grad_norm": 0.25489315390586853,
      "learning_rate": 1.968855218855219e-05,
      "loss": 0.0309,
      "step": 545
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.5546852946281433,
      "learning_rate": 1.9678030303030305e-05,
      "loss": 0.0317,
      "step": 550
    },
    {
      "epoch": 0.6727272727272727,
      "grad_norm": 0.29473018646240234,
      "learning_rate": 1.9667508417508418e-05,
      "loss": 0.0319,
      "step": 555
    },
    {
      "epoch": 0.6787878787878788,
      "grad_norm": 0.2568839192390442,
      "learning_rate": 1.9656986531986534e-05,
      "loss": 0.0308,
      "step": 560
    },
    {
      "epoch": 0.6848484848484848,
      "grad_norm": 0.32141977548599243,
      "learning_rate": 1.964646464646465e-05,
      "loss": 0.0321,
      "step": 565
    },
    {
      "epoch": 0.6909090909090909,
      "grad_norm": 0.24373693764209747,
      "learning_rate": 1.9635942760942763e-05,
      "loss": 0.0306,
      "step": 570
    },
    {
      "epoch": 0.696969696969697,
      "grad_norm": 0.2891671359539032,
      "learning_rate": 1.962542087542088e-05,
      "loss": 0.032,
      "step": 575
    },
    {
      "epoch": 0.703030303030303,
      "grad_norm": 0.5750558376312256,
      "learning_rate": 1.961489898989899e-05,
      "loss": 0.0324,
      "step": 580
    },
    {
      "epoch": 0.7090909090909091,
      "grad_norm": 0.24102090299129486,
      "learning_rate": 1.9604377104377107e-05,
      "loss": 0.0317,
      "step": 585
    },
    {
      "epoch": 0.7151515151515152,
      "grad_norm": 0.31139931082725525,
      "learning_rate": 1.959385521885522e-05,
      "loss": 0.0319,
      "step": 590
    },
    {
      "epoch": 0.7212121212121212,
      "grad_norm": 0.38392558693885803,
      "learning_rate": 1.9583333333333333e-05,
      "loss": 0.0317,
      "step": 595
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.35479679703712463,
      "learning_rate": 1.957281144781145e-05,
      "loss": 0.0321,
      "step": 600
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.26235559582710266,
      "learning_rate": 1.9562289562289565e-05,
      "loss": 0.03,
      "step": 605
    },
    {
      "epoch": 0.7393939393939394,
      "grad_norm": 0.41770321130752563,
      "learning_rate": 1.9551767676767678e-05,
      "loss": 0.0333,
      "step": 610
    },
    {
      "epoch": 0.7454545454545455,
      "grad_norm": 0.3011866807937622,
      "learning_rate": 1.9541245791245794e-05,
      "loss": 0.0311,
      "step": 615
    },
    {
      "epoch": 0.7515151515151515,
      "grad_norm": 0.23577293753623962,
      "learning_rate": 1.9530723905723906e-05,
      "loss": 0.0293,
      "step": 620
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 0.2772422730922699,
      "learning_rate": 1.9520202020202022e-05,
      "loss": 0.0313,
      "step": 625
    },
    {
      "epoch": 0.7636363636363637,
      "grad_norm": 0.32617759704589844,
      "learning_rate": 1.9509680134680135e-05,
      "loss": 0.0327,
      "step": 630
    },
    {
      "epoch": 0.7696969696969697,
      "grad_norm": 0.2570483088493347,
      "learning_rate": 1.949915824915825e-05,
      "loss": 0.0306,
      "step": 635
    },
    {
      "epoch": 0.7757575757575758,
      "grad_norm": 0.375601202249527,
      "learning_rate": 1.9488636363636367e-05,
      "loss": 0.0301,
      "step": 640
    },
    {
      "epoch": 0.7818181818181819,
      "grad_norm": 0.3407788872718811,
      "learning_rate": 1.947811447811448e-05,
      "loss": 0.0328,
      "step": 645
    },
    {
      "epoch": 0.7878787878787878,
      "grad_norm": 0.20373927056789398,
      "learning_rate": 1.9467592592592596e-05,
      "loss": 0.0305,
      "step": 650
    },
    {
      "epoch": 0.793939393939394,
      "grad_norm": 0.25077903270721436,
      "learning_rate": 1.945707070707071e-05,
      "loss": 0.0308,
      "step": 655
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.27440720796585083,
      "learning_rate": 1.944654882154882e-05,
      "loss": 0.0314,
      "step": 660
    },
    {
      "epoch": 0.806060606060606,
      "grad_norm": 0.34862589836120605,
      "learning_rate": 1.9436026936026938e-05,
      "loss": 0.0313,
      "step": 665
    },
    {
      "epoch": 0.8121212121212121,
      "grad_norm": 0.27242010831832886,
      "learning_rate": 1.942550505050505e-05,
      "loss": 0.0327,
      "step": 670
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 0.2974311411380768,
      "learning_rate": 1.9414983164983166e-05,
      "loss": 0.0314,
      "step": 675
    },
    {
      "epoch": 0.8242424242424242,
      "grad_norm": 0.2687873840332031,
      "learning_rate": 1.9404461279461282e-05,
      "loss": 0.0308,
      "step": 680
    },
    {
      "epoch": 0.8303030303030303,
      "grad_norm": 0.32290545105934143,
      "learning_rate": 1.9393939393939395e-05,
      "loss": 0.0312,
      "step": 685
    },
    {
      "epoch": 0.8363636363636363,
      "grad_norm": 0.2659580707550049,
      "learning_rate": 1.938341750841751e-05,
      "loss": 0.0301,
      "step": 690
    },
    {
      "epoch": 0.8424242424242424,
      "grad_norm": 0.26104384660720825,
      "learning_rate": 1.9372895622895624e-05,
      "loss": 0.0313,
      "step": 695
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 0.2625015676021576,
      "learning_rate": 1.936237373737374e-05,
      "loss": 0.0306,
      "step": 700
    },
    {
      "epoch": 0.8545454545454545,
      "grad_norm": 0.21909096837043762,
      "learning_rate": 1.9351851851851853e-05,
      "loss": 0.0308,
      "step": 705
    },
    {
      "epoch": 0.8606060606060606,
      "grad_norm": 0.6454488039016724,
      "learning_rate": 1.9341329966329965e-05,
      "loss": 0.0299,
      "step": 710
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.24899975955486298,
      "learning_rate": 1.9330808080808085e-05,
      "loss": 0.029,
      "step": 715
    },
    {
      "epoch": 0.8727272727272727,
      "grad_norm": 0.28456464409828186,
      "learning_rate": 1.9320286195286198e-05,
      "loss": 0.0319,
      "step": 720
    },
    {
      "epoch": 0.8787878787878788,
      "grad_norm": 0.28144142031669617,
      "learning_rate": 1.930976430976431e-05,
      "loss": 0.0292,
      "step": 725
    },
    {
      "epoch": 0.8848484848484849,
      "grad_norm": 0.3047611713409424,
      "learning_rate": 1.9299242424242426e-05,
      "loss": 0.0316,
      "step": 730
    },
    {
      "epoch": 0.8909090909090909,
      "grad_norm": 0.38740065693855286,
      "learning_rate": 1.928872053872054e-05,
      "loss": 0.0307,
      "step": 735
    },
    {
      "epoch": 0.896969696969697,
      "grad_norm": 0.453657865524292,
      "learning_rate": 1.9278198653198655e-05,
      "loss": 0.0302,
      "step": 740
    },
    {
      "epoch": 0.9030303030303031,
      "grad_norm": 0.30072638392448425,
      "learning_rate": 1.9267676767676768e-05,
      "loss": 0.0317,
      "step": 745
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.3334069848060608,
      "learning_rate": 1.9257154882154884e-05,
      "loss": 0.0297,
      "step": 750
    },
    {
      "epoch": 0.9151515151515152,
      "grad_norm": 0.411685973405838,
      "learning_rate": 1.9246632996633e-05,
      "loss": 0.0294,
      "step": 755
    },
    {
      "epoch": 0.9212121212121213,
      "grad_norm": 0.5461416244506836,
      "learning_rate": 1.9236111111111113e-05,
      "loss": 0.0321,
      "step": 760
    },
    {
      "epoch": 0.9272727272727272,
      "grad_norm": 0.3038751482963562,
      "learning_rate": 1.922558922558923e-05,
      "loss": 0.0296,
      "step": 765
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.3308413624763489,
      "learning_rate": 1.921506734006734e-05,
      "loss": 0.0317,
      "step": 770
    },
    {
      "epoch": 0.9393939393939394,
      "grad_norm": 0.3853442370891571,
      "learning_rate": 1.9204545454545454e-05,
      "loss": 0.0283,
      "step": 775
    },
    {
      "epoch": 0.9454545454545454,
      "grad_norm": 0.2355109006166458,
      "learning_rate": 1.919402356902357e-05,
      "loss": 0.0304,
      "step": 780
    },
    {
      "epoch": 0.9515151515151515,
      "grad_norm": 0.25554534792900085,
      "learning_rate": 1.9183501683501683e-05,
      "loss": 0.0311,
      "step": 785
    },
    {
      "epoch": 0.9575757575757575,
      "grad_norm": 0.38685283064842224,
      "learning_rate": 1.91729797979798e-05,
      "loss": 0.0317,
      "step": 790
    },
    {
      "epoch": 0.9636363636363636,
      "grad_norm": 0.41898226737976074,
      "learning_rate": 1.9162457912457915e-05,
      "loss": 0.0304,
      "step": 795
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 0.27706390619277954,
      "learning_rate": 1.9151936026936028e-05,
      "loss": 0.0306,
      "step": 800
    },
    {
      "epoch": 0.9757575757575757,
      "grad_norm": 0.257541686296463,
      "learning_rate": 1.9141414141414144e-05,
      "loss": 0.0306,
      "step": 805
    },
    {
      "epoch": 0.9818181818181818,
      "grad_norm": 0.3957936465740204,
      "learning_rate": 1.9130892255892257e-05,
      "loss": 0.0297,
      "step": 810
    },
    {
      "epoch": 0.9878787878787879,
      "grad_norm": 0.22699956595897675,
      "learning_rate": 1.9120370370370373e-05,
      "loss": 0.0299,
      "step": 815
    },
    {
      "epoch": 0.9939393939393939,
      "grad_norm": 0.24206435680389404,
      "learning_rate": 1.9109848484848485e-05,
      "loss": 0.0302,
      "step": 820
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.2516419291496277,
      "learning_rate": 1.90993265993266e-05,
      "loss": 0.0311,
      "step": 825
    },
    {
      "epoch": 1.0,
      "eval_average": 0.5796812654551271,
      "eval_crossner_ai": 0.5594771241328611,
      "eval_crossner_literature": 0.5849056603272751,
      "eval_crossner_music": 0.6920315864538805,
      "eval_crossner_politics": 0.6281945588946617,
      "eval_crossner_science": 0.6608315097966942,
      "eval_mit-movie": 0.5577464788240397,
      "eval_mit-restaurant": 0.374581939756477,
      "eval_runtime": 20.7135,
      "eval_samples_per_second": 33.794,
      "eval_steps_per_second": 0.338,
      "step": 825
    },
    {
      "epoch": 1.006060606060606,
      "grad_norm": 0.4400379955768585,
      "learning_rate": 1.9090909090909094e-05,
      "loss": 0.0255,
      "step": 830
    },
    {
      "epoch": 1.0121212121212122,
      "grad_norm": 0.2648255527019501,
      "learning_rate": 1.9080387205387206e-05,
      "loss": 0.0225,
      "step": 835
    },
    {
      "epoch": 1.018181818181818,
      "grad_norm": 0.4055320620536804,
      "learning_rate": 1.9069865319865322e-05,
      "loss": 0.021,
      "step": 840
    },
    {
      "epoch": 1.0242424242424242,
      "grad_norm": 0.2593342661857605,
      "learning_rate": 1.9059343434343435e-05,
      "loss": 0.0221,
      "step": 845
    },
    {
      "epoch": 1.0303030303030303,
      "grad_norm": 0.35960671305656433,
      "learning_rate": 1.904882154882155e-05,
      "loss": 0.0229,
      "step": 850
    },
    {
      "epoch": 1.0363636363636364,
      "grad_norm": 0.3963358700275421,
      "learning_rate": 1.9038299663299664e-05,
      "loss": 0.0221,
      "step": 855
    },
    {
      "epoch": 1.0424242424242425,
      "grad_norm": 0.4948763847351074,
      "learning_rate": 1.902777777777778e-05,
      "loss": 0.0217,
      "step": 860
    },
    {
      "epoch": 1.0484848484848486,
      "grad_norm": 0.4198768734931946,
      "learning_rate": 1.9017255892255896e-05,
      "loss": 0.0219,
      "step": 865
    },
    {
      "epoch": 1.0545454545454545,
      "grad_norm": 0.4277552366256714,
      "learning_rate": 1.900673400673401e-05,
      "loss": 0.0225,
      "step": 870
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 0.3041873276233673,
      "learning_rate": 1.8996212121212125e-05,
      "loss": 0.0219,
      "step": 875
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.2769505977630615,
      "learning_rate": 1.8985690235690237e-05,
      "loss": 0.0227,
      "step": 880
    },
    {
      "epoch": 1.0727272727272728,
      "grad_norm": 0.23667094111442566,
      "learning_rate": 1.897516835016835e-05,
      "loss": 0.0213,
      "step": 885
    },
    {
      "epoch": 1.0787878787878789,
      "grad_norm": 0.43135538697242737,
      "learning_rate": 1.8964646464646466e-05,
      "loss": 0.0233,
      "step": 890
    },
    {
      "epoch": 1.084848484848485,
      "grad_norm": 0.32635658979415894,
      "learning_rate": 1.895412457912458e-05,
      "loss": 0.0219,
      "step": 895
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 0.40671712160110474,
      "learning_rate": 1.8943602693602695e-05,
      "loss": 0.0216,
      "step": 900
    },
    {
      "epoch": 1.096969696969697,
      "grad_norm": 0.360250860452652,
      "learning_rate": 1.893308080808081e-05,
      "loss": 0.0218,
      "step": 905
    },
    {
      "epoch": 1.103030303030303,
      "grad_norm": 0.3280063569545746,
      "learning_rate": 1.8922558922558924e-05,
      "loss": 0.0227,
      "step": 910
    },
    {
      "epoch": 1.1090909090909091,
      "grad_norm": 0.28214868903160095,
      "learning_rate": 1.891203703703704e-05,
      "loss": 0.0225,
      "step": 915
    },
    {
      "epoch": 1.1151515151515152,
      "grad_norm": 0.38145509362220764,
      "learning_rate": 1.8901515151515153e-05,
      "loss": 0.0215,
      "step": 920
    },
    {
      "epoch": 1.121212121212121,
      "grad_norm": 0.22794072329998016,
      "learning_rate": 1.889099326599327e-05,
      "loss": 0.0231,
      "step": 925
    },
    {
      "epoch": 1.1272727272727272,
      "grad_norm": 0.22696715593338013,
      "learning_rate": 1.888047138047138e-05,
      "loss": 0.0211,
      "step": 930
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.2450752705335617,
      "learning_rate": 1.8869949494949494e-05,
      "loss": 0.0224,
      "step": 935
    },
    {
      "epoch": 1.1393939393939394,
      "grad_norm": 0.2649388909339905,
      "learning_rate": 1.8859427609427613e-05,
      "loss": 0.0222,
      "step": 940
    },
    {
      "epoch": 1.1454545454545455,
      "grad_norm": 0.2956222891807556,
      "learning_rate": 1.8848905723905726e-05,
      "loss": 0.0224,
      "step": 945
    },
    {
      "epoch": 1.1515151515151516,
      "grad_norm": 0.22416982054710388,
      "learning_rate": 1.883838383838384e-05,
      "loss": 0.0214,
      "step": 950
    },
    {
      "epoch": 1.1575757575757575,
      "grad_norm": 0.22547370195388794,
      "learning_rate": 1.8827861952861955e-05,
      "loss": 0.0202,
      "step": 955
    },
    {
      "epoch": 1.1636363636363636,
      "grad_norm": 0.2777990996837616,
      "learning_rate": 1.8817340067340068e-05,
      "loss": 0.0233,
      "step": 960
    },
    {
      "epoch": 1.1696969696969697,
      "grad_norm": 0.3015587031841278,
      "learning_rate": 1.8806818181818184e-05,
      "loss": 0.0223,
      "step": 965
    },
    {
      "epoch": 1.1757575757575758,
      "grad_norm": 0.221028670668602,
      "learning_rate": 1.8796296296296296e-05,
      "loss": 0.0225,
      "step": 970
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 0.27499160170555115,
      "learning_rate": 1.8785774410774412e-05,
      "loss": 0.0225,
      "step": 975
    },
    {
      "epoch": 1.187878787878788,
      "grad_norm": 0.3166850805282593,
      "learning_rate": 1.877525252525253e-05,
      "loss": 0.0216,
      "step": 980
    },
    {
      "epoch": 1.1939393939393939,
      "grad_norm": 0.24268050491809845,
      "learning_rate": 1.876473063973064e-05,
      "loss": 0.0218,
      "step": 985
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.2744690179824829,
      "learning_rate": 1.8754208754208757e-05,
      "loss": 0.0221,
      "step": 990
    },
    {
      "epoch": 1.206060606060606,
      "grad_norm": 0.362761914730072,
      "learning_rate": 1.874368686868687e-05,
      "loss": 0.0205,
      "step": 995
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 0.4049486219882965,
      "learning_rate": 1.8733164983164983e-05,
      "loss": 0.0223,
      "step": 1000
    },
    {
      "epoch": 1.2181818181818183,
      "grad_norm": 0.3078119456768036,
      "learning_rate": 1.87226430976431e-05,
      "loss": 0.0219,
      "step": 1005
    },
    {
      "epoch": 1.2242424242424241,
      "grad_norm": 0.3626125454902649,
      "learning_rate": 1.871212121212121e-05,
      "loss": 0.0226,
      "step": 1010
    },
    {
      "epoch": 1.2303030303030302,
      "grad_norm": 0.2282802313566208,
      "learning_rate": 1.8701599326599328e-05,
      "loss": 0.0223,
      "step": 1015
    },
    {
      "epoch": 1.2363636363636363,
      "grad_norm": 0.25088784098625183,
      "learning_rate": 1.8691077441077444e-05,
      "loss": 0.0205,
      "step": 1020
    },
    {
      "epoch": 1.2424242424242424,
      "grad_norm": 0.251303106546402,
      "learning_rate": 1.8680555555555556e-05,
      "loss": 0.023,
      "step": 1025
    },
    {
      "epoch": 1.2484848484848485,
      "grad_norm": 0.30349820852279663,
      "learning_rate": 1.8670033670033672e-05,
      "loss": 0.0225,
      "step": 1030
    },
    {
      "epoch": 1.2545454545454546,
      "grad_norm": 0.27469730377197266,
      "learning_rate": 1.8659511784511785e-05,
      "loss": 0.0223,
      "step": 1035
    },
    {
      "epoch": 1.2606060606060605,
      "grad_norm": 0.30436843633651733,
      "learning_rate": 1.86489898989899e-05,
      "loss": 0.0216,
      "step": 1040
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.3041312098503113,
      "learning_rate": 1.8638468013468014e-05,
      "loss": 0.0221,
      "step": 1045
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 0.2689203917980194,
      "learning_rate": 1.862794612794613e-05,
      "loss": 0.022,
      "step": 1050
    },
    {
      "epoch": 1.2787878787878788,
      "grad_norm": 0.29724082350730896,
      "learning_rate": 1.8617424242424246e-05,
      "loss": 0.0213,
      "step": 1055
    },
    {
      "epoch": 1.284848484848485,
      "grad_norm": 0.3023717403411865,
      "learning_rate": 1.860690235690236e-05,
      "loss": 0.0226,
      "step": 1060
    },
    {
      "epoch": 1.290909090909091,
      "grad_norm": 0.2631073296070099,
      "learning_rate": 1.859638047138047e-05,
      "loss": 0.0217,
      "step": 1065
    },
    {
      "epoch": 1.2969696969696969,
      "grad_norm": 0.23508770763874054,
      "learning_rate": 1.8585858585858588e-05,
      "loss": 0.0229,
      "step": 1070
    },
    {
      "epoch": 1.303030303030303,
      "grad_norm": 0.2794710099697113,
      "learning_rate": 1.85753367003367e-05,
      "loss": 0.0224,
      "step": 1075
    },
    {
      "epoch": 1.309090909090909,
      "grad_norm": 0.48751217126846313,
      "learning_rate": 1.8564814814814816e-05,
      "loss": 0.0219,
      "step": 1080
    },
    {
      "epoch": 1.3151515151515152,
      "grad_norm": 0.3470744490623474,
      "learning_rate": 1.8554292929292932e-05,
      "loss": 0.0225,
      "step": 1085
    },
    {
      "epoch": 1.3212121212121213,
      "grad_norm": 0.3293347656726837,
      "learning_rate": 1.8543771043771045e-05,
      "loss": 0.0215,
      "step": 1090
    },
    {
      "epoch": 1.3272727272727274,
      "grad_norm": 0.28373968601226807,
      "learning_rate": 1.853324915824916e-05,
      "loss": 0.0227,
      "step": 1095
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.31946295499801636,
      "learning_rate": 1.8522727272727274e-05,
      "loss": 0.0231,
      "step": 1100
    },
    {
      "epoch": 1.3393939393939394,
      "grad_norm": 0.25139912962913513,
      "learning_rate": 1.851220538720539e-05,
      "loss": 0.0227,
      "step": 1105
    },
    {
      "epoch": 1.3454545454545455,
      "grad_norm": 0.31972330808639526,
      "learning_rate": 1.8501683501683503e-05,
      "loss": 0.0217,
      "step": 1110
    },
    {
      "epoch": 1.3515151515151516,
      "grad_norm": 0.5395520925521851,
      "learning_rate": 1.8491161616161615e-05,
      "loss": 0.0215,
      "step": 1115
    },
    {
      "epoch": 1.3575757575757577,
      "grad_norm": 0.40440139174461365,
      "learning_rate": 1.848063973063973e-05,
      "loss": 0.0235,
      "step": 1120
    },
    {
      "epoch": 1.3636363636363635,
      "grad_norm": 0.26157671213150024,
      "learning_rate": 1.8470117845117848e-05,
      "loss": 0.0208,
      "step": 1125
    },
    {
      "epoch": 1.3696969696969696,
      "grad_norm": 0.2564912736415863,
      "learning_rate": 1.845959595959596e-05,
      "loss": 0.0234,
      "step": 1130
    },
    {
      "epoch": 1.3757575757575757,
      "grad_norm": 0.2972109913825989,
      "learning_rate": 1.8449074074074076e-05,
      "loss": 0.0222,
      "step": 1135
    },
    {
      "epoch": 1.3818181818181818,
      "grad_norm": 0.25731217861175537,
      "learning_rate": 1.843855218855219e-05,
      "loss": 0.0224,
      "step": 1140
    },
    {
      "epoch": 1.387878787878788,
      "grad_norm": 0.21306392550468445,
      "learning_rate": 1.8428030303030305e-05,
      "loss": 0.0221,
      "step": 1145
    },
    {
      "epoch": 1.393939393939394,
      "grad_norm": 0.2847110629081726,
      "learning_rate": 1.8417508417508418e-05,
      "loss": 0.0218,
      "step": 1150
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.2902642488479614,
      "learning_rate": 1.8406986531986534e-05,
      "loss": 0.0211,
      "step": 1155
    },
    {
      "epoch": 1.406060606060606,
      "grad_norm": 0.2736600935459137,
      "learning_rate": 1.839646464646465e-05,
      "loss": 0.0233,
      "step": 1160
    },
    {
      "epoch": 1.412121212121212,
      "grad_norm": 0.2298624962568283,
      "learning_rate": 1.8385942760942763e-05,
      "loss": 0.0225,
      "step": 1165
    },
    {
      "epoch": 1.4181818181818182,
      "grad_norm": 0.22921550273895264,
      "learning_rate": 1.837542087542088e-05,
      "loss": 0.021,
      "step": 1170
    },
    {
      "epoch": 1.4242424242424243,
      "grad_norm": 0.255136102437973,
      "learning_rate": 1.836489898989899e-05,
      "loss": 0.0228,
      "step": 1175
    },
    {
      "epoch": 1.4303030303030304,
      "grad_norm": 0.3466205298900604,
      "learning_rate": 1.8354377104377104e-05,
      "loss": 0.0221,
      "step": 1180
    },
    {
      "epoch": 1.4363636363636363,
      "grad_norm": 0.26402780413627625,
      "learning_rate": 1.834385521885522e-05,
      "loss": 0.0225,
      "step": 1185
    },
    {
      "epoch": 1.4424242424242424,
      "grad_norm": 0.5900474786758423,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0215,
      "step": 1190
    },
    {
      "epoch": 1.4484848484848485,
      "grad_norm": 0.317525178194046,
      "learning_rate": 1.832281144781145e-05,
      "loss": 0.0231,
      "step": 1195
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 0.3154687285423279,
      "learning_rate": 1.8312289562289565e-05,
      "loss": 0.0236,
      "step": 1200
    },
    {
      "epoch": 1.4606060606060607,
      "grad_norm": 0.2829839885234833,
      "learning_rate": 1.8301767676767678e-05,
      "loss": 0.0223,
      "step": 1205
    },
    {
      "epoch": 1.4666666666666666,
      "grad_norm": 0.25394293665885925,
      "learning_rate": 1.8291245791245794e-05,
      "loss": 0.0214,
      "step": 1210
    },
    {
      "epoch": 1.4727272727272727,
      "grad_norm": 0.410250723361969,
      "learning_rate": 1.8280723905723906e-05,
      "loss": 0.0217,
      "step": 1215
    },
    {
      "epoch": 1.4787878787878788,
      "grad_norm": 0.5648049116134644,
      "learning_rate": 1.8270202020202023e-05,
      "loss": 0.0219,
      "step": 1220
    },
    {
      "epoch": 1.4848484848484849,
      "grad_norm": 0.2602587044239044,
      "learning_rate": 1.8259680134680135e-05,
      "loss": 0.0215,
      "step": 1225
    },
    {
      "epoch": 1.490909090909091,
      "grad_norm": 0.20647259056568146,
      "learning_rate": 1.824915824915825e-05,
      "loss": 0.0224,
      "step": 1230
    },
    {
      "epoch": 1.496969696969697,
      "grad_norm": 0.23807981610298157,
      "learning_rate": 1.8238636363636367e-05,
      "loss": 0.0219,
      "step": 1235
    },
    {
      "epoch": 1.5006060606060605,
      "eval_average": 0.5624935483503481,
      "eval_crossner_ai": 0.505494505444576,
      "eval_crossner_literature": 0.5803468207591225,
      "eval_crossner_music": 0.6979388769932665,
      "eval_crossner_politics": 0.6214689265035891,
      "eval_crossner_science": 0.5559174809488194,
      "eval_mit-movie": 0.5246753246252305,
      "eval_mit-restaurant": 0.45161290317783304,
      "eval_runtime": 21.134,
      "eval_samples_per_second": 33.122,
      "eval_steps_per_second": 0.331,
      "step": 1238
    },
    {
      "epoch": 1.503030303030303,
      "grad_norm": 0.25991180539131165,
      "learning_rate": 1.822811447811448e-05,
      "loss": 0.022,
      "step": 1240
    },
    {
      "epoch": 1.509090909090909,
      "grad_norm": 0.2780468463897705,
      "learning_rate": 1.8217592592592593e-05,
      "loss": 0.0235,
      "step": 1245
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 0.25895485281944275,
      "learning_rate": 1.820707070707071e-05,
      "loss": 0.0225,
      "step": 1250
    },
    {
      "epoch": 1.5212121212121212,
      "grad_norm": 0.3120538890361786,
      "learning_rate": 1.819654882154882e-05,
      "loss": 0.021,
      "step": 1255
    },
    {
      "epoch": 1.5272727272727273,
      "grad_norm": 0.2695126533508301,
      "learning_rate": 1.8186026936026938e-05,
      "loss": 0.0212,
      "step": 1260
    },
    {
      "epoch": 1.5333333333333334,
      "grad_norm": 0.3700115978717804,
      "learning_rate": 1.817550505050505e-05,
      "loss": 0.0224,
      "step": 1265
    },
    {
      "epoch": 1.5393939393939393,
      "grad_norm": 0.46398699283599854,
      "learning_rate": 1.8164983164983166e-05,
      "loss": 0.0211,
      "step": 1270
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 0.3886953890323639,
      "learning_rate": 1.8154461279461283e-05,
      "loss": 0.0218,
      "step": 1275
    },
    {
      "epoch": 1.5515151515151515,
      "grad_norm": 0.2502823770046234,
      "learning_rate": 1.8143939393939395e-05,
      "loss": 0.0215,
      "step": 1280
    },
    {
      "epoch": 1.5575757575757576,
      "grad_norm": 0.28438419103622437,
      "learning_rate": 1.813341750841751e-05,
      "loss": 0.0226,
      "step": 1285
    },
    {
      "epoch": 1.5636363636363637,
      "grad_norm": 0.2821308374404907,
      "learning_rate": 1.8122895622895624e-05,
      "loss": 0.0222,
      "step": 1290
    },
    {
      "epoch": 1.5696969696969696,
      "grad_norm": 0.2962355315685272,
      "learning_rate": 1.811237373737374e-05,
      "loss": 0.0217,
      "step": 1295
    },
    {
      "epoch": 1.5757575757575757,
      "grad_norm": 0.22602900862693787,
      "learning_rate": 1.8101851851851853e-05,
      "loss": 0.0221,
      "step": 1300
    },
    {
      "epoch": 1.5818181818181818,
      "grad_norm": 0.22668851912021637,
      "learning_rate": 1.8091329966329965e-05,
      "loss": 0.0206,
      "step": 1305
    },
    {
      "epoch": 1.587878787878788,
      "grad_norm": 0.21950723230838776,
      "learning_rate": 1.8080808080808085e-05,
      "loss": 0.0221,
      "step": 1310
    },
    {
      "epoch": 1.593939393939394,
      "grad_norm": 0.24155132472515106,
      "learning_rate": 1.8070286195286198e-05,
      "loss": 0.0229,
      "step": 1315
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.3180074393749237,
      "learning_rate": 1.805976430976431e-05,
      "loss": 0.0217,
      "step": 1320
    },
    {
      "epoch": 1.606060606060606,
      "grad_norm": 0.24955514073371887,
      "learning_rate": 1.8049242424242426e-05,
      "loss": 0.0233,
      "step": 1325
    },
    {
      "epoch": 1.612121212121212,
      "grad_norm": 0.2189788669347763,
      "learning_rate": 1.803872053872054e-05,
      "loss": 0.0226,
      "step": 1330
    },
    {
      "epoch": 1.6181818181818182,
      "grad_norm": 0.29159337282180786,
      "learning_rate": 1.8028198653198655e-05,
      "loss": 0.0222,
      "step": 1335
    },
    {
      "epoch": 1.6242424242424243,
      "grad_norm": 0.2182127833366394,
      "learning_rate": 1.8017676767676768e-05,
      "loss": 0.0212,
      "step": 1340
    },
    {
      "epoch": 1.6303030303030304,
      "grad_norm": 0.356303870677948,
      "learning_rate": 1.8007154882154884e-05,
      "loss": 0.0233,
      "step": 1345
    },
    {
      "epoch": 1.6363636363636365,
      "grad_norm": 0.34199491143226624,
      "learning_rate": 1.7996632996633e-05,
      "loss": 0.0216,
      "step": 1350
    },
    {
      "epoch": 1.6424242424242423,
      "grad_norm": 0.28165844082832336,
      "learning_rate": 1.7986111111111113e-05,
      "loss": 0.0217,
      "step": 1355
    },
    {
      "epoch": 1.6484848484848484,
      "grad_norm": 0.19479770958423615,
      "learning_rate": 1.797558922558923e-05,
      "loss": 0.0209,
      "step": 1360
    },
    {
      "epoch": 1.6545454545454545,
      "grad_norm": 0.22598131000995636,
      "learning_rate": 1.796506734006734e-05,
      "loss": 0.0215,
      "step": 1365
    },
    {
      "epoch": 1.6606060606060606,
      "grad_norm": 0.2755233645439148,
      "learning_rate": 1.7954545454545454e-05,
      "loss": 0.0232,
      "step": 1370
    },
    {
      "epoch": 1.6666666666666667,
      "grad_norm": 0.2936984896659851,
      "learning_rate": 1.794402356902357e-05,
      "loss": 0.0214,
      "step": 1375
    },
    {
      "epoch": 1.6727272727272726,
      "grad_norm": 0.31211191415786743,
      "learning_rate": 1.7933501683501683e-05,
      "loss": 0.0217,
      "step": 1380
    },
    {
      "epoch": 1.6787878787878787,
      "grad_norm": 0.28766417503356934,
      "learning_rate": 1.79229797979798e-05,
      "loss": 0.0213,
      "step": 1385
    },
    {
      "epoch": 1.6848484848484848,
      "grad_norm": 0.26861152052879333,
      "learning_rate": 1.7912457912457915e-05,
      "loss": 0.0214,
      "step": 1390
    },
    {
      "epoch": 1.690909090909091,
      "grad_norm": 0.2387586236000061,
      "learning_rate": 1.7901936026936028e-05,
      "loss": 0.0218,
      "step": 1395
    },
    {
      "epoch": 1.696969696969697,
      "grad_norm": 0.3855009973049164,
      "learning_rate": 1.7891414141414144e-05,
      "loss": 0.022,
      "step": 1400
    },
    {
      "epoch": 1.7030303030303031,
      "grad_norm": 0.30070140957832336,
      "learning_rate": 1.7880892255892257e-05,
      "loss": 0.0217,
      "step": 1405
    },
    {
      "epoch": 1.709090909090909,
      "grad_norm": 0.3289494216442108,
      "learning_rate": 1.7870370370370373e-05,
      "loss": 0.0215,
      "step": 1410
    },
    {
      "epoch": 1.715151515151515,
      "grad_norm": 0.23573727905750275,
      "learning_rate": 1.7859848484848485e-05,
      "loss": 0.0212,
      "step": 1415
    },
    {
      "epoch": 1.7212121212121212,
      "grad_norm": 0.24287883937358856,
      "learning_rate": 1.78493265993266e-05,
      "loss": 0.0226,
      "step": 1420
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.20747259259223938,
      "learning_rate": 1.7838804713804718e-05,
      "loss": 0.0222,
      "step": 1425
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.4425068497657776,
      "learning_rate": 1.782828282828283e-05,
      "loss": 0.0226,
      "step": 1430
    },
    {
      "epoch": 1.7393939393939395,
      "grad_norm": 0.3694164454936981,
      "learning_rate": 1.7817760942760943e-05,
      "loss": 0.0228,
      "step": 1435
    },
    {
      "epoch": 1.7454545454545454,
      "grad_norm": 0.2606164515018463,
      "learning_rate": 1.780723905723906e-05,
      "loss": 0.0225,
      "step": 1440
    },
    {
      "epoch": 1.7515151515151515,
      "grad_norm": 0.3158935308456421,
      "learning_rate": 1.7796717171717172e-05,
      "loss": 0.0224,
      "step": 1445
    },
    {
      "epoch": 1.7575757575757576,
      "grad_norm": 0.2738884687423706,
      "learning_rate": 1.7786195286195288e-05,
      "loss": 0.0218,
      "step": 1450
    },
    {
      "epoch": 1.7636363636363637,
      "grad_norm": 0.2958800196647644,
      "learning_rate": 1.77756734006734e-05,
      "loss": 0.0223,
      "step": 1455
    },
    {
      "epoch": 1.7696969696969698,
      "grad_norm": 0.2516324520111084,
      "learning_rate": 1.7765151515151517e-05,
      "loss": 0.0233,
      "step": 1460
    },
    {
      "epoch": 1.7757575757575759,
      "grad_norm": 0.30084657669067383,
      "learning_rate": 1.7754629629629633e-05,
      "loss": 0.023,
      "step": 1465
    },
    {
      "epoch": 1.7818181818181817,
      "grad_norm": 0.25291115045547485,
      "learning_rate": 1.7744107744107745e-05,
      "loss": 0.0226,
      "step": 1470
    },
    {
      "epoch": 1.7878787878787878,
      "grad_norm": 0.26246267557144165,
      "learning_rate": 1.773358585858586e-05,
      "loss": 0.0214,
      "step": 1475
    },
    {
      "epoch": 1.793939393939394,
      "grad_norm": 0.23570364713668823,
      "learning_rate": 1.7723063973063974e-05,
      "loss": 0.0223,
      "step": 1480
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.24564942717552185,
      "learning_rate": 1.7712542087542087e-05,
      "loss": 0.0215,
      "step": 1485
    },
    {
      "epoch": 1.8060606060606061,
      "grad_norm": 0.24947935342788696,
      "learning_rate": 1.7702020202020203e-05,
      "loss": 0.0225,
      "step": 1490
    },
    {
      "epoch": 1.812121212121212,
      "grad_norm": 0.22033944725990295,
      "learning_rate": 1.769149831649832e-05,
      "loss": 0.0227,
      "step": 1495
    },
    {
      "epoch": 1.8181818181818181,
      "grad_norm": 0.24024741351604462,
      "learning_rate": 1.768097643097643e-05,
      "loss": 0.0216,
      "step": 1500
    },
    {
      "epoch": 1.8242424242424242,
      "grad_norm": 0.2044433355331421,
      "learning_rate": 1.7670454545454548e-05,
      "loss": 0.021,
      "step": 1505
    },
    {
      "epoch": 1.8303030303030303,
      "grad_norm": 0.20354753732681274,
      "learning_rate": 1.765993265993266e-05,
      "loss": 0.0222,
      "step": 1510
    },
    {
      "epoch": 1.8363636363636364,
      "grad_norm": 0.3119855225086212,
      "learning_rate": 1.7649410774410777e-05,
      "loss": 0.0226,
      "step": 1515
    },
    {
      "epoch": 1.8424242424242425,
      "grad_norm": 0.32089123129844666,
      "learning_rate": 1.763888888888889e-05,
      "loss": 0.0219,
      "step": 1520
    },
    {
      "epoch": 1.8484848484848484,
      "grad_norm": 0.2285999357700348,
      "learning_rate": 1.7628367003367005e-05,
      "loss": 0.0219,
      "step": 1525
    },
    {
      "epoch": 1.8545454545454545,
      "grad_norm": 0.3225323259830475,
      "learning_rate": 1.7617845117845118e-05,
      "loss": 0.0239,
      "step": 1530
    },
    {
      "epoch": 1.8606060606060606,
      "grad_norm": 0.2581339180469513,
      "learning_rate": 1.7607323232323234e-05,
      "loss": 0.023,
      "step": 1535
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.31842324137687683,
      "learning_rate": 1.759680134680135e-05,
      "loss": 0.0226,
      "step": 1540
    },
    {
      "epoch": 1.8727272727272728,
      "grad_norm": 0.2776392698287964,
      "learning_rate": 1.7586279461279463e-05,
      "loss": 0.0222,
      "step": 1545
    },
    {
      "epoch": 1.878787878787879,
      "grad_norm": 0.23660686612129211,
      "learning_rate": 1.7575757575757576e-05,
      "loss": 0.021,
      "step": 1550
    },
    {
      "epoch": 1.8848484848484848,
      "grad_norm": 0.3247326612472534,
      "learning_rate": 1.756523569023569e-05,
      "loss": 0.0241,
      "step": 1555
    },
    {
      "epoch": 1.8909090909090909,
      "grad_norm": 0.335563063621521,
      "learning_rate": 1.7554713804713804e-05,
      "loss": 0.0229,
      "step": 1560
    },
    {
      "epoch": 1.896969696969697,
      "grad_norm": 0.2251315861940384,
      "learning_rate": 1.754419191919192e-05,
      "loss": 0.0221,
      "step": 1565
    },
    {
      "epoch": 1.903030303030303,
      "grad_norm": 0.38145750761032104,
      "learning_rate": 1.7533670033670036e-05,
      "loss": 0.0221,
      "step": 1570
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 0.24702540040016174,
      "learning_rate": 1.752314814814815e-05,
      "loss": 0.0221,
      "step": 1575
    },
    {
      "epoch": 1.915151515151515,
      "grad_norm": 0.319678395986557,
      "learning_rate": 1.7512626262626265e-05,
      "loss": 0.0227,
      "step": 1580
    },
    {
      "epoch": 1.9212121212121211,
      "grad_norm": 0.4400114417076111,
      "learning_rate": 1.7502104377104378e-05,
      "loss": 0.0226,
      "step": 1585
    },
    {
      "epoch": 1.9272727272727272,
      "grad_norm": 0.22215083241462708,
      "learning_rate": 1.7491582491582494e-05,
      "loss": 0.0222,
      "step": 1590
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.4261590838432312,
      "learning_rate": 1.7481060606060607e-05,
      "loss": 0.0239,
      "step": 1595
    },
    {
      "epoch": 1.9393939393939394,
      "grad_norm": 0.40792784094810486,
      "learning_rate": 1.7470538720538723e-05,
      "loss": 0.0234,
      "step": 1600
    },
    {
      "epoch": 1.9454545454545455,
      "grad_norm": 0.30221790075302124,
      "learning_rate": 1.746001683501684e-05,
      "loss": 0.022,
      "step": 1605
    },
    {
      "epoch": 1.9515151515151514,
      "grad_norm": 0.2864539921283722,
      "learning_rate": 1.744949494949495e-05,
      "loss": 0.0229,
      "step": 1610
    },
    {
      "epoch": 1.9575757575757575,
      "grad_norm": 0.2444179654121399,
      "learning_rate": 1.7438973063973064e-05,
      "loss": 0.0221,
      "step": 1615
    },
    {
      "epoch": 1.9636363636363636,
      "grad_norm": 0.2774612009525299,
      "learning_rate": 1.742845117845118e-05,
      "loss": 0.0233,
      "step": 1620
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 0.2556855082511902,
      "learning_rate": 1.7417929292929293e-05,
      "loss": 0.0218,
      "step": 1625
    },
    {
      "epoch": 1.9757575757575758,
      "grad_norm": 0.19792823493480682,
      "learning_rate": 1.740740740740741e-05,
      "loss": 0.0223,
      "step": 1630
    },
    {
      "epoch": 1.981818181818182,
      "grad_norm": 0.26134195923805237,
      "learning_rate": 1.7396885521885522e-05,
      "loss": 0.0222,
      "step": 1635
    },
    {
      "epoch": 1.9878787878787878,
      "grad_norm": 0.33057186007499695,
      "learning_rate": 1.7386363636363638e-05,
      "loss": 0.022,
      "step": 1640
    },
    {
      "epoch": 1.993939393939394,
      "grad_norm": 0.2483343780040741,
      "learning_rate": 1.7375841750841754e-05,
      "loss": 0.0214,
      "step": 1645
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.2338462620973587,
      "learning_rate": 1.7365319865319867e-05,
      "loss": 0.0213,
      "step": 1650
    },
    {
      "epoch": 2.0,
      "eval_average": 0.5840201620937151,
      "eval_crossner_ai": 0.556894243591122,
      "eval_crossner_literature": 0.5865384614884604,
      "eval_crossner_music": 0.7562408222701237,
      "eval_crossner_politics": 0.6507413508559884,
      "eval_crossner_science": 0.6157303370285195,
      "eval_mit-movie": 0.5271739129937825,
      "eval_mit-restaurant": 0.3948220064280098,
      "eval_runtime": 20.4005,
      "eval_samples_per_second": 34.313,
      "eval_steps_per_second": 0.343,
      "step": 1650
    }
  ],
  "logging_steps": 5,
  "max_steps": 9900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 12,
  "save_steps": 9223372036854775807,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 9.744355544583373e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
