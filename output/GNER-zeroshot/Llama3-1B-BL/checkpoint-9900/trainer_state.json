{
  "best_metric": 0.5840201620937151,
  "best_model_checkpoint": "output/GNER-zeroshot/Llama3-1B-BL/checkpoint-1650",
  "epoch": 12.0,
  "eval_steps": 9223372036854775807,
  "global_step": 9900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006060606060606061,
      "grad_norm": 8.63490104675293,
      "learning_rate": 5.381462829997033e-06,
      "loss": 0.5825,
      "step": 5
    },
    {
      "epoch": 0.012121212121212121,
      "grad_norm": 4.322505474090576,
      "learning_rate": 7.699132719020058e-06,
      "loss": 0.3051,
      "step": 10
    },
    {
      "epoch": 0.01818181818181818,
      "grad_norm": 3.8407723903656006,
      "learning_rate": 9.05488269314909e-06,
      "loss": 0.15,
      "step": 15
    },
    {
      "epoch": 0.024242424242424242,
      "grad_norm": 3.2588558197021484,
      "learning_rate": 1.001680260804308e-05,
      "loss": 0.1129,
      "step": 20
    },
    {
      "epoch": 0.030303030303030304,
      "grad_norm": 1.7190009355545044,
      "learning_rate": 1.0762925659994066e-05,
      "loss": 0.0809,
      "step": 25
    },
    {
      "epoch": 0.03636363636363636,
      "grad_norm": 2.713549852371216,
      "learning_rate": 1.1372552582172113e-05,
      "loss": 0.0729,
      "step": 30
    },
    {
      "epoch": 0.04242424242424243,
      "grad_norm": 1.786845088005066,
      "learning_rate": 1.1887984800650519e-05,
      "loss": 0.065,
      "step": 35
    },
    {
      "epoch": 0.048484848484848485,
      "grad_norm": 0.8609691262245178,
      "learning_rate": 1.2334472497066103e-05,
      "loss": 0.0594,
      "step": 40
    },
    {
      "epoch": 0.05454545454545454,
      "grad_norm": 0.5927493572235107,
      "learning_rate": 1.2728302556301145e-05,
      "loss": 0.0542,
      "step": 45
    },
    {
      "epoch": 0.06060606060606061,
      "grad_norm": 0.5668410062789917,
      "learning_rate": 1.308059554901709e-05,
      "loss": 0.0535,
      "step": 50
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.7819388508796692,
      "learning_rate": 1.3399283325646878e-05,
      "loss": 0.0504,
      "step": 55
    },
    {
      "epoch": 0.07272727272727272,
      "grad_norm": 0.4519084393978119,
      "learning_rate": 1.3690222471195137e-05,
      "loss": 0.049,
      "step": 60
    },
    {
      "epoch": 0.07878787878787878,
      "grad_norm": 0.45400285720825195,
      "learning_rate": 1.3957860540877486e-05,
      "loss": 0.0488,
      "step": 65
    },
    {
      "epoch": 0.08484848484848485,
      "grad_norm": 0.5067660212516785,
      "learning_rate": 1.4205654689673544e-05,
      "loss": 0.0474,
      "step": 70
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 0.4362829923629761,
      "learning_rate": 1.4436345523146123e-05,
      "loss": 0.0466,
      "step": 75
    },
    {
      "epoch": 0.09696969696969697,
      "grad_norm": 0.5346341133117676,
      "learning_rate": 1.4652142386089125e-05,
      "loss": 0.0438,
      "step": 80
    },
    {
      "epoch": 0.10303030303030303,
      "grad_norm": 0.7718464732170105,
      "learning_rate": 1.4854852379663441e-05,
      "loss": 0.0463,
      "step": 85
    },
    {
      "epoch": 0.10909090909090909,
      "grad_norm": 0.8955128192901611,
      "learning_rate": 1.5045972445324168e-05,
      "loss": 0.0445,
      "step": 90
    },
    {
      "epoch": 0.11515151515151516,
      "grad_norm": 1.0805050134658813,
      "learning_rate": 1.5226756518657677e-05,
      "loss": 0.0438,
      "step": 95
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 0.6605945825576782,
      "learning_rate": 1.5398265438040116e-05,
      "loss": 0.0451,
      "step": 100
    },
    {
      "epoch": 0.12727272727272726,
      "grad_norm": 0.3935721516609192,
      "learning_rate": 1.5561404663802574e-05,
      "loss": 0.0414,
      "step": 105
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.34878817200660706,
      "learning_rate": 1.5716953214669903e-05,
      "loss": 0.0423,
      "step": 110
    },
    {
      "epoch": 0.1393939393939394,
      "grad_norm": 0.38836702704429626,
      "learning_rate": 1.5865586166680462e-05,
      "loss": 0.041,
      "step": 115
    },
    {
      "epoch": 0.14545454545454545,
      "grad_norm": 0.5763735771179199,
      "learning_rate": 1.600789236021816e-05,
      "loss": 0.0404,
      "step": 120
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 0.6142232418060303,
      "learning_rate": 1.6144388489991102e-05,
      "loss": 0.0399,
      "step": 125
    },
    {
      "epoch": 0.15757575757575756,
      "grad_norm": 0.457793265581131,
      "learning_rate": 1.627553042990051e-05,
      "loss": 0.0375,
      "step": 130
    },
    {
      "epoch": 0.16363636363636364,
      "grad_norm": 0.44654425978660583,
      "learning_rate": 1.6401722419453202e-05,
      "loss": 0.0376,
      "step": 135
    },
    {
      "epoch": 0.1696969696969697,
      "grad_norm": 0.4559710621833801,
      "learning_rate": 1.6523324578696566e-05,
      "loss": 0.04,
      "step": 140
    },
    {
      "epoch": 0.17575757575757575,
      "grad_norm": 0.5468865036964417,
      "learning_rate": 1.664065910385031e-05,
      "loss": 0.0379,
      "step": 145
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.3406648635864258,
      "learning_rate": 1.6754015412169146e-05,
      "loss": 0.0378,
      "step": 150
    },
    {
      "epoch": 0.18787878787878787,
      "grad_norm": 0.305560439825058,
      "learning_rate": 1.6863654442889655e-05,
      "loss": 0.0393,
      "step": 155
    },
    {
      "epoch": 0.19393939393939394,
      "grad_norm": 0.709934651851654,
      "learning_rate": 1.696981227511215e-05,
      "loss": 0.0389,
      "step": 160
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.4553469121456146,
      "learning_rate": 1.7072703188798933e-05,
      "loss": 0.038,
      "step": 165
    },
    {
      "epoch": 0.20606060606060606,
      "grad_norm": 0.4402814507484436,
      "learning_rate": 1.7172522268686466e-05,
      "loss": 0.0386,
      "step": 170
    },
    {
      "epoch": 0.21212121212121213,
      "grad_norm": 0.4144954979419708,
      "learning_rate": 1.7269447630647555e-05,
      "loss": 0.0369,
      "step": 175
    },
    {
      "epoch": 0.21818181818181817,
      "grad_norm": 0.35709768533706665,
      "learning_rate": 1.7363642334347193e-05,
      "loss": 0.0379,
      "step": 180
    },
    {
      "epoch": 0.22424242424242424,
      "grad_norm": 0.4880967140197754,
      "learning_rate": 1.7455256033784897e-05,
      "loss": 0.0367,
      "step": 185
    },
    {
      "epoch": 0.23030303030303031,
      "grad_norm": 0.35466328263282776,
      "learning_rate": 1.75444264076807e-05,
      "loss": 0.0358,
      "step": 190
    },
    {
      "epoch": 0.23636363636363636,
      "grad_norm": 0.31103697419166565,
      "learning_rate": 1.7631280404029543e-05,
      "loss": 0.036,
      "step": 195
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 0.4138535261154175,
      "learning_rate": 1.7715935327063138e-05,
      "loss": 0.0361,
      "step": 200
    },
    {
      "epoch": 0.24848484848484848,
      "grad_norm": 0.6836885809898376,
      "learning_rate": 1.7798499789975307e-05,
      "loss": 0.0378,
      "step": 205
    },
    {
      "epoch": 0.2545454545454545,
      "grad_norm": 0.5516701936721802,
      "learning_rate": 1.78790745528256e-05,
      "loss": 0.0369,
      "step": 210
    },
    {
      "epoch": 0.2606060606060606,
      "grad_norm": 0.39796844124794006,
      "learning_rate": 1.7957753261836987e-05,
      "loss": 0.0385,
      "step": 215
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.44762420654296875,
      "learning_rate": 1.8034623103692924e-05,
      "loss": 0.0366,
      "step": 220
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 0.35374191403388977,
      "learning_rate": 1.810976538629818e-05,
      "loss": 0.0372,
      "step": 225
    },
    {
      "epoch": 0.2787878787878788,
      "grad_norm": 0.29907068610191345,
      "learning_rate": 1.8183256055703488e-05,
      "loss": 0.0365,
      "step": 230
    },
    {
      "epoch": 0.28484848484848485,
      "grad_norm": 0.6014831066131592,
      "learning_rate": 1.8255166157433265e-05,
      "loss": 0.0362,
      "step": 235
    },
    {
      "epoch": 0.2909090909090909,
      "grad_norm": 0.5096117258071899,
      "learning_rate": 1.8325562249241182e-05,
      "loss": 0.034,
      "step": 240
    },
    {
      "epoch": 0.296969696969697,
      "grad_norm": 0.5617567300796509,
      "learning_rate": 1.839450677130401e-05,
      "loss": 0.0347,
      "step": 245
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 0.3558465540409088,
      "learning_rate": 1.8462058379014124e-05,
      "loss": 0.0351,
      "step": 250
    },
    {
      "epoch": 0.3090909090909091,
      "grad_norm": 0.71063631772995,
      "learning_rate": 1.8528272242815496e-05,
      "loss": 0.0366,
      "step": 255
    },
    {
      "epoch": 0.3151515151515151,
      "grad_norm": 0.5253021717071533,
      "learning_rate": 1.8593200318923534e-05,
      "loss": 0.0366,
      "step": 260
    },
    {
      "epoch": 0.3212121212121212,
      "grad_norm": 0.40601053833961487,
      "learning_rate": 1.8656891594257228e-05,
      "loss": 0.0345,
      "step": 265
    },
    {
      "epoch": 0.32727272727272727,
      "grad_norm": 0.5746368169784546,
      "learning_rate": 1.8719392308476227e-05,
      "loss": 0.0365,
      "step": 270
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.5711379647254944,
      "learning_rate": 1.878074615564391e-05,
      "loss": 0.0361,
      "step": 275
    },
    {
      "epoch": 0.3393939393939394,
      "grad_norm": 0.6803319454193115,
      "learning_rate": 1.8840994467719588e-05,
      "loss": 0.0353,
      "step": 280
    },
    {
      "epoch": 0.34545454545454546,
      "grad_norm": 0.3054657280445099,
      "learning_rate": 1.8900176381809734e-05,
      "loss": 0.0361,
      "step": 285
    },
    {
      "epoch": 0.3515151515151515,
      "grad_norm": 0.2711184620857239,
      "learning_rate": 1.8958328992873333e-05,
      "loss": 0.0339,
      "step": 290
    },
    {
      "epoch": 0.3575757575757576,
      "grad_norm": 0.31115850806236267,
      "learning_rate": 1.9015487493373553e-05,
      "loss": 0.0348,
      "step": 295
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.33295345306396484,
      "learning_rate": 1.9071685301192168e-05,
      "loss": 0.035,
      "step": 300
    },
    {
      "epoch": 0.3696969696969697,
      "grad_norm": 0.3145270049571991,
      "learning_rate": 1.912695417697111e-05,
      "loss": 0.0349,
      "step": 305
    },
    {
      "epoch": 0.37575757575757573,
      "grad_norm": 0.4976232051849365,
      "learning_rate": 1.9181324331912677e-05,
      "loss": 0.037,
      "step": 310
    },
    {
      "epoch": 0.38181818181818183,
      "grad_norm": 0.545088529586792,
      "learning_rate": 1.9234824526954633e-05,
      "loss": 0.0343,
      "step": 315
    },
    {
      "epoch": 0.3878787878787879,
      "grad_norm": 0.3774203956127167,
      "learning_rate": 1.9287482164135173e-05,
      "loss": 0.0329,
      "step": 320
    },
    {
      "epoch": 0.3939393939393939,
      "grad_norm": 0.3258519470691681,
      "learning_rate": 1.933932337087452e-05,
      "loss": 0.0336,
      "step": 325
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.31861215829849243,
      "learning_rate": 1.9390373077821954e-05,
      "loss": 0.0354,
      "step": 330
    },
    {
      "epoch": 0.40606060606060607,
      "grad_norm": 0.2723672389984131,
      "learning_rate": 1.944065509084906e-05,
      "loss": 0.0339,
      "step": 335
    },
    {
      "epoch": 0.4121212121212121,
      "grad_norm": 0.48070028424263,
      "learning_rate": 1.9490192157709488e-05,
      "loss": 0.0333,
      "step": 340
    },
    {
      "epoch": 0.41818181818181815,
      "grad_norm": 0.459739625453949,
      "learning_rate": 1.9539006029832518e-05,
      "loss": 0.0343,
      "step": 345
    },
    {
      "epoch": 0.42424242424242425,
      "grad_norm": 0.3008536398410797,
      "learning_rate": 1.9587117519670574e-05,
      "loss": 0.0353,
      "step": 350
    },
    {
      "epoch": 0.4303030303030303,
      "grad_norm": 0.38889071345329285,
      "learning_rate": 1.963454655397911e-05,
      "loss": 0.0341,
      "step": 355
    },
    {
      "epoch": 0.43636363636363634,
      "grad_norm": 0.4915270209312439,
      "learning_rate": 1.968131222337022e-05,
      "loss": 0.0353,
      "step": 360
    },
    {
      "epoch": 0.44242424242424244,
      "grad_norm": 0.4057636559009552,
      "learning_rate": 1.9727432828448466e-05,
      "loss": 0.0345,
      "step": 365
    },
    {
      "epoch": 0.4484848484848485,
      "grad_norm": 0.32680678367614746,
      "learning_rate": 1.977292592280792e-05,
      "loss": 0.0332,
      "step": 370
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.2668668329715729,
      "learning_rate": 1.9817808353143154e-05,
      "loss": 0.0315,
      "step": 375
    },
    {
      "epoch": 0.46060606060606063,
      "grad_norm": 0.26270952820777893,
      "learning_rate": 1.9862096296703722e-05,
      "loss": 0.0338,
      "step": 380
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.2758481800556183,
      "learning_rate": 1.9905805296300364e-05,
      "loss": 0.0331,
      "step": 385
    },
    {
      "epoch": 0.4727272727272727,
      "grad_norm": 0.2486269325017929,
      "learning_rate": 1.9948950293052568e-05,
      "loss": 0.0336,
      "step": 390
    },
    {
      "epoch": 0.47878787878787876,
      "grad_norm": 0.43292659521102905,
      "learning_rate": 1.999154565705013e-05,
      "loss": 0.0336,
      "step": 395
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 0.32967817783355713,
      "learning_rate": 1.999368686868687e-05,
      "loss": 0.0333,
      "step": 400
    },
    {
      "epoch": 0.4909090909090909,
      "grad_norm": 0.29001298546791077,
      "learning_rate": 1.9983164983164986e-05,
      "loss": 0.0321,
      "step": 405
    },
    {
      "epoch": 0.49696969696969695,
      "grad_norm": 0.32173898816108704,
      "learning_rate": 1.99726430976431e-05,
      "loss": 0.0325,
      "step": 410
    },
    {
      "epoch": 0.4993939393939394,
      "eval_average": 0.5791917974219782,
      "eval_crossner_ai": 0.520146520096587,
      "eval_crossner_literature": 0.5606407322153182,
      "eval_crossner_music": 0.7170349251102842,
      "eval_crossner_politics": 0.6584967319760383,
      "eval_crossner_science": 0.6681034482257635,
      "eval_mit-movie": 0.5561497325704369,
      "eval_mit-restaurant": 0.3737704917594195,
      "eval_runtime": 20.8165,
      "eval_samples_per_second": 33.627,
      "eval_steps_per_second": 0.336,
      "step": 412
    },
    {
      "epoch": 0.503030303030303,
      "grad_norm": 0.35715827345848083,
      "learning_rate": 1.996212121212121e-05,
      "loss": 0.0324,
      "step": 415
    },
    {
      "epoch": 0.509090909090909,
      "grad_norm": 0.2450762689113617,
      "learning_rate": 1.9951599326599327e-05,
      "loss": 0.0323,
      "step": 420
    },
    {
      "epoch": 0.5151515151515151,
      "grad_norm": 0.29769468307495117,
      "learning_rate": 1.9941077441077444e-05,
      "loss": 0.0327,
      "step": 425
    },
    {
      "epoch": 0.5212121212121212,
      "grad_norm": 0.3731805384159088,
      "learning_rate": 1.9930555555555556e-05,
      "loss": 0.0333,
      "step": 430
    },
    {
      "epoch": 0.5272727272727272,
      "grad_norm": 0.4432564377784729,
      "learning_rate": 1.9920033670033672e-05,
      "loss": 0.0333,
      "step": 435
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.31727364659309387,
      "learning_rate": 1.9909511784511785e-05,
      "loss": 0.0332,
      "step": 440
    },
    {
      "epoch": 0.5393939393939394,
      "grad_norm": 0.5506933331489563,
      "learning_rate": 1.98989898989899e-05,
      "loss": 0.0339,
      "step": 445
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.2608805000782013,
      "learning_rate": 1.9888468013468014e-05,
      "loss": 0.0314,
      "step": 450
    },
    {
      "epoch": 0.5515151515151515,
      "grad_norm": 0.28970998525619507,
      "learning_rate": 1.987794612794613e-05,
      "loss": 0.0324,
      "step": 455
    },
    {
      "epoch": 0.5575757575757576,
      "grad_norm": 0.2809615731239319,
      "learning_rate": 1.9867424242424246e-05,
      "loss": 0.0304,
      "step": 460
    },
    {
      "epoch": 0.5636363636363636,
      "grad_norm": 0.3160002827644348,
      "learning_rate": 1.985690235690236e-05,
      "loss": 0.0298,
      "step": 465
    },
    {
      "epoch": 0.5696969696969697,
      "grad_norm": 0.5111832022666931,
      "learning_rate": 1.9846380471380475e-05,
      "loss": 0.034,
      "step": 470
    },
    {
      "epoch": 0.5757575757575758,
      "grad_norm": 0.326456755399704,
      "learning_rate": 1.9835858585858587e-05,
      "loss": 0.0325,
      "step": 475
    },
    {
      "epoch": 0.5818181818181818,
      "grad_norm": 0.42159873247146606,
      "learning_rate": 1.98253367003367e-05,
      "loss": 0.0322,
      "step": 480
    },
    {
      "epoch": 0.5878787878787879,
      "grad_norm": 0.29852959513664246,
      "learning_rate": 1.9814814814814816e-05,
      "loss": 0.0322,
      "step": 485
    },
    {
      "epoch": 0.593939393939394,
      "grad_norm": 0.2802867889404297,
      "learning_rate": 1.9804292929292932e-05,
      "loss": 0.0331,
      "step": 490
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.2572622001171112,
      "learning_rate": 1.9793771043771045e-05,
      "loss": 0.0313,
      "step": 495
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 0.26064616441726685,
      "learning_rate": 1.978324915824916e-05,
      "loss": 0.033,
      "step": 500
    },
    {
      "epoch": 0.6121212121212121,
      "grad_norm": 0.3742619752883911,
      "learning_rate": 1.9772727272727274e-05,
      "loss": 0.0312,
      "step": 505
    },
    {
      "epoch": 0.6181818181818182,
      "grad_norm": 0.5433458089828491,
      "learning_rate": 1.976220538720539e-05,
      "loss": 0.0328,
      "step": 510
    },
    {
      "epoch": 0.6242424242424243,
      "grad_norm": 0.26670128107070923,
      "learning_rate": 1.9751683501683503e-05,
      "loss": 0.0303,
      "step": 515
    },
    {
      "epoch": 0.6303030303030303,
      "grad_norm": 0.23925013840198517,
      "learning_rate": 1.974116161616162e-05,
      "loss": 0.0306,
      "step": 520
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 0.3795340061187744,
      "learning_rate": 1.973063973063973e-05,
      "loss": 0.0306,
      "step": 525
    },
    {
      "epoch": 0.6424242424242425,
      "grad_norm": 0.407589852809906,
      "learning_rate": 1.9720117845117847e-05,
      "loss": 0.0309,
      "step": 530
    },
    {
      "epoch": 0.6484848484848484,
      "grad_norm": 0.4038148820400238,
      "learning_rate": 1.9709595959595963e-05,
      "loss": 0.032,
      "step": 535
    },
    {
      "epoch": 0.6545454545454545,
      "grad_norm": 0.27322643995285034,
      "learning_rate": 1.9699074074074076e-05,
      "loss": 0.0305,
      "step": 540
    },
    {
      "epoch": 0.6606060606060606,
      "grad_norm": 0.25489315390586853,
      "learning_rate": 1.968855218855219e-05,
      "loss": 0.0309,
      "step": 545
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.5546852946281433,
      "learning_rate": 1.9678030303030305e-05,
      "loss": 0.0317,
      "step": 550
    },
    {
      "epoch": 0.6727272727272727,
      "grad_norm": 0.29473018646240234,
      "learning_rate": 1.9667508417508418e-05,
      "loss": 0.0319,
      "step": 555
    },
    {
      "epoch": 0.6787878787878788,
      "grad_norm": 0.2568839192390442,
      "learning_rate": 1.9656986531986534e-05,
      "loss": 0.0308,
      "step": 560
    },
    {
      "epoch": 0.6848484848484848,
      "grad_norm": 0.32141977548599243,
      "learning_rate": 1.964646464646465e-05,
      "loss": 0.0321,
      "step": 565
    },
    {
      "epoch": 0.6909090909090909,
      "grad_norm": 0.24373693764209747,
      "learning_rate": 1.9635942760942763e-05,
      "loss": 0.0306,
      "step": 570
    },
    {
      "epoch": 0.696969696969697,
      "grad_norm": 0.2891671359539032,
      "learning_rate": 1.962542087542088e-05,
      "loss": 0.032,
      "step": 575
    },
    {
      "epoch": 0.703030303030303,
      "grad_norm": 0.5750558376312256,
      "learning_rate": 1.961489898989899e-05,
      "loss": 0.0324,
      "step": 580
    },
    {
      "epoch": 0.7090909090909091,
      "grad_norm": 0.24102090299129486,
      "learning_rate": 1.9604377104377107e-05,
      "loss": 0.0317,
      "step": 585
    },
    {
      "epoch": 0.7151515151515152,
      "grad_norm": 0.31139931082725525,
      "learning_rate": 1.959385521885522e-05,
      "loss": 0.0319,
      "step": 590
    },
    {
      "epoch": 0.7212121212121212,
      "grad_norm": 0.38392558693885803,
      "learning_rate": 1.9583333333333333e-05,
      "loss": 0.0317,
      "step": 595
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.35479679703712463,
      "learning_rate": 1.957281144781145e-05,
      "loss": 0.0321,
      "step": 600
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.26235559582710266,
      "learning_rate": 1.9562289562289565e-05,
      "loss": 0.03,
      "step": 605
    },
    {
      "epoch": 0.7393939393939394,
      "grad_norm": 0.41770321130752563,
      "learning_rate": 1.9551767676767678e-05,
      "loss": 0.0333,
      "step": 610
    },
    {
      "epoch": 0.7454545454545455,
      "grad_norm": 0.3011866807937622,
      "learning_rate": 1.9541245791245794e-05,
      "loss": 0.0311,
      "step": 615
    },
    {
      "epoch": 0.7515151515151515,
      "grad_norm": 0.23577293753623962,
      "learning_rate": 1.9530723905723906e-05,
      "loss": 0.0293,
      "step": 620
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 0.2772422730922699,
      "learning_rate": 1.9520202020202022e-05,
      "loss": 0.0313,
      "step": 625
    },
    {
      "epoch": 0.7636363636363637,
      "grad_norm": 0.32617759704589844,
      "learning_rate": 1.9509680134680135e-05,
      "loss": 0.0327,
      "step": 630
    },
    {
      "epoch": 0.7696969696969697,
      "grad_norm": 0.2570483088493347,
      "learning_rate": 1.949915824915825e-05,
      "loss": 0.0306,
      "step": 635
    },
    {
      "epoch": 0.7757575757575758,
      "grad_norm": 0.375601202249527,
      "learning_rate": 1.9488636363636367e-05,
      "loss": 0.0301,
      "step": 640
    },
    {
      "epoch": 0.7818181818181819,
      "grad_norm": 0.3407788872718811,
      "learning_rate": 1.947811447811448e-05,
      "loss": 0.0328,
      "step": 645
    },
    {
      "epoch": 0.7878787878787878,
      "grad_norm": 0.20373927056789398,
      "learning_rate": 1.9467592592592596e-05,
      "loss": 0.0305,
      "step": 650
    },
    {
      "epoch": 0.793939393939394,
      "grad_norm": 0.25077903270721436,
      "learning_rate": 1.945707070707071e-05,
      "loss": 0.0308,
      "step": 655
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.27440720796585083,
      "learning_rate": 1.944654882154882e-05,
      "loss": 0.0314,
      "step": 660
    },
    {
      "epoch": 0.806060606060606,
      "grad_norm": 0.34862589836120605,
      "learning_rate": 1.9436026936026938e-05,
      "loss": 0.0313,
      "step": 665
    },
    {
      "epoch": 0.8121212121212121,
      "grad_norm": 0.27242010831832886,
      "learning_rate": 1.942550505050505e-05,
      "loss": 0.0327,
      "step": 670
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 0.2974311411380768,
      "learning_rate": 1.9414983164983166e-05,
      "loss": 0.0314,
      "step": 675
    },
    {
      "epoch": 0.8242424242424242,
      "grad_norm": 0.2687873840332031,
      "learning_rate": 1.9404461279461282e-05,
      "loss": 0.0308,
      "step": 680
    },
    {
      "epoch": 0.8303030303030303,
      "grad_norm": 0.32290545105934143,
      "learning_rate": 1.9393939393939395e-05,
      "loss": 0.0312,
      "step": 685
    },
    {
      "epoch": 0.8363636363636363,
      "grad_norm": 0.2659580707550049,
      "learning_rate": 1.938341750841751e-05,
      "loss": 0.0301,
      "step": 690
    },
    {
      "epoch": 0.8424242424242424,
      "grad_norm": 0.26104384660720825,
      "learning_rate": 1.9372895622895624e-05,
      "loss": 0.0313,
      "step": 695
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 0.2625015676021576,
      "learning_rate": 1.936237373737374e-05,
      "loss": 0.0306,
      "step": 700
    },
    {
      "epoch": 0.8545454545454545,
      "grad_norm": 0.21909096837043762,
      "learning_rate": 1.9351851851851853e-05,
      "loss": 0.0308,
      "step": 705
    },
    {
      "epoch": 0.8606060606060606,
      "grad_norm": 0.6454488039016724,
      "learning_rate": 1.9341329966329965e-05,
      "loss": 0.0299,
      "step": 710
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.24899975955486298,
      "learning_rate": 1.9330808080808085e-05,
      "loss": 0.029,
      "step": 715
    },
    {
      "epoch": 0.8727272727272727,
      "grad_norm": 0.28456464409828186,
      "learning_rate": 1.9320286195286198e-05,
      "loss": 0.0319,
      "step": 720
    },
    {
      "epoch": 0.8787878787878788,
      "grad_norm": 0.28144142031669617,
      "learning_rate": 1.930976430976431e-05,
      "loss": 0.0292,
      "step": 725
    },
    {
      "epoch": 0.8848484848484849,
      "grad_norm": 0.3047611713409424,
      "learning_rate": 1.9299242424242426e-05,
      "loss": 0.0316,
      "step": 730
    },
    {
      "epoch": 0.8909090909090909,
      "grad_norm": 0.38740065693855286,
      "learning_rate": 1.928872053872054e-05,
      "loss": 0.0307,
      "step": 735
    },
    {
      "epoch": 0.896969696969697,
      "grad_norm": 0.453657865524292,
      "learning_rate": 1.9278198653198655e-05,
      "loss": 0.0302,
      "step": 740
    },
    {
      "epoch": 0.9030303030303031,
      "grad_norm": 0.30072638392448425,
      "learning_rate": 1.9267676767676768e-05,
      "loss": 0.0317,
      "step": 745
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.3334069848060608,
      "learning_rate": 1.9257154882154884e-05,
      "loss": 0.0297,
      "step": 750
    },
    {
      "epoch": 0.9151515151515152,
      "grad_norm": 0.411685973405838,
      "learning_rate": 1.9246632996633e-05,
      "loss": 0.0294,
      "step": 755
    },
    {
      "epoch": 0.9212121212121213,
      "grad_norm": 0.5461416244506836,
      "learning_rate": 1.9236111111111113e-05,
      "loss": 0.0321,
      "step": 760
    },
    {
      "epoch": 0.9272727272727272,
      "grad_norm": 0.3038751482963562,
      "learning_rate": 1.922558922558923e-05,
      "loss": 0.0296,
      "step": 765
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.3308413624763489,
      "learning_rate": 1.921506734006734e-05,
      "loss": 0.0317,
      "step": 770
    },
    {
      "epoch": 0.9393939393939394,
      "grad_norm": 0.3853442370891571,
      "learning_rate": 1.9204545454545454e-05,
      "loss": 0.0283,
      "step": 775
    },
    {
      "epoch": 0.9454545454545454,
      "grad_norm": 0.2355109006166458,
      "learning_rate": 1.919402356902357e-05,
      "loss": 0.0304,
      "step": 780
    },
    {
      "epoch": 0.9515151515151515,
      "grad_norm": 0.25554534792900085,
      "learning_rate": 1.9183501683501683e-05,
      "loss": 0.0311,
      "step": 785
    },
    {
      "epoch": 0.9575757575757575,
      "grad_norm": 0.38685283064842224,
      "learning_rate": 1.91729797979798e-05,
      "loss": 0.0317,
      "step": 790
    },
    {
      "epoch": 0.9636363636363636,
      "grad_norm": 0.41898226737976074,
      "learning_rate": 1.9162457912457915e-05,
      "loss": 0.0304,
      "step": 795
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 0.27706390619277954,
      "learning_rate": 1.9151936026936028e-05,
      "loss": 0.0306,
      "step": 800
    },
    {
      "epoch": 0.9757575757575757,
      "grad_norm": 0.257541686296463,
      "learning_rate": 1.9141414141414144e-05,
      "loss": 0.0306,
      "step": 805
    },
    {
      "epoch": 0.9818181818181818,
      "grad_norm": 0.3957936465740204,
      "learning_rate": 1.9130892255892257e-05,
      "loss": 0.0297,
      "step": 810
    },
    {
      "epoch": 0.9878787878787879,
      "grad_norm": 0.22699956595897675,
      "learning_rate": 1.9120370370370373e-05,
      "loss": 0.0299,
      "step": 815
    },
    {
      "epoch": 0.9939393939393939,
      "grad_norm": 0.24206435680389404,
      "learning_rate": 1.9109848484848485e-05,
      "loss": 0.0302,
      "step": 820
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.2516419291496277,
      "learning_rate": 1.90993265993266e-05,
      "loss": 0.0311,
      "step": 825
    },
    {
      "epoch": 1.0,
      "eval_average": 0.5796812654551271,
      "eval_crossner_ai": 0.5594771241328611,
      "eval_crossner_literature": 0.5849056603272751,
      "eval_crossner_music": 0.6920315864538805,
      "eval_crossner_politics": 0.6281945588946617,
      "eval_crossner_science": 0.6608315097966942,
      "eval_mit-movie": 0.5577464788240397,
      "eval_mit-restaurant": 0.374581939756477,
      "eval_runtime": 20.7135,
      "eval_samples_per_second": 33.794,
      "eval_steps_per_second": 0.338,
      "step": 825
    },
    {
      "epoch": 1.006060606060606,
      "grad_norm": 0.4400379955768585,
      "learning_rate": 1.9090909090909094e-05,
      "loss": 0.0255,
      "step": 830
    },
    {
      "epoch": 1.0121212121212122,
      "grad_norm": 0.2648255527019501,
      "learning_rate": 1.9080387205387206e-05,
      "loss": 0.0225,
      "step": 835
    },
    {
      "epoch": 1.018181818181818,
      "grad_norm": 0.4055320620536804,
      "learning_rate": 1.9069865319865322e-05,
      "loss": 0.021,
      "step": 840
    },
    {
      "epoch": 1.0242424242424242,
      "grad_norm": 0.2593342661857605,
      "learning_rate": 1.9059343434343435e-05,
      "loss": 0.0221,
      "step": 845
    },
    {
      "epoch": 1.0303030303030303,
      "grad_norm": 0.35960671305656433,
      "learning_rate": 1.904882154882155e-05,
      "loss": 0.0229,
      "step": 850
    },
    {
      "epoch": 1.0363636363636364,
      "grad_norm": 0.3963358700275421,
      "learning_rate": 1.9038299663299664e-05,
      "loss": 0.0221,
      "step": 855
    },
    {
      "epoch": 1.0424242424242425,
      "grad_norm": 0.4948763847351074,
      "learning_rate": 1.902777777777778e-05,
      "loss": 0.0217,
      "step": 860
    },
    {
      "epoch": 1.0484848484848486,
      "grad_norm": 0.4198768734931946,
      "learning_rate": 1.9017255892255896e-05,
      "loss": 0.0219,
      "step": 865
    },
    {
      "epoch": 1.0545454545454545,
      "grad_norm": 0.4277552366256714,
      "learning_rate": 1.900673400673401e-05,
      "loss": 0.0225,
      "step": 870
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 0.3041873276233673,
      "learning_rate": 1.8996212121212125e-05,
      "loss": 0.0219,
      "step": 875
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.2769505977630615,
      "learning_rate": 1.8985690235690237e-05,
      "loss": 0.0227,
      "step": 880
    },
    {
      "epoch": 1.0727272727272728,
      "grad_norm": 0.23667094111442566,
      "learning_rate": 1.897516835016835e-05,
      "loss": 0.0213,
      "step": 885
    },
    {
      "epoch": 1.0787878787878789,
      "grad_norm": 0.43135538697242737,
      "learning_rate": 1.8964646464646466e-05,
      "loss": 0.0233,
      "step": 890
    },
    {
      "epoch": 1.084848484848485,
      "grad_norm": 0.32635658979415894,
      "learning_rate": 1.895412457912458e-05,
      "loss": 0.0219,
      "step": 895
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 0.40671712160110474,
      "learning_rate": 1.8943602693602695e-05,
      "loss": 0.0216,
      "step": 900
    },
    {
      "epoch": 1.096969696969697,
      "grad_norm": 0.360250860452652,
      "learning_rate": 1.893308080808081e-05,
      "loss": 0.0218,
      "step": 905
    },
    {
      "epoch": 1.103030303030303,
      "grad_norm": 0.3280063569545746,
      "learning_rate": 1.8922558922558924e-05,
      "loss": 0.0227,
      "step": 910
    },
    {
      "epoch": 1.1090909090909091,
      "grad_norm": 0.28214868903160095,
      "learning_rate": 1.891203703703704e-05,
      "loss": 0.0225,
      "step": 915
    },
    {
      "epoch": 1.1151515151515152,
      "grad_norm": 0.38145509362220764,
      "learning_rate": 1.8901515151515153e-05,
      "loss": 0.0215,
      "step": 920
    },
    {
      "epoch": 1.121212121212121,
      "grad_norm": 0.22794072329998016,
      "learning_rate": 1.889099326599327e-05,
      "loss": 0.0231,
      "step": 925
    },
    {
      "epoch": 1.1272727272727272,
      "grad_norm": 0.22696715593338013,
      "learning_rate": 1.888047138047138e-05,
      "loss": 0.0211,
      "step": 930
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.2450752705335617,
      "learning_rate": 1.8869949494949494e-05,
      "loss": 0.0224,
      "step": 935
    },
    {
      "epoch": 1.1393939393939394,
      "grad_norm": 0.2649388909339905,
      "learning_rate": 1.8859427609427613e-05,
      "loss": 0.0222,
      "step": 940
    },
    {
      "epoch": 1.1454545454545455,
      "grad_norm": 0.2956222891807556,
      "learning_rate": 1.8848905723905726e-05,
      "loss": 0.0224,
      "step": 945
    },
    {
      "epoch": 1.1515151515151516,
      "grad_norm": 0.22416982054710388,
      "learning_rate": 1.883838383838384e-05,
      "loss": 0.0214,
      "step": 950
    },
    {
      "epoch": 1.1575757575757575,
      "grad_norm": 0.22547370195388794,
      "learning_rate": 1.8827861952861955e-05,
      "loss": 0.0202,
      "step": 955
    },
    {
      "epoch": 1.1636363636363636,
      "grad_norm": 0.2777990996837616,
      "learning_rate": 1.8817340067340068e-05,
      "loss": 0.0233,
      "step": 960
    },
    {
      "epoch": 1.1696969696969697,
      "grad_norm": 0.3015587031841278,
      "learning_rate": 1.8806818181818184e-05,
      "loss": 0.0223,
      "step": 965
    },
    {
      "epoch": 1.1757575757575758,
      "grad_norm": 0.221028670668602,
      "learning_rate": 1.8796296296296296e-05,
      "loss": 0.0225,
      "step": 970
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 0.27499160170555115,
      "learning_rate": 1.8785774410774412e-05,
      "loss": 0.0225,
      "step": 975
    },
    {
      "epoch": 1.187878787878788,
      "grad_norm": 0.3166850805282593,
      "learning_rate": 1.877525252525253e-05,
      "loss": 0.0216,
      "step": 980
    },
    {
      "epoch": 1.1939393939393939,
      "grad_norm": 0.24268050491809845,
      "learning_rate": 1.876473063973064e-05,
      "loss": 0.0218,
      "step": 985
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.2744690179824829,
      "learning_rate": 1.8754208754208757e-05,
      "loss": 0.0221,
      "step": 990
    },
    {
      "epoch": 1.206060606060606,
      "grad_norm": 0.362761914730072,
      "learning_rate": 1.874368686868687e-05,
      "loss": 0.0205,
      "step": 995
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 0.4049486219882965,
      "learning_rate": 1.8733164983164983e-05,
      "loss": 0.0223,
      "step": 1000
    },
    {
      "epoch": 1.2181818181818183,
      "grad_norm": 0.3078119456768036,
      "learning_rate": 1.87226430976431e-05,
      "loss": 0.0219,
      "step": 1005
    },
    {
      "epoch": 1.2242424242424241,
      "grad_norm": 0.3626125454902649,
      "learning_rate": 1.871212121212121e-05,
      "loss": 0.0226,
      "step": 1010
    },
    {
      "epoch": 1.2303030303030302,
      "grad_norm": 0.2282802313566208,
      "learning_rate": 1.8701599326599328e-05,
      "loss": 0.0223,
      "step": 1015
    },
    {
      "epoch": 1.2363636363636363,
      "grad_norm": 0.25088784098625183,
      "learning_rate": 1.8691077441077444e-05,
      "loss": 0.0205,
      "step": 1020
    },
    {
      "epoch": 1.2424242424242424,
      "grad_norm": 0.251303106546402,
      "learning_rate": 1.8680555555555556e-05,
      "loss": 0.023,
      "step": 1025
    },
    {
      "epoch": 1.2484848484848485,
      "grad_norm": 0.30349820852279663,
      "learning_rate": 1.8670033670033672e-05,
      "loss": 0.0225,
      "step": 1030
    },
    {
      "epoch": 1.2545454545454546,
      "grad_norm": 0.27469730377197266,
      "learning_rate": 1.8659511784511785e-05,
      "loss": 0.0223,
      "step": 1035
    },
    {
      "epoch": 1.2606060606060605,
      "grad_norm": 0.30436843633651733,
      "learning_rate": 1.86489898989899e-05,
      "loss": 0.0216,
      "step": 1040
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.3041312098503113,
      "learning_rate": 1.8638468013468014e-05,
      "loss": 0.0221,
      "step": 1045
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 0.2689203917980194,
      "learning_rate": 1.862794612794613e-05,
      "loss": 0.022,
      "step": 1050
    },
    {
      "epoch": 1.2787878787878788,
      "grad_norm": 0.29724082350730896,
      "learning_rate": 1.8617424242424246e-05,
      "loss": 0.0213,
      "step": 1055
    },
    {
      "epoch": 1.284848484848485,
      "grad_norm": 0.3023717403411865,
      "learning_rate": 1.860690235690236e-05,
      "loss": 0.0226,
      "step": 1060
    },
    {
      "epoch": 1.290909090909091,
      "grad_norm": 0.2631073296070099,
      "learning_rate": 1.859638047138047e-05,
      "loss": 0.0217,
      "step": 1065
    },
    {
      "epoch": 1.2969696969696969,
      "grad_norm": 0.23508770763874054,
      "learning_rate": 1.8585858585858588e-05,
      "loss": 0.0229,
      "step": 1070
    },
    {
      "epoch": 1.303030303030303,
      "grad_norm": 0.2794710099697113,
      "learning_rate": 1.85753367003367e-05,
      "loss": 0.0224,
      "step": 1075
    },
    {
      "epoch": 1.309090909090909,
      "grad_norm": 0.48751217126846313,
      "learning_rate": 1.8564814814814816e-05,
      "loss": 0.0219,
      "step": 1080
    },
    {
      "epoch": 1.3151515151515152,
      "grad_norm": 0.3470744490623474,
      "learning_rate": 1.8554292929292932e-05,
      "loss": 0.0225,
      "step": 1085
    },
    {
      "epoch": 1.3212121212121213,
      "grad_norm": 0.3293347656726837,
      "learning_rate": 1.8543771043771045e-05,
      "loss": 0.0215,
      "step": 1090
    },
    {
      "epoch": 1.3272727272727274,
      "grad_norm": 0.28373968601226807,
      "learning_rate": 1.853324915824916e-05,
      "loss": 0.0227,
      "step": 1095
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.31946295499801636,
      "learning_rate": 1.8522727272727274e-05,
      "loss": 0.0231,
      "step": 1100
    },
    {
      "epoch": 1.3393939393939394,
      "grad_norm": 0.25139912962913513,
      "learning_rate": 1.851220538720539e-05,
      "loss": 0.0227,
      "step": 1105
    },
    {
      "epoch": 1.3454545454545455,
      "grad_norm": 0.31972330808639526,
      "learning_rate": 1.8501683501683503e-05,
      "loss": 0.0217,
      "step": 1110
    },
    {
      "epoch": 1.3515151515151516,
      "grad_norm": 0.5395520925521851,
      "learning_rate": 1.8491161616161615e-05,
      "loss": 0.0215,
      "step": 1115
    },
    {
      "epoch": 1.3575757575757577,
      "grad_norm": 0.40440139174461365,
      "learning_rate": 1.848063973063973e-05,
      "loss": 0.0235,
      "step": 1120
    },
    {
      "epoch": 1.3636363636363635,
      "grad_norm": 0.26157671213150024,
      "learning_rate": 1.8470117845117848e-05,
      "loss": 0.0208,
      "step": 1125
    },
    {
      "epoch": 1.3696969696969696,
      "grad_norm": 0.2564912736415863,
      "learning_rate": 1.845959595959596e-05,
      "loss": 0.0234,
      "step": 1130
    },
    {
      "epoch": 1.3757575757575757,
      "grad_norm": 0.2972109913825989,
      "learning_rate": 1.8449074074074076e-05,
      "loss": 0.0222,
      "step": 1135
    },
    {
      "epoch": 1.3818181818181818,
      "grad_norm": 0.25731217861175537,
      "learning_rate": 1.843855218855219e-05,
      "loss": 0.0224,
      "step": 1140
    },
    {
      "epoch": 1.387878787878788,
      "grad_norm": 0.21306392550468445,
      "learning_rate": 1.8428030303030305e-05,
      "loss": 0.0221,
      "step": 1145
    },
    {
      "epoch": 1.393939393939394,
      "grad_norm": 0.2847110629081726,
      "learning_rate": 1.8417508417508418e-05,
      "loss": 0.0218,
      "step": 1150
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.2902642488479614,
      "learning_rate": 1.8406986531986534e-05,
      "loss": 0.0211,
      "step": 1155
    },
    {
      "epoch": 1.406060606060606,
      "grad_norm": 0.2736600935459137,
      "learning_rate": 1.839646464646465e-05,
      "loss": 0.0233,
      "step": 1160
    },
    {
      "epoch": 1.412121212121212,
      "grad_norm": 0.2298624962568283,
      "learning_rate": 1.8385942760942763e-05,
      "loss": 0.0225,
      "step": 1165
    },
    {
      "epoch": 1.4181818181818182,
      "grad_norm": 0.22921550273895264,
      "learning_rate": 1.837542087542088e-05,
      "loss": 0.021,
      "step": 1170
    },
    {
      "epoch": 1.4242424242424243,
      "grad_norm": 0.255136102437973,
      "learning_rate": 1.836489898989899e-05,
      "loss": 0.0228,
      "step": 1175
    },
    {
      "epoch": 1.4303030303030304,
      "grad_norm": 0.3466205298900604,
      "learning_rate": 1.8354377104377104e-05,
      "loss": 0.0221,
      "step": 1180
    },
    {
      "epoch": 1.4363636363636363,
      "grad_norm": 0.26402780413627625,
      "learning_rate": 1.834385521885522e-05,
      "loss": 0.0225,
      "step": 1185
    },
    {
      "epoch": 1.4424242424242424,
      "grad_norm": 0.5900474786758423,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0215,
      "step": 1190
    },
    {
      "epoch": 1.4484848484848485,
      "grad_norm": 0.317525178194046,
      "learning_rate": 1.832281144781145e-05,
      "loss": 0.0231,
      "step": 1195
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 0.3154687285423279,
      "learning_rate": 1.8312289562289565e-05,
      "loss": 0.0236,
      "step": 1200
    },
    {
      "epoch": 1.4606060606060607,
      "grad_norm": 0.2829839885234833,
      "learning_rate": 1.8301767676767678e-05,
      "loss": 0.0223,
      "step": 1205
    },
    {
      "epoch": 1.4666666666666666,
      "grad_norm": 0.25394293665885925,
      "learning_rate": 1.8291245791245794e-05,
      "loss": 0.0214,
      "step": 1210
    },
    {
      "epoch": 1.4727272727272727,
      "grad_norm": 0.410250723361969,
      "learning_rate": 1.8280723905723906e-05,
      "loss": 0.0217,
      "step": 1215
    },
    {
      "epoch": 1.4787878787878788,
      "grad_norm": 0.5648049116134644,
      "learning_rate": 1.8270202020202023e-05,
      "loss": 0.0219,
      "step": 1220
    },
    {
      "epoch": 1.4848484848484849,
      "grad_norm": 0.2602587044239044,
      "learning_rate": 1.8259680134680135e-05,
      "loss": 0.0215,
      "step": 1225
    },
    {
      "epoch": 1.490909090909091,
      "grad_norm": 0.20647259056568146,
      "learning_rate": 1.824915824915825e-05,
      "loss": 0.0224,
      "step": 1230
    },
    {
      "epoch": 1.496969696969697,
      "grad_norm": 0.23807981610298157,
      "learning_rate": 1.8238636363636367e-05,
      "loss": 0.0219,
      "step": 1235
    },
    {
      "epoch": 1.5006060606060605,
      "eval_average": 0.5624935483503481,
      "eval_crossner_ai": 0.505494505444576,
      "eval_crossner_literature": 0.5803468207591225,
      "eval_crossner_music": 0.6979388769932665,
      "eval_crossner_politics": 0.6214689265035891,
      "eval_crossner_science": 0.5559174809488194,
      "eval_mit-movie": 0.5246753246252305,
      "eval_mit-restaurant": 0.45161290317783304,
      "eval_runtime": 21.134,
      "eval_samples_per_second": 33.122,
      "eval_steps_per_second": 0.331,
      "step": 1238
    },
    {
      "epoch": 1.503030303030303,
      "grad_norm": 0.25991180539131165,
      "learning_rate": 1.822811447811448e-05,
      "loss": 0.022,
      "step": 1240
    },
    {
      "epoch": 1.509090909090909,
      "grad_norm": 0.2780468463897705,
      "learning_rate": 1.8217592592592593e-05,
      "loss": 0.0235,
      "step": 1245
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 0.25895485281944275,
      "learning_rate": 1.820707070707071e-05,
      "loss": 0.0225,
      "step": 1250
    },
    {
      "epoch": 1.5212121212121212,
      "grad_norm": 0.3120538890361786,
      "learning_rate": 1.819654882154882e-05,
      "loss": 0.021,
      "step": 1255
    },
    {
      "epoch": 1.5272727272727273,
      "grad_norm": 0.2695126533508301,
      "learning_rate": 1.8186026936026938e-05,
      "loss": 0.0212,
      "step": 1260
    },
    {
      "epoch": 1.5333333333333334,
      "grad_norm": 0.3700115978717804,
      "learning_rate": 1.817550505050505e-05,
      "loss": 0.0224,
      "step": 1265
    },
    {
      "epoch": 1.5393939393939393,
      "grad_norm": 0.46398699283599854,
      "learning_rate": 1.8164983164983166e-05,
      "loss": 0.0211,
      "step": 1270
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 0.3886953890323639,
      "learning_rate": 1.8154461279461283e-05,
      "loss": 0.0218,
      "step": 1275
    },
    {
      "epoch": 1.5515151515151515,
      "grad_norm": 0.2502823770046234,
      "learning_rate": 1.8143939393939395e-05,
      "loss": 0.0215,
      "step": 1280
    },
    {
      "epoch": 1.5575757575757576,
      "grad_norm": 0.28438419103622437,
      "learning_rate": 1.813341750841751e-05,
      "loss": 0.0226,
      "step": 1285
    },
    {
      "epoch": 1.5636363636363637,
      "grad_norm": 0.2821308374404907,
      "learning_rate": 1.8122895622895624e-05,
      "loss": 0.0222,
      "step": 1290
    },
    {
      "epoch": 1.5696969696969696,
      "grad_norm": 0.2962355315685272,
      "learning_rate": 1.811237373737374e-05,
      "loss": 0.0217,
      "step": 1295
    },
    {
      "epoch": 1.5757575757575757,
      "grad_norm": 0.22602900862693787,
      "learning_rate": 1.8101851851851853e-05,
      "loss": 0.0221,
      "step": 1300
    },
    {
      "epoch": 1.5818181818181818,
      "grad_norm": 0.22668851912021637,
      "learning_rate": 1.8091329966329965e-05,
      "loss": 0.0206,
      "step": 1305
    },
    {
      "epoch": 1.587878787878788,
      "grad_norm": 0.21950723230838776,
      "learning_rate": 1.8080808080808085e-05,
      "loss": 0.0221,
      "step": 1310
    },
    {
      "epoch": 1.593939393939394,
      "grad_norm": 0.24155132472515106,
      "learning_rate": 1.8070286195286198e-05,
      "loss": 0.0229,
      "step": 1315
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.3180074393749237,
      "learning_rate": 1.805976430976431e-05,
      "loss": 0.0217,
      "step": 1320
    },
    {
      "epoch": 1.606060606060606,
      "grad_norm": 0.24955514073371887,
      "learning_rate": 1.8049242424242426e-05,
      "loss": 0.0233,
      "step": 1325
    },
    {
      "epoch": 1.612121212121212,
      "grad_norm": 0.2189788669347763,
      "learning_rate": 1.803872053872054e-05,
      "loss": 0.0226,
      "step": 1330
    },
    {
      "epoch": 1.6181818181818182,
      "grad_norm": 0.29159337282180786,
      "learning_rate": 1.8028198653198655e-05,
      "loss": 0.0222,
      "step": 1335
    },
    {
      "epoch": 1.6242424242424243,
      "grad_norm": 0.2182127833366394,
      "learning_rate": 1.8017676767676768e-05,
      "loss": 0.0212,
      "step": 1340
    },
    {
      "epoch": 1.6303030303030304,
      "grad_norm": 0.356303870677948,
      "learning_rate": 1.8007154882154884e-05,
      "loss": 0.0233,
      "step": 1345
    },
    {
      "epoch": 1.6363636363636365,
      "grad_norm": 0.34199491143226624,
      "learning_rate": 1.7996632996633e-05,
      "loss": 0.0216,
      "step": 1350
    },
    {
      "epoch": 1.6424242424242423,
      "grad_norm": 0.28165844082832336,
      "learning_rate": 1.7986111111111113e-05,
      "loss": 0.0217,
      "step": 1355
    },
    {
      "epoch": 1.6484848484848484,
      "grad_norm": 0.19479770958423615,
      "learning_rate": 1.797558922558923e-05,
      "loss": 0.0209,
      "step": 1360
    },
    {
      "epoch": 1.6545454545454545,
      "grad_norm": 0.22598131000995636,
      "learning_rate": 1.796506734006734e-05,
      "loss": 0.0215,
      "step": 1365
    },
    {
      "epoch": 1.6606060606060606,
      "grad_norm": 0.2755233645439148,
      "learning_rate": 1.7954545454545454e-05,
      "loss": 0.0232,
      "step": 1370
    },
    {
      "epoch": 1.6666666666666667,
      "grad_norm": 0.2936984896659851,
      "learning_rate": 1.794402356902357e-05,
      "loss": 0.0214,
      "step": 1375
    },
    {
      "epoch": 1.6727272727272726,
      "grad_norm": 0.31211191415786743,
      "learning_rate": 1.7933501683501683e-05,
      "loss": 0.0217,
      "step": 1380
    },
    {
      "epoch": 1.6787878787878787,
      "grad_norm": 0.28766417503356934,
      "learning_rate": 1.79229797979798e-05,
      "loss": 0.0213,
      "step": 1385
    },
    {
      "epoch": 1.6848484848484848,
      "grad_norm": 0.26861152052879333,
      "learning_rate": 1.7912457912457915e-05,
      "loss": 0.0214,
      "step": 1390
    },
    {
      "epoch": 1.690909090909091,
      "grad_norm": 0.2387586236000061,
      "learning_rate": 1.7901936026936028e-05,
      "loss": 0.0218,
      "step": 1395
    },
    {
      "epoch": 1.696969696969697,
      "grad_norm": 0.3855009973049164,
      "learning_rate": 1.7891414141414144e-05,
      "loss": 0.022,
      "step": 1400
    },
    {
      "epoch": 1.7030303030303031,
      "grad_norm": 0.30070140957832336,
      "learning_rate": 1.7880892255892257e-05,
      "loss": 0.0217,
      "step": 1405
    },
    {
      "epoch": 1.709090909090909,
      "grad_norm": 0.3289494216442108,
      "learning_rate": 1.7870370370370373e-05,
      "loss": 0.0215,
      "step": 1410
    },
    {
      "epoch": 1.715151515151515,
      "grad_norm": 0.23573727905750275,
      "learning_rate": 1.7859848484848485e-05,
      "loss": 0.0212,
      "step": 1415
    },
    {
      "epoch": 1.7212121212121212,
      "grad_norm": 0.24287883937358856,
      "learning_rate": 1.78493265993266e-05,
      "loss": 0.0226,
      "step": 1420
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.20747259259223938,
      "learning_rate": 1.7838804713804718e-05,
      "loss": 0.0222,
      "step": 1425
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.4425068497657776,
      "learning_rate": 1.782828282828283e-05,
      "loss": 0.0226,
      "step": 1430
    },
    {
      "epoch": 1.7393939393939395,
      "grad_norm": 0.3694164454936981,
      "learning_rate": 1.7817760942760943e-05,
      "loss": 0.0228,
      "step": 1435
    },
    {
      "epoch": 1.7454545454545454,
      "grad_norm": 0.2606164515018463,
      "learning_rate": 1.780723905723906e-05,
      "loss": 0.0225,
      "step": 1440
    },
    {
      "epoch": 1.7515151515151515,
      "grad_norm": 0.3158935308456421,
      "learning_rate": 1.7796717171717172e-05,
      "loss": 0.0224,
      "step": 1445
    },
    {
      "epoch": 1.7575757575757576,
      "grad_norm": 0.2738884687423706,
      "learning_rate": 1.7786195286195288e-05,
      "loss": 0.0218,
      "step": 1450
    },
    {
      "epoch": 1.7636363636363637,
      "grad_norm": 0.2958800196647644,
      "learning_rate": 1.77756734006734e-05,
      "loss": 0.0223,
      "step": 1455
    },
    {
      "epoch": 1.7696969696969698,
      "grad_norm": 0.2516324520111084,
      "learning_rate": 1.7765151515151517e-05,
      "loss": 0.0233,
      "step": 1460
    },
    {
      "epoch": 1.7757575757575759,
      "grad_norm": 0.30084657669067383,
      "learning_rate": 1.7754629629629633e-05,
      "loss": 0.023,
      "step": 1465
    },
    {
      "epoch": 1.7818181818181817,
      "grad_norm": 0.25291115045547485,
      "learning_rate": 1.7744107744107745e-05,
      "loss": 0.0226,
      "step": 1470
    },
    {
      "epoch": 1.7878787878787878,
      "grad_norm": 0.26246267557144165,
      "learning_rate": 1.773358585858586e-05,
      "loss": 0.0214,
      "step": 1475
    },
    {
      "epoch": 1.793939393939394,
      "grad_norm": 0.23570364713668823,
      "learning_rate": 1.7723063973063974e-05,
      "loss": 0.0223,
      "step": 1480
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.24564942717552185,
      "learning_rate": 1.7712542087542087e-05,
      "loss": 0.0215,
      "step": 1485
    },
    {
      "epoch": 1.8060606060606061,
      "grad_norm": 0.24947935342788696,
      "learning_rate": 1.7702020202020203e-05,
      "loss": 0.0225,
      "step": 1490
    },
    {
      "epoch": 1.812121212121212,
      "grad_norm": 0.22033944725990295,
      "learning_rate": 1.769149831649832e-05,
      "loss": 0.0227,
      "step": 1495
    },
    {
      "epoch": 1.8181818181818181,
      "grad_norm": 0.24024741351604462,
      "learning_rate": 1.768097643097643e-05,
      "loss": 0.0216,
      "step": 1500
    },
    {
      "epoch": 1.8242424242424242,
      "grad_norm": 0.2044433355331421,
      "learning_rate": 1.7670454545454548e-05,
      "loss": 0.021,
      "step": 1505
    },
    {
      "epoch": 1.8303030303030303,
      "grad_norm": 0.20354753732681274,
      "learning_rate": 1.765993265993266e-05,
      "loss": 0.0222,
      "step": 1510
    },
    {
      "epoch": 1.8363636363636364,
      "grad_norm": 0.3119855225086212,
      "learning_rate": 1.7649410774410777e-05,
      "loss": 0.0226,
      "step": 1515
    },
    {
      "epoch": 1.8424242424242425,
      "grad_norm": 0.32089123129844666,
      "learning_rate": 1.763888888888889e-05,
      "loss": 0.0219,
      "step": 1520
    },
    {
      "epoch": 1.8484848484848484,
      "grad_norm": 0.2285999357700348,
      "learning_rate": 1.7628367003367005e-05,
      "loss": 0.0219,
      "step": 1525
    },
    {
      "epoch": 1.8545454545454545,
      "grad_norm": 0.3225323259830475,
      "learning_rate": 1.7617845117845118e-05,
      "loss": 0.0239,
      "step": 1530
    },
    {
      "epoch": 1.8606060606060606,
      "grad_norm": 0.2581339180469513,
      "learning_rate": 1.7607323232323234e-05,
      "loss": 0.023,
      "step": 1535
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.31842324137687683,
      "learning_rate": 1.759680134680135e-05,
      "loss": 0.0226,
      "step": 1540
    },
    {
      "epoch": 1.8727272727272728,
      "grad_norm": 0.2776392698287964,
      "learning_rate": 1.7586279461279463e-05,
      "loss": 0.0222,
      "step": 1545
    },
    {
      "epoch": 1.878787878787879,
      "grad_norm": 0.23660686612129211,
      "learning_rate": 1.7575757575757576e-05,
      "loss": 0.021,
      "step": 1550
    },
    {
      "epoch": 1.8848484848484848,
      "grad_norm": 0.3247326612472534,
      "learning_rate": 1.756523569023569e-05,
      "loss": 0.0241,
      "step": 1555
    },
    {
      "epoch": 1.8909090909090909,
      "grad_norm": 0.335563063621521,
      "learning_rate": 1.7554713804713804e-05,
      "loss": 0.0229,
      "step": 1560
    },
    {
      "epoch": 1.896969696969697,
      "grad_norm": 0.2251315861940384,
      "learning_rate": 1.754419191919192e-05,
      "loss": 0.0221,
      "step": 1565
    },
    {
      "epoch": 1.903030303030303,
      "grad_norm": 0.38145750761032104,
      "learning_rate": 1.7533670033670036e-05,
      "loss": 0.0221,
      "step": 1570
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 0.24702540040016174,
      "learning_rate": 1.752314814814815e-05,
      "loss": 0.0221,
      "step": 1575
    },
    {
      "epoch": 1.915151515151515,
      "grad_norm": 0.319678395986557,
      "learning_rate": 1.7512626262626265e-05,
      "loss": 0.0227,
      "step": 1580
    },
    {
      "epoch": 1.9212121212121211,
      "grad_norm": 0.4400114417076111,
      "learning_rate": 1.7502104377104378e-05,
      "loss": 0.0226,
      "step": 1585
    },
    {
      "epoch": 1.9272727272727272,
      "grad_norm": 0.22215083241462708,
      "learning_rate": 1.7491582491582494e-05,
      "loss": 0.0222,
      "step": 1590
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.4261590838432312,
      "learning_rate": 1.7481060606060607e-05,
      "loss": 0.0239,
      "step": 1595
    },
    {
      "epoch": 1.9393939393939394,
      "grad_norm": 0.40792784094810486,
      "learning_rate": 1.7470538720538723e-05,
      "loss": 0.0234,
      "step": 1600
    },
    {
      "epoch": 1.9454545454545455,
      "grad_norm": 0.30221790075302124,
      "learning_rate": 1.746001683501684e-05,
      "loss": 0.022,
      "step": 1605
    },
    {
      "epoch": 1.9515151515151514,
      "grad_norm": 0.2864539921283722,
      "learning_rate": 1.744949494949495e-05,
      "loss": 0.0229,
      "step": 1610
    },
    {
      "epoch": 1.9575757575757575,
      "grad_norm": 0.2444179654121399,
      "learning_rate": 1.7438973063973064e-05,
      "loss": 0.0221,
      "step": 1615
    },
    {
      "epoch": 1.9636363636363636,
      "grad_norm": 0.2774612009525299,
      "learning_rate": 1.742845117845118e-05,
      "loss": 0.0233,
      "step": 1620
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 0.2556855082511902,
      "learning_rate": 1.7417929292929293e-05,
      "loss": 0.0218,
      "step": 1625
    },
    {
      "epoch": 1.9757575757575758,
      "grad_norm": 0.19792823493480682,
      "learning_rate": 1.740740740740741e-05,
      "loss": 0.0223,
      "step": 1630
    },
    {
      "epoch": 1.981818181818182,
      "grad_norm": 0.26134195923805237,
      "learning_rate": 1.7396885521885522e-05,
      "loss": 0.0222,
      "step": 1635
    },
    {
      "epoch": 1.9878787878787878,
      "grad_norm": 0.33057186007499695,
      "learning_rate": 1.7386363636363638e-05,
      "loss": 0.022,
      "step": 1640
    },
    {
      "epoch": 1.993939393939394,
      "grad_norm": 0.2483343780040741,
      "learning_rate": 1.7375841750841754e-05,
      "loss": 0.0214,
      "step": 1645
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.2338462620973587,
      "learning_rate": 1.7365319865319867e-05,
      "loss": 0.0213,
      "step": 1650
    },
    {
      "epoch": 2.0,
      "eval_average": 0.5840201620937151,
      "eval_crossner_ai": 0.556894243591122,
      "eval_crossner_literature": 0.5865384614884604,
      "eval_crossner_music": 0.7562408222701237,
      "eval_crossner_politics": 0.6507413508559884,
      "eval_crossner_science": 0.6157303370285195,
      "eval_mit-movie": 0.5271739129937825,
      "eval_mit-restaurant": 0.3948220064280098,
      "eval_runtime": 20.4005,
      "eval_samples_per_second": 34.313,
      "eval_steps_per_second": 0.343,
      "step": 1650
    },
    {
      "epoch": 2.006060606060606,
      "grad_norm": 0.1618904173374176,
      "learning_rate": 1.7354797979797983e-05,
      "loss": 0.016,
      "step": 1655
    },
    {
      "epoch": 2.012121212121212,
      "grad_norm": 0.3419892489910126,
      "learning_rate": 1.7344276094276095e-05,
      "loss": 0.0118,
      "step": 1660
    },
    {
      "epoch": 2.018181818181818,
      "grad_norm": 0.34006646275520325,
      "learning_rate": 1.733375420875421e-05,
      "loss": 0.012,
      "step": 1665
    },
    {
      "epoch": 2.0242424242424244,
      "grad_norm": 0.25033968687057495,
      "learning_rate": 1.7323232323232324e-05,
      "loss": 0.0118,
      "step": 1670
    },
    {
      "epoch": 2.0303030303030303,
      "grad_norm": 0.2597855031490326,
      "learning_rate": 1.7312710437710437e-05,
      "loss": 0.0104,
      "step": 1675
    },
    {
      "epoch": 2.036363636363636,
      "grad_norm": 0.266411155462265,
      "learning_rate": 1.7302188552188553e-05,
      "loss": 0.0117,
      "step": 1680
    },
    {
      "epoch": 2.0424242424242425,
      "grad_norm": 0.2722558379173279,
      "learning_rate": 1.729166666666667e-05,
      "loss": 0.0104,
      "step": 1685
    },
    {
      "epoch": 2.0484848484848484,
      "grad_norm": 0.2640942335128784,
      "learning_rate": 1.7281144781144782e-05,
      "loss": 0.0103,
      "step": 1690
    },
    {
      "epoch": 2.0545454545454547,
      "grad_norm": 0.2571050524711609,
      "learning_rate": 1.7270622895622898e-05,
      "loss": 0.0109,
      "step": 1695
    },
    {
      "epoch": 2.0606060606060606,
      "grad_norm": 0.26760923862457275,
      "learning_rate": 1.726010101010101e-05,
      "loss": 0.0105,
      "step": 1700
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.3142719566822052,
      "learning_rate": 1.7249579124579127e-05,
      "loss": 0.0106,
      "step": 1705
    },
    {
      "epoch": 2.0727272727272728,
      "grad_norm": 0.2843815088272095,
      "learning_rate": 1.723905723905724e-05,
      "loss": 0.0109,
      "step": 1710
    },
    {
      "epoch": 2.0787878787878786,
      "grad_norm": 0.3560183644294739,
      "learning_rate": 1.7228535353535355e-05,
      "loss": 0.0108,
      "step": 1715
    },
    {
      "epoch": 2.084848484848485,
      "grad_norm": 0.37386396527290344,
      "learning_rate": 1.721801346801347e-05,
      "loss": 0.0117,
      "step": 1720
    },
    {
      "epoch": 2.090909090909091,
      "grad_norm": 0.31739798188209534,
      "learning_rate": 1.7207491582491584e-05,
      "loss": 0.0101,
      "step": 1725
    },
    {
      "epoch": 2.096969696969697,
      "grad_norm": 0.2762084901332855,
      "learning_rate": 1.71969696969697e-05,
      "loss": 0.0119,
      "step": 1730
    },
    {
      "epoch": 2.103030303030303,
      "grad_norm": 0.28096550703048706,
      "learning_rate": 1.7186447811447813e-05,
      "loss": 0.0111,
      "step": 1735
    },
    {
      "epoch": 2.109090909090909,
      "grad_norm": 0.30281853675842285,
      "learning_rate": 1.7175925925925926e-05,
      "loss": 0.011,
      "step": 1740
    },
    {
      "epoch": 2.1151515151515152,
      "grad_norm": 0.33616048097610474,
      "learning_rate": 1.7165404040404042e-05,
      "loss": 0.0106,
      "step": 1745
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 0.25687795877456665,
      "learning_rate": 1.7154882154882154e-05,
      "loss": 0.01,
      "step": 1750
    },
    {
      "epoch": 2.1272727272727274,
      "grad_norm": 0.3211696743965149,
      "learning_rate": 1.714436026936027e-05,
      "loss": 0.0115,
      "step": 1755
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.24808833003044128,
      "learning_rate": 1.7133838383838387e-05,
      "loss": 0.0105,
      "step": 1760
    },
    {
      "epoch": 2.139393939393939,
      "grad_norm": 0.2579379081726074,
      "learning_rate": 1.71233164983165e-05,
      "loss": 0.011,
      "step": 1765
    },
    {
      "epoch": 2.1454545454545455,
      "grad_norm": 0.28279685974121094,
      "learning_rate": 1.7112794612794615e-05,
      "loss": 0.0113,
      "step": 1770
    },
    {
      "epoch": 2.1515151515151514,
      "grad_norm": 0.4101005792617798,
      "learning_rate": 1.7102272727272728e-05,
      "loss": 0.0108,
      "step": 1775
    },
    {
      "epoch": 2.1575757575757577,
      "grad_norm": 0.3944685161113739,
      "learning_rate": 1.7091750841750844e-05,
      "loss": 0.0119,
      "step": 1780
    },
    {
      "epoch": 2.1636363636363636,
      "grad_norm": 0.28269174695014954,
      "learning_rate": 1.7081228956228957e-05,
      "loss": 0.0113,
      "step": 1785
    },
    {
      "epoch": 2.16969696969697,
      "grad_norm": 0.31652042269706726,
      "learning_rate": 1.707070707070707e-05,
      "loss": 0.0118,
      "step": 1790
    },
    {
      "epoch": 2.175757575757576,
      "grad_norm": 0.2645286023616791,
      "learning_rate": 1.706018518518519e-05,
      "loss": 0.0104,
      "step": 1795
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 0.3718743920326233,
      "learning_rate": 1.7049663299663302e-05,
      "loss": 0.0116,
      "step": 1800
    },
    {
      "epoch": 2.187878787878788,
      "grad_norm": 0.330829381942749,
      "learning_rate": 1.7039141414141414e-05,
      "loss": 0.0112,
      "step": 1805
    },
    {
      "epoch": 2.193939393939394,
      "grad_norm": 0.2694070637226105,
      "learning_rate": 1.702861952861953e-05,
      "loss": 0.0122,
      "step": 1810
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.26952868700027466,
      "learning_rate": 1.7018097643097643e-05,
      "loss": 0.0115,
      "step": 1815
    },
    {
      "epoch": 2.206060606060606,
      "grad_norm": 0.2718161344528198,
      "learning_rate": 1.700757575757576e-05,
      "loss": 0.0108,
      "step": 1820
    },
    {
      "epoch": 2.212121212121212,
      "grad_norm": 0.22478964924812317,
      "learning_rate": 1.6997053872053872e-05,
      "loss": 0.0109,
      "step": 1825
    },
    {
      "epoch": 2.2181818181818183,
      "grad_norm": 0.3214358985424042,
      "learning_rate": 1.6986531986531988e-05,
      "loss": 0.0111,
      "step": 1830
    },
    {
      "epoch": 2.224242424242424,
      "grad_norm": 0.2802523672580719,
      "learning_rate": 1.6976010101010104e-05,
      "loss": 0.0102,
      "step": 1835
    },
    {
      "epoch": 2.2303030303030305,
      "grad_norm": 0.2561115026473999,
      "learning_rate": 1.6965488215488217e-05,
      "loss": 0.0118,
      "step": 1840
    },
    {
      "epoch": 2.2363636363636363,
      "grad_norm": 0.3453032672405243,
      "learning_rate": 1.6954966329966333e-05,
      "loss": 0.011,
      "step": 1845
    },
    {
      "epoch": 2.242424242424242,
      "grad_norm": 0.3034798800945282,
      "learning_rate": 1.6944444444444446e-05,
      "loss": 0.0116,
      "step": 1850
    },
    {
      "epoch": 2.2484848484848485,
      "grad_norm": 0.3479810357093811,
      "learning_rate": 1.6933922558922558e-05,
      "loss": 0.0113,
      "step": 1855
    },
    {
      "epoch": 2.2545454545454544,
      "grad_norm": 0.2488391399383545,
      "learning_rate": 1.6923400673400674e-05,
      "loss": 0.0113,
      "step": 1860
    },
    {
      "epoch": 2.2606060606060607,
      "grad_norm": 0.2648508846759796,
      "learning_rate": 1.691287878787879e-05,
      "loss": 0.0113,
      "step": 1865
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.2935727536678314,
      "learning_rate": 1.6902356902356903e-05,
      "loss": 0.0111,
      "step": 1870
    },
    {
      "epoch": 2.272727272727273,
      "grad_norm": 0.35602104663848877,
      "learning_rate": 1.689183501683502e-05,
      "loss": 0.0116,
      "step": 1875
    },
    {
      "epoch": 2.278787878787879,
      "grad_norm": 0.32553160190582275,
      "learning_rate": 1.6881313131313132e-05,
      "loss": 0.011,
      "step": 1880
    },
    {
      "epoch": 2.2848484848484847,
      "grad_norm": 0.2826681137084961,
      "learning_rate": 1.6870791245791248e-05,
      "loss": 0.0114,
      "step": 1885
    },
    {
      "epoch": 2.290909090909091,
      "grad_norm": 0.3339107930660248,
      "learning_rate": 1.686026936026936e-05,
      "loss": 0.0123,
      "step": 1890
    },
    {
      "epoch": 2.296969696969697,
      "grad_norm": 0.2630999684333801,
      "learning_rate": 1.6849747474747477e-05,
      "loss": 0.0112,
      "step": 1895
    },
    {
      "epoch": 2.303030303030303,
      "grad_norm": 0.29252368211746216,
      "learning_rate": 1.683922558922559e-05,
      "loss": 0.0115,
      "step": 1900
    },
    {
      "epoch": 2.309090909090909,
      "grad_norm": 0.322020560503006,
      "learning_rate": 1.6828703703703706e-05,
      "loss": 0.0122,
      "step": 1905
    },
    {
      "epoch": 2.315151515151515,
      "grad_norm": 0.300738662481308,
      "learning_rate": 1.681818181818182e-05,
      "loss": 0.0115,
      "step": 1910
    },
    {
      "epoch": 2.3212121212121213,
      "grad_norm": 0.30728548765182495,
      "learning_rate": 1.6807659932659934e-05,
      "loss": 0.0118,
      "step": 1915
    },
    {
      "epoch": 2.327272727272727,
      "grad_norm": 0.2880406081676483,
      "learning_rate": 1.6797138047138047e-05,
      "loss": 0.012,
      "step": 1920
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.33597028255462646,
      "learning_rate": 1.6786616161616163e-05,
      "loss": 0.0118,
      "step": 1925
    },
    {
      "epoch": 2.3393939393939394,
      "grad_norm": 0.2946256101131439,
      "learning_rate": 1.6776094276094276e-05,
      "loss": 0.0126,
      "step": 1930
    },
    {
      "epoch": 2.3454545454545452,
      "grad_norm": 0.29263290762901306,
      "learning_rate": 1.6765572390572392e-05,
      "loss": 0.0115,
      "step": 1935
    },
    {
      "epoch": 2.3515151515151516,
      "grad_norm": 0.28232911229133606,
      "learning_rate": 1.6755050505050508e-05,
      "loss": 0.0115,
      "step": 1940
    },
    {
      "epoch": 2.3575757575757574,
      "grad_norm": 0.28268107771873474,
      "learning_rate": 1.674452861952862e-05,
      "loss": 0.0109,
      "step": 1945
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 0.28213533759117126,
      "learning_rate": 1.6734006734006737e-05,
      "loss": 0.011,
      "step": 1950
    },
    {
      "epoch": 2.3696969696969696,
      "grad_norm": 0.22684381902217865,
      "learning_rate": 1.672348484848485e-05,
      "loss": 0.0117,
      "step": 1955
    },
    {
      "epoch": 2.375757575757576,
      "grad_norm": 0.2657870650291443,
      "learning_rate": 1.6712962962962966e-05,
      "loss": 0.0113,
      "step": 1960
    },
    {
      "epoch": 2.381818181818182,
      "grad_norm": 0.24432873725891113,
      "learning_rate": 1.6702441077441078e-05,
      "loss": 0.0128,
      "step": 1965
    },
    {
      "epoch": 2.3878787878787877,
      "grad_norm": 0.24602483212947845,
      "learning_rate": 1.669191919191919e-05,
      "loss": 0.0117,
      "step": 1970
    },
    {
      "epoch": 2.393939393939394,
      "grad_norm": 0.3458459675312042,
      "learning_rate": 1.6681397306397307e-05,
      "loss": 0.0118,
      "step": 1975
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.4982515573501587,
      "learning_rate": 1.6670875420875423e-05,
      "loss": 0.0119,
      "step": 1980
    },
    {
      "epoch": 2.4060606060606062,
      "grad_norm": 0.30867841839790344,
      "learning_rate": 1.6660353535353536e-05,
      "loss": 0.0123,
      "step": 1985
    },
    {
      "epoch": 2.412121212121212,
      "grad_norm": 0.3707466125488281,
      "learning_rate": 1.6649831649831652e-05,
      "loss": 0.0117,
      "step": 1990
    },
    {
      "epoch": 2.418181818181818,
      "grad_norm": 0.3118560314178467,
      "learning_rate": 1.6639309764309765e-05,
      "loss": 0.0121,
      "step": 1995
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 0.3062528371810913,
      "learning_rate": 1.662878787878788e-05,
      "loss": 0.0131,
      "step": 2000
    },
    {
      "epoch": 2.43030303030303,
      "grad_norm": 0.31685858964920044,
      "learning_rate": 1.6618265993265993e-05,
      "loss": 0.012,
      "step": 2005
    },
    {
      "epoch": 2.4363636363636365,
      "grad_norm": 0.30381453037261963,
      "learning_rate": 1.660774410774411e-05,
      "loss": 0.0115,
      "step": 2010
    },
    {
      "epoch": 2.4424242424242424,
      "grad_norm": 0.3506297469139099,
      "learning_rate": 1.6597222222222225e-05,
      "loss": 0.0115,
      "step": 2015
    },
    {
      "epoch": 2.4484848484848483,
      "grad_norm": 0.33299145102500916,
      "learning_rate": 1.6586700336700338e-05,
      "loss": 0.0123,
      "step": 2020
    },
    {
      "epoch": 2.4545454545454546,
      "grad_norm": 0.26865124702453613,
      "learning_rate": 1.6576178451178454e-05,
      "loss": 0.0119,
      "step": 2025
    },
    {
      "epoch": 2.4606060606060605,
      "grad_norm": 0.22771459817886353,
      "learning_rate": 1.6565656565656567e-05,
      "loss": 0.0128,
      "step": 2030
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.33867815136909485,
      "learning_rate": 1.655513468013468e-05,
      "loss": 0.0121,
      "step": 2035
    },
    {
      "epoch": 2.4727272727272727,
      "grad_norm": 0.3852105736732483,
      "learning_rate": 1.6544612794612796e-05,
      "loss": 0.0127,
      "step": 2040
    },
    {
      "epoch": 2.478787878787879,
      "grad_norm": 0.3353423774242401,
      "learning_rate": 1.653409090909091e-05,
      "loss": 0.0123,
      "step": 2045
    },
    {
      "epoch": 2.484848484848485,
      "grad_norm": 0.2938811480998993,
      "learning_rate": 1.6523569023569024e-05,
      "loss": 0.0122,
      "step": 2050
    },
    {
      "epoch": 2.4909090909090907,
      "grad_norm": 0.29160669445991516,
      "learning_rate": 1.651304713804714e-05,
      "loss": 0.0126,
      "step": 2055
    },
    {
      "epoch": 2.496969696969697,
      "grad_norm": 0.3103395998477936,
      "learning_rate": 1.6502525252525253e-05,
      "loss": 0.0123,
      "step": 2060
    },
    {
      "epoch": 2.4993939393939395,
      "eval_average": 0.5548661997563403,
      "eval_crossner_ai": 0.5213032580953034,
      "eval_crossner_literature": 0.5486111110609938,
      "eval_crossner_music": 0.7462473194638533,
      "eval_crossner_politics": 0.5534693877050143,
      "eval_crossner_science": 0.5702036441085685,
      "eval_mit-movie": 0.5365853658039264,
      "eval_mit-restaurant": 0.40764331205672244,
      "eval_runtime": 21.1254,
      "eval_samples_per_second": 33.135,
      "eval_steps_per_second": 0.331,
      "step": 2062
    },
    {
      "epoch": 2.503030303030303,
      "grad_norm": 0.3222511410713196,
      "learning_rate": 1.649200336700337e-05,
      "loss": 0.0117,
      "step": 2065
    },
    {
      "epoch": 2.5090909090909093,
      "grad_norm": 0.2923223674297333,
      "learning_rate": 1.6481481481481482e-05,
      "loss": 0.0116,
      "step": 2070
    },
    {
      "epoch": 2.515151515151515,
      "grad_norm": 0.26493188738822937,
      "learning_rate": 1.6470959595959598e-05,
      "loss": 0.0123,
      "step": 2075
    },
    {
      "epoch": 2.521212121212121,
      "grad_norm": 0.25480538606643677,
      "learning_rate": 1.646043771043771e-05,
      "loss": 0.0123,
      "step": 2080
    },
    {
      "epoch": 2.5272727272727273,
      "grad_norm": 0.28262826800346375,
      "learning_rate": 1.6449915824915827e-05,
      "loss": 0.0112,
      "step": 2085
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.2994636297225952,
      "learning_rate": 1.6439393939393943e-05,
      "loss": 0.011,
      "step": 2090
    },
    {
      "epoch": 2.5393939393939395,
      "grad_norm": 0.284716933965683,
      "learning_rate": 1.6428872053872056e-05,
      "loss": 0.0113,
      "step": 2095
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 0.2763708829879761,
      "learning_rate": 1.641835016835017e-05,
      "loss": 0.0115,
      "step": 2100
    },
    {
      "epoch": 2.5515151515151517,
      "grad_norm": 0.34547945857048035,
      "learning_rate": 1.6407828282828284e-05,
      "loss": 0.0114,
      "step": 2105
    },
    {
      "epoch": 2.5575757575757576,
      "grad_norm": 0.3384125530719757,
      "learning_rate": 1.6397306397306397e-05,
      "loss": 0.012,
      "step": 2110
    },
    {
      "epoch": 2.5636363636363635,
      "grad_norm": 0.3004201352596283,
      "learning_rate": 1.6386784511784513e-05,
      "loss": 0.0118,
      "step": 2115
    },
    {
      "epoch": 2.56969696969697,
      "grad_norm": 0.3148300349712372,
      "learning_rate": 1.6376262626262626e-05,
      "loss": 0.012,
      "step": 2120
    },
    {
      "epoch": 2.5757575757575757,
      "grad_norm": 0.2848632335662842,
      "learning_rate": 1.6365740740740742e-05,
      "loss": 0.0124,
      "step": 2125
    },
    {
      "epoch": 2.581818181818182,
      "grad_norm": 0.3525362014770508,
      "learning_rate": 1.6355218855218858e-05,
      "loss": 0.0111,
      "step": 2130
    },
    {
      "epoch": 2.587878787878788,
      "grad_norm": 0.23936454951763153,
      "learning_rate": 1.634469696969697e-05,
      "loss": 0.0127,
      "step": 2135
    },
    {
      "epoch": 2.5939393939393938,
      "grad_norm": 0.3378409743309021,
      "learning_rate": 1.6334175084175087e-05,
      "loss": 0.0131,
      "step": 2140
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.2590164244174957,
      "learning_rate": 1.63236531986532e-05,
      "loss": 0.0115,
      "step": 2145
    },
    {
      "epoch": 2.606060606060606,
      "grad_norm": 0.3690125644207001,
      "learning_rate": 1.6313131313131316e-05,
      "loss": 0.0133,
      "step": 2150
    },
    {
      "epoch": 2.6121212121212123,
      "grad_norm": 0.3291385769844055,
      "learning_rate": 1.630260942760943e-05,
      "loss": 0.0124,
      "step": 2155
    },
    {
      "epoch": 2.618181818181818,
      "grad_norm": 0.30285370349884033,
      "learning_rate": 1.629208754208754e-05,
      "loss": 0.0116,
      "step": 2160
    },
    {
      "epoch": 2.624242424242424,
      "grad_norm": 0.26839324831962585,
      "learning_rate": 1.6281565656565657e-05,
      "loss": 0.0123,
      "step": 2165
    },
    {
      "epoch": 2.6303030303030304,
      "grad_norm": 0.2724572718143463,
      "learning_rate": 1.6271043771043773e-05,
      "loss": 0.0131,
      "step": 2170
    },
    {
      "epoch": 2.6363636363636362,
      "grad_norm": 0.2786667048931122,
      "learning_rate": 1.6260521885521886e-05,
      "loss": 0.0122,
      "step": 2175
    },
    {
      "epoch": 2.6424242424242426,
      "grad_norm": 0.3100707232952118,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 0.0122,
      "step": 2180
    },
    {
      "epoch": 2.6484848484848484,
      "grad_norm": 0.3047126829624176,
      "learning_rate": 1.6239478114478115e-05,
      "loss": 0.0121,
      "step": 2185
    },
    {
      "epoch": 2.6545454545454548,
      "grad_norm": 0.27792996168136597,
      "learning_rate": 1.622895622895623e-05,
      "loss": 0.0123,
      "step": 2190
    },
    {
      "epoch": 2.6606060606060606,
      "grad_norm": 0.26950356364250183,
      "learning_rate": 1.6218434343434343e-05,
      "loss": 0.0118,
      "step": 2195
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.2568863332271576,
      "learning_rate": 1.620791245791246e-05,
      "loss": 0.0132,
      "step": 2200
    },
    {
      "epoch": 2.672727272727273,
      "grad_norm": 0.3198610246181488,
      "learning_rate": 1.6197390572390576e-05,
      "loss": 0.0126,
      "step": 2205
    },
    {
      "epoch": 2.6787878787878787,
      "grad_norm": 0.3166673183441162,
      "learning_rate": 1.6186868686868688e-05,
      "loss": 0.0127,
      "step": 2210
    },
    {
      "epoch": 2.684848484848485,
      "grad_norm": 0.289173424243927,
      "learning_rate": 1.6176346801346804e-05,
      "loss": 0.013,
      "step": 2215
    },
    {
      "epoch": 2.690909090909091,
      "grad_norm": 0.24265529215335846,
      "learning_rate": 1.6165824915824917e-05,
      "loss": 0.013,
      "step": 2220
    },
    {
      "epoch": 2.696969696969697,
      "grad_norm": 0.32856208086013794,
      "learning_rate": 1.615530303030303e-05,
      "loss": 0.0105,
      "step": 2225
    },
    {
      "epoch": 2.703030303030303,
      "grad_norm": 0.36661237478256226,
      "learning_rate": 1.6144781144781146e-05,
      "loss": 0.0126,
      "step": 2230
    },
    {
      "epoch": 2.709090909090909,
      "grad_norm": 0.30214548110961914,
      "learning_rate": 1.613425925925926e-05,
      "loss": 0.0117,
      "step": 2235
    },
    {
      "epoch": 2.7151515151515153,
      "grad_norm": 0.2760744094848633,
      "learning_rate": 1.6123737373737375e-05,
      "loss": 0.0128,
      "step": 2240
    },
    {
      "epoch": 2.721212121212121,
      "grad_norm": 0.36803749203681946,
      "learning_rate": 1.611321548821549e-05,
      "loss": 0.0132,
      "step": 2245
    },
    {
      "epoch": 2.727272727272727,
      "grad_norm": 0.48875054717063904,
      "learning_rate": 1.6102693602693603e-05,
      "loss": 0.0131,
      "step": 2250
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.3877354860305786,
      "learning_rate": 1.609217171717172e-05,
      "loss": 0.0135,
      "step": 2255
    },
    {
      "epoch": 2.7393939393939393,
      "grad_norm": 0.2742873728275299,
      "learning_rate": 1.6081649831649832e-05,
      "loss": 0.0125,
      "step": 2260
    },
    {
      "epoch": 2.7454545454545456,
      "grad_norm": 0.3105032742023468,
      "learning_rate": 1.6071127946127948e-05,
      "loss": 0.012,
      "step": 2265
    },
    {
      "epoch": 2.7515151515151515,
      "grad_norm": 0.30752795934677124,
      "learning_rate": 1.606060606060606e-05,
      "loss": 0.0129,
      "step": 2270
    },
    {
      "epoch": 2.757575757575758,
      "grad_norm": 0.2765566110610962,
      "learning_rate": 1.6050084175084177e-05,
      "loss": 0.013,
      "step": 2275
    },
    {
      "epoch": 2.7636363636363637,
      "grad_norm": 0.2795851230621338,
      "learning_rate": 1.6039562289562293e-05,
      "loss": 0.0126,
      "step": 2280
    },
    {
      "epoch": 2.7696969696969695,
      "grad_norm": 0.3241623640060425,
      "learning_rate": 1.6029040404040406e-05,
      "loss": 0.0121,
      "step": 2285
    },
    {
      "epoch": 2.775757575757576,
      "grad_norm": 0.2579801082611084,
      "learning_rate": 1.601851851851852e-05,
      "loss": 0.0119,
      "step": 2290
    },
    {
      "epoch": 2.7818181818181817,
      "grad_norm": 0.26610973477363586,
      "learning_rate": 1.6007996632996635e-05,
      "loss": 0.012,
      "step": 2295
    },
    {
      "epoch": 2.787878787878788,
      "grad_norm": 0.27912789583206177,
      "learning_rate": 1.5997474747474747e-05,
      "loss": 0.0129,
      "step": 2300
    },
    {
      "epoch": 2.793939393939394,
      "grad_norm": 0.2679251432418823,
      "learning_rate": 1.5986952861952863e-05,
      "loss": 0.0126,
      "step": 2305
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.3572099804878235,
      "learning_rate": 1.597643097643098e-05,
      "loss": 0.0119,
      "step": 2310
    },
    {
      "epoch": 2.806060606060606,
      "grad_norm": 0.3097558915615082,
      "learning_rate": 1.5965909090909092e-05,
      "loss": 0.0129,
      "step": 2315
    },
    {
      "epoch": 2.812121212121212,
      "grad_norm": 0.5956810116767883,
      "learning_rate": 1.5955387205387208e-05,
      "loss": 0.0131,
      "step": 2320
    },
    {
      "epoch": 2.8181818181818183,
      "grad_norm": 0.40670695900917053,
      "learning_rate": 1.594486531986532e-05,
      "loss": 0.013,
      "step": 2325
    },
    {
      "epoch": 2.824242424242424,
      "grad_norm": 0.22743193805217743,
      "learning_rate": 1.5934343434343437e-05,
      "loss": 0.0129,
      "step": 2330
    },
    {
      "epoch": 2.83030303030303,
      "grad_norm": 0.45680683851242065,
      "learning_rate": 1.592382154882155e-05,
      "loss": 0.013,
      "step": 2335
    },
    {
      "epoch": 2.8363636363636364,
      "grad_norm": 0.33112093806266785,
      "learning_rate": 1.5913299663299662e-05,
      "loss": 0.012,
      "step": 2340
    },
    {
      "epoch": 2.8424242424242423,
      "grad_norm": 0.37196117639541626,
      "learning_rate": 1.590277777777778e-05,
      "loss": 0.0131,
      "step": 2345
    },
    {
      "epoch": 2.8484848484848486,
      "grad_norm": 0.22631892561912537,
      "learning_rate": 1.5892255892255895e-05,
      "loss": 0.0118,
      "step": 2350
    },
    {
      "epoch": 2.8545454545454545,
      "grad_norm": 0.4076189696788788,
      "learning_rate": 1.5881734006734007e-05,
      "loss": 0.0119,
      "step": 2355
    },
    {
      "epoch": 2.860606060606061,
      "grad_norm": 0.25830814242362976,
      "learning_rate": 1.5871212121212123e-05,
      "loss": 0.0126,
      "step": 2360
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.30308404564857483,
      "learning_rate": 1.5860690235690236e-05,
      "loss": 0.0118,
      "step": 2365
    },
    {
      "epoch": 2.8727272727272726,
      "grad_norm": 0.3034072518348694,
      "learning_rate": 1.5850168350168352e-05,
      "loss": 0.0124,
      "step": 2370
    },
    {
      "epoch": 2.878787878787879,
      "grad_norm": 0.3159584105014801,
      "learning_rate": 1.5839646464646465e-05,
      "loss": 0.0131,
      "step": 2375
    },
    {
      "epoch": 2.8848484848484848,
      "grad_norm": 0.27374714612960815,
      "learning_rate": 1.582912457912458e-05,
      "loss": 0.0119,
      "step": 2380
    },
    {
      "epoch": 2.890909090909091,
      "grad_norm": 0.35710638761520386,
      "learning_rate": 1.5818602693602697e-05,
      "loss": 0.0127,
      "step": 2385
    },
    {
      "epoch": 2.896969696969697,
      "grad_norm": 0.42968693375587463,
      "learning_rate": 1.580808080808081e-05,
      "loss": 0.0128,
      "step": 2390
    },
    {
      "epoch": 2.903030303030303,
      "grad_norm": 0.31236234307289124,
      "learning_rate": 1.5797558922558926e-05,
      "loss": 0.0135,
      "step": 2395
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 0.25568437576293945,
      "learning_rate": 1.578703703703704e-05,
      "loss": 0.0121,
      "step": 2400
    },
    {
      "epoch": 2.915151515151515,
      "grad_norm": 0.26940274238586426,
      "learning_rate": 1.577651515151515e-05,
      "loss": 0.0119,
      "step": 2405
    },
    {
      "epoch": 2.9212121212121214,
      "grad_norm": 0.25586020946502686,
      "learning_rate": 1.5765993265993267e-05,
      "loss": 0.012,
      "step": 2410
    },
    {
      "epoch": 2.9272727272727272,
      "grad_norm": 0.3019687533378601,
      "learning_rate": 1.575547138047138e-05,
      "loss": 0.0135,
      "step": 2415
    },
    {
      "epoch": 2.933333333333333,
      "grad_norm": 0.25517913699150085,
      "learning_rate": 1.5744949494949496e-05,
      "loss": 0.0123,
      "step": 2420
    },
    {
      "epoch": 2.9393939393939394,
      "grad_norm": 0.2601567804813385,
      "learning_rate": 1.5734427609427612e-05,
      "loss": 0.0124,
      "step": 2425
    },
    {
      "epoch": 2.9454545454545453,
      "grad_norm": 0.24827276170253754,
      "learning_rate": 1.5723905723905725e-05,
      "loss": 0.0124,
      "step": 2430
    },
    {
      "epoch": 2.9515151515151516,
      "grad_norm": 0.2525596022605896,
      "learning_rate": 1.571338383838384e-05,
      "loss": 0.0131,
      "step": 2435
    },
    {
      "epoch": 2.9575757575757575,
      "grad_norm": 0.2703907787799835,
      "learning_rate": 1.5702861952861954e-05,
      "loss": 0.0123,
      "step": 2440
    },
    {
      "epoch": 2.963636363636364,
      "grad_norm": 0.3193957209587097,
      "learning_rate": 1.569234006734007e-05,
      "loss": 0.0108,
      "step": 2445
    },
    {
      "epoch": 2.9696969696969697,
      "grad_norm": 0.2931104898452759,
      "learning_rate": 1.5681818181818182e-05,
      "loss": 0.0127,
      "step": 2450
    },
    {
      "epoch": 2.9757575757575756,
      "grad_norm": 0.3042631149291992,
      "learning_rate": 1.5671296296296295e-05,
      "loss": 0.0123,
      "step": 2455
    },
    {
      "epoch": 2.981818181818182,
      "grad_norm": 0.264446496963501,
      "learning_rate": 1.5660774410774414e-05,
      "loss": 0.0128,
      "step": 2460
    },
    {
      "epoch": 2.987878787878788,
      "grad_norm": 0.2892887592315674,
      "learning_rate": 1.5650252525252527e-05,
      "loss": 0.0123,
      "step": 2465
    },
    {
      "epoch": 2.993939393939394,
      "grad_norm": 0.30362749099731445,
      "learning_rate": 1.563973063973064e-05,
      "loss": 0.0128,
      "step": 2470
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.27986520528793335,
      "learning_rate": 1.5629208754208756e-05,
      "loss": 0.0121,
      "step": 2475
    },
    {
      "epoch": 3.0,
      "eval_average": 0.548430103919821,
      "eval_crossner_ai": 0.5227568269979785,
      "eval_crossner_literature": 0.514757969253361,
      "eval_crossner_music": 0.6724386723886212,
      "eval_crossner_politics": 0.5539215685773626,
      "eval_crossner_science": 0.6111111110609754,
      "eval_mit-movie": 0.5745856353096059,
      "eval_mit-restaurant": 0.38943894385084254,
      "eval_runtime": 21.0894,
      "eval_samples_per_second": 33.192,
      "eval_steps_per_second": 0.332,
      "step": 2475
    },
    {
      "epoch": 3.006060606060606,
      "grad_norm": 0.16709867119789124,
      "learning_rate": 1.5620791245791248e-05,
      "loss": 0.0088,
      "step": 2480
    },
    {
      "epoch": 3.012121212121212,
      "grad_norm": 0.23106741905212402,
      "learning_rate": 1.561026936026936e-05,
      "loss": 0.0043,
      "step": 2485
    },
    {
      "epoch": 3.018181818181818,
      "grad_norm": 0.33609113097190857,
      "learning_rate": 1.5599747474747477e-05,
      "loss": 0.0038,
      "step": 2490
    },
    {
      "epoch": 3.0242424242424244,
      "grad_norm": 0.25372499227523804,
      "learning_rate": 1.558922558922559e-05,
      "loss": 0.0046,
      "step": 2495
    },
    {
      "epoch": 3.0303030303030303,
      "grad_norm": 0.23775698244571686,
      "learning_rate": 1.5578703703703706e-05,
      "loss": 0.004,
      "step": 2500
    },
    {
      "epoch": 3.036363636363636,
      "grad_norm": 0.24226875603199005,
      "learning_rate": 1.5568181818181822e-05,
      "loss": 0.0043,
      "step": 2505
    },
    {
      "epoch": 3.0424242424242425,
      "grad_norm": 0.243282288312912,
      "learning_rate": 1.5557659932659934e-05,
      "loss": 0.0044,
      "step": 2510
    },
    {
      "epoch": 3.0484848484848484,
      "grad_norm": 0.2522165775299072,
      "learning_rate": 1.5547138047138047e-05,
      "loss": 0.0045,
      "step": 2515
    },
    {
      "epoch": 3.0545454545454547,
      "grad_norm": 0.2998723089694977,
      "learning_rate": 1.5536616161616163e-05,
      "loss": 0.0044,
      "step": 2520
    },
    {
      "epoch": 3.0606060606060606,
      "grad_norm": 0.2562747895717621,
      "learning_rate": 1.5526094276094276e-05,
      "loss": 0.0039,
      "step": 2525
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.23944923281669617,
      "learning_rate": 1.5515572390572392e-05,
      "loss": 0.0045,
      "step": 2530
    },
    {
      "epoch": 3.0727272727272728,
      "grad_norm": 0.3657647371292114,
      "learning_rate": 1.5505050505050508e-05,
      "loss": 0.0046,
      "step": 2535
    },
    {
      "epoch": 3.0787878787878786,
      "grad_norm": 0.24658584594726562,
      "learning_rate": 1.549452861952862e-05,
      "loss": 0.0043,
      "step": 2540
    },
    {
      "epoch": 3.084848484848485,
      "grad_norm": 0.2736847698688507,
      "learning_rate": 1.5484006734006737e-05,
      "loss": 0.0041,
      "step": 2545
    },
    {
      "epoch": 3.090909090909091,
      "grad_norm": 0.2983143627643585,
      "learning_rate": 1.547348484848485e-05,
      "loss": 0.0041,
      "step": 2550
    },
    {
      "epoch": 3.096969696969697,
      "grad_norm": 0.35472923517227173,
      "learning_rate": 1.5462962962962966e-05,
      "loss": 0.0044,
      "step": 2555
    },
    {
      "epoch": 3.103030303030303,
      "grad_norm": 0.2681572437286377,
      "learning_rate": 1.545244107744108e-05,
      "loss": 0.0044,
      "step": 2560
    },
    {
      "epoch": 3.109090909090909,
      "grad_norm": 0.3392554521560669,
      "learning_rate": 1.544191919191919e-05,
      "loss": 0.0046,
      "step": 2565
    },
    {
      "epoch": 3.1151515151515152,
      "grad_norm": 0.2920863628387451,
      "learning_rate": 1.5431397306397307e-05,
      "loss": 0.0044,
      "step": 2570
    },
    {
      "epoch": 3.121212121212121,
      "grad_norm": 0.2642986476421356,
      "learning_rate": 1.5420875420875423e-05,
      "loss": 0.0043,
      "step": 2575
    },
    {
      "epoch": 3.1272727272727274,
      "grad_norm": 0.3352542519569397,
      "learning_rate": 1.5410353535353536e-05,
      "loss": 0.0046,
      "step": 2580
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.2975146174430847,
      "learning_rate": 1.5399831649831652e-05,
      "loss": 0.0042,
      "step": 2585
    },
    {
      "epoch": 3.139393939393939,
      "grad_norm": 0.2710624039173126,
      "learning_rate": 1.5389309764309765e-05,
      "loss": 0.0044,
      "step": 2590
    },
    {
      "epoch": 3.1454545454545455,
      "grad_norm": 0.23641809821128845,
      "learning_rate": 1.537878787878788e-05,
      "loss": 0.0051,
      "step": 2595
    },
    {
      "epoch": 3.1515151515151514,
      "grad_norm": 0.23434343934059143,
      "learning_rate": 1.5368265993265993e-05,
      "loss": 0.0048,
      "step": 2600
    },
    {
      "epoch": 3.1575757575757577,
      "grad_norm": 0.2549766004085541,
      "learning_rate": 1.535774410774411e-05,
      "loss": 0.0043,
      "step": 2605
    },
    {
      "epoch": 3.1636363636363636,
      "grad_norm": 0.2927738428115845,
      "learning_rate": 1.5347222222222226e-05,
      "loss": 0.0043,
      "step": 2610
    },
    {
      "epoch": 3.16969696969697,
      "grad_norm": 0.27510741353034973,
      "learning_rate": 1.5336700336700338e-05,
      "loss": 0.0044,
      "step": 2615
    },
    {
      "epoch": 3.175757575757576,
      "grad_norm": 0.29057613015174866,
      "learning_rate": 1.5326178451178454e-05,
      "loss": 0.0042,
      "step": 2620
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 0.2940966784954071,
      "learning_rate": 1.5315656565656567e-05,
      "loss": 0.0044,
      "step": 2625
    },
    {
      "epoch": 3.187878787878788,
      "grad_norm": 0.30909425020217896,
      "learning_rate": 1.530513468013468e-05,
      "loss": 0.0045,
      "step": 2630
    },
    {
      "epoch": 3.193939393939394,
      "grad_norm": 0.28748810291290283,
      "learning_rate": 1.5294612794612796e-05,
      "loss": 0.0045,
      "step": 2635
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.541128396987915,
      "learning_rate": 1.528409090909091e-05,
      "loss": 0.0047,
      "step": 2640
    },
    {
      "epoch": 3.206060606060606,
      "grad_norm": 0.30497246980667114,
      "learning_rate": 1.5273569023569025e-05,
      "loss": 0.0051,
      "step": 2645
    },
    {
      "epoch": 3.212121212121212,
      "grad_norm": 0.23459145426750183,
      "learning_rate": 1.526304713804714e-05,
      "loss": 0.005,
      "step": 2650
    },
    {
      "epoch": 3.2181818181818183,
      "grad_norm": 0.2732677757740021,
      "learning_rate": 1.5252525252525255e-05,
      "loss": 0.0049,
      "step": 2655
    },
    {
      "epoch": 3.224242424242424,
      "grad_norm": 0.2801437973976135,
      "learning_rate": 1.5242003367003368e-05,
      "loss": 0.0048,
      "step": 2660
    },
    {
      "epoch": 3.2303030303030305,
      "grad_norm": 0.3381393253803253,
      "learning_rate": 1.5231481481481482e-05,
      "loss": 0.0046,
      "step": 2665
    },
    {
      "epoch": 3.2363636363636363,
      "grad_norm": 0.2695382237434387,
      "learning_rate": 1.5220959595959597e-05,
      "loss": 0.0043,
      "step": 2670
    },
    {
      "epoch": 3.242424242424242,
      "grad_norm": 0.2963900566101074,
      "learning_rate": 1.5210437710437711e-05,
      "loss": 0.0041,
      "step": 2675
    },
    {
      "epoch": 3.2484848484848485,
      "grad_norm": 0.26134130358695984,
      "learning_rate": 1.5199915824915825e-05,
      "loss": 0.0041,
      "step": 2680
    },
    {
      "epoch": 3.2545454545454544,
      "grad_norm": 0.2785872220993042,
      "learning_rate": 1.5189393939393941e-05,
      "loss": 0.0047,
      "step": 2685
    },
    {
      "epoch": 3.2606060606060607,
      "grad_norm": 0.3082360029220581,
      "learning_rate": 1.5178872053872056e-05,
      "loss": 0.0045,
      "step": 2690
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.294791579246521,
      "learning_rate": 1.516835016835017e-05,
      "loss": 0.005,
      "step": 2695
    },
    {
      "epoch": 3.272727272727273,
      "grad_norm": 0.33751747012138367,
      "learning_rate": 1.5157828282828285e-05,
      "loss": 0.0047,
      "step": 2700
    },
    {
      "epoch": 3.278787878787879,
      "grad_norm": 0.22974511981010437,
      "learning_rate": 1.5147306397306399e-05,
      "loss": 0.0049,
      "step": 2705
    },
    {
      "epoch": 3.2848484848484847,
      "grad_norm": 0.3360849916934967,
      "learning_rate": 1.5136784511784512e-05,
      "loss": 0.0053,
      "step": 2710
    },
    {
      "epoch": 3.290909090909091,
      "grad_norm": 0.25327256321907043,
      "learning_rate": 1.5126262626262626e-05,
      "loss": 0.0051,
      "step": 2715
    },
    {
      "epoch": 3.296969696969697,
      "grad_norm": 0.24397382140159607,
      "learning_rate": 1.5115740740740744e-05,
      "loss": 0.0048,
      "step": 2720
    },
    {
      "epoch": 3.303030303030303,
      "grad_norm": 0.24220740795135498,
      "learning_rate": 1.5105218855218857e-05,
      "loss": 0.0047,
      "step": 2725
    },
    {
      "epoch": 3.309090909090909,
      "grad_norm": 0.2960638403892517,
      "learning_rate": 1.5094696969696971e-05,
      "loss": 0.0045,
      "step": 2730
    },
    {
      "epoch": 3.315151515151515,
      "grad_norm": 0.3193482458591461,
      "learning_rate": 1.5084175084175085e-05,
      "loss": 0.0047,
      "step": 2735
    },
    {
      "epoch": 3.3212121212121213,
      "grad_norm": 0.27051395177841187,
      "learning_rate": 1.50736531986532e-05,
      "loss": 0.0046,
      "step": 2740
    },
    {
      "epoch": 3.327272727272727,
      "grad_norm": 0.2653113007545471,
      "learning_rate": 1.5063131313131314e-05,
      "loss": 0.0043,
      "step": 2745
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.2513991892337799,
      "learning_rate": 1.5052609427609428e-05,
      "loss": 0.0042,
      "step": 2750
    },
    {
      "epoch": 3.3393939393939394,
      "grad_norm": 0.313594251871109,
      "learning_rate": 1.5042087542087543e-05,
      "loss": 0.0049,
      "step": 2755
    },
    {
      "epoch": 3.3454545454545452,
      "grad_norm": 0.2798522710800171,
      "learning_rate": 1.5031565656565659e-05,
      "loss": 0.0051,
      "step": 2760
    },
    {
      "epoch": 3.3515151515151516,
      "grad_norm": 0.2923886775970459,
      "learning_rate": 1.5021043771043773e-05,
      "loss": 0.005,
      "step": 2765
    },
    {
      "epoch": 3.3575757575757574,
      "grad_norm": 0.2865530848503113,
      "learning_rate": 1.5010521885521888e-05,
      "loss": 0.0041,
      "step": 2770
    },
    {
      "epoch": 3.3636363636363638,
      "grad_norm": 0.32369938492774963,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 0.0048,
      "step": 2775
    },
    {
      "epoch": 3.3696969696969696,
      "grad_norm": 0.24903059005737305,
      "learning_rate": 1.4989478114478115e-05,
      "loss": 0.0047,
      "step": 2780
    },
    {
      "epoch": 3.375757575757576,
      "grad_norm": 0.3315272629261017,
      "learning_rate": 1.497895622895623e-05,
      "loss": 0.0052,
      "step": 2785
    },
    {
      "epoch": 3.381818181818182,
      "grad_norm": 0.2783204913139343,
      "learning_rate": 1.4968434343434344e-05,
      "loss": 0.0053,
      "step": 2790
    },
    {
      "epoch": 3.3878787878787877,
      "grad_norm": 0.32429802417755127,
      "learning_rate": 1.495791245791246e-05,
      "loss": 0.0049,
      "step": 2795
    },
    {
      "epoch": 3.393939393939394,
      "grad_norm": 0.26747116446495056,
      "learning_rate": 1.4947390572390574e-05,
      "loss": 0.0046,
      "step": 2800
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.32726940512657166,
      "learning_rate": 1.4936868686868688e-05,
      "loss": 0.0054,
      "step": 2805
    },
    {
      "epoch": 3.4060606060606062,
      "grad_norm": 0.27352964878082275,
      "learning_rate": 1.4926346801346803e-05,
      "loss": 0.0045,
      "step": 2810
    },
    {
      "epoch": 3.412121212121212,
      "grad_norm": 0.27787935733795166,
      "learning_rate": 1.4915824915824917e-05,
      "loss": 0.0044,
      "step": 2815
    },
    {
      "epoch": 3.418181818181818,
      "grad_norm": 0.2502579092979431,
      "learning_rate": 1.4905303030303032e-05,
      "loss": 0.005,
      "step": 2820
    },
    {
      "epoch": 3.4242424242424243,
      "grad_norm": 0.6739312410354614,
      "learning_rate": 1.4894781144781146e-05,
      "loss": 0.0049,
      "step": 2825
    },
    {
      "epoch": 3.43030303030303,
      "grad_norm": 0.2656075060367584,
      "learning_rate": 1.4884259259259259e-05,
      "loss": 0.0044,
      "step": 2830
    },
    {
      "epoch": 3.4363636363636365,
      "grad_norm": 0.25147396326065063,
      "learning_rate": 1.4873737373737376e-05,
      "loss": 0.0043,
      "step": 2835
    },
    {
      "epoch": 3.4424242424242424,
      "grad_norm": 0.32029929757118225,
      "learning_rate": 1.486321548821549e-05,
      "loss": 0.0048,
      "step": 2840
    },
    {
      "epoch": 3.4484848484848483,
      "grad_norm": 0.3136039972305298,
      "learning_rate": 1.4852693602693604e-05,
      "loss": 0.005,
      "step": 2845
    },
    {
      "epoch": 3.4545454545454546,
      "grad_norm": 0.30051150918006897,
      "learning_rate": 1.4842171717171718e-05,
      "loss": 0.0054,
      "step": 2850
    },
    {
      "epoch": 3.4606060606060605,
      "grad_norm": 0.3090931475162506,
      "learning_rate": 1.4831649831649832e-05,
      "loss": 0.0051,
      "step": 2855
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.2846376299858093,
      "learning_rate": 1.4821127946127947e-05,
      "loss": 0.0057,
      "step": 2860
    },
    {
      "epoch": 3.4727272727272727,
      "grad_norm": 0.280364066362381,
      "learning_rate": 1.4810606060606061e-05,
      "loss": 0.0052,
      "step": 2865
    },
    {
      "epoch": 3.478787878787879,
      "grad_norm": 0.3652738034725189,
      "learning_rate": 1.4800084175084177e-05,
      "loss": 0.0049,
      "step": 2870
    },
    {
      "epoch": 3.484848484848485,
      "grad_norm": 0.2978995442390442,
      "learning_rate": 1.4789562289562292e-05,
      "loss": 0.0047,
      "step": 2875
    },
    {
      "epoch": 3.4909090909090907,
      "grad_norm": 0.3236129581928253,
      "learning_rate": 1.4779040404040406e-05,
      "loss": 0.0054,
      "step": 2880
    },
    {
      "epoch": 3.496969696969697,
      "grad_norm": 0.2485118955373764,
      "learning_rate": 1.476851851851852e-05,
      "loss": 0.0049,
      "step": 2885
    },
    {
      "epoch": 3.5006060606060605,
      "eval_average": 0.548341839297503,
      "eval_crossner_ai": 0.4987146529061783,
      "eval_crossner_literature": 0.5741626793758144,
      "eval_crossner_music": 0.6899055918163445,
      "eval_crossner_politics": 0.6236378876280364,
      "eval_crossner_science": 0.6388261850514484,
      "eval_mit-movie": 0.4912280701270135,
      "eval_mit-restaurant": 0.3219178081776858,
      "eval_runtime": 20.9107,
      "eval_samples_per_second": 33.476,
      "eval_steps_per_second": 0.335,
      "step": 2888
    },
    {
      "epoch": 3.503030303030303,
      "grad_norm": 0.28161972761154175,
      "learning_rate": 1.4757996632996635e-05,
      "loss": 0.0049,
      "step": 2890
    },
    {
      "epoch": 3.5090909090909093,
      "grad_norm": 0.2839902341365814,
      "learning_rate": 1.4747474747474747e-05,
      "loss": 0.0051,
      "step": 2895
    },
    {
      "epoch": 3.515151515151515,
      "grad_norm": 0.2953062653541565,
      "learning_rate": 1.4736952861952862e-05,
      "loss": 0.0048,
      "step": 2900
    },
    {
      "epoch": 3.521212121212121,
      "grad_norm": 0.3416697382926941,
      "learning_rate": 1.472643097643098e-05,
      "loss": 0.0051,
      "step": 2905
    },
    {
      "epoch": 3.5272727272727273,
      "grad_norm": 0.24704895913600922,
      "learning_rate": 1.4715909090909092e-05,
      "loss": 0.0045,
      "step": 2910
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.2722042202949524,
      "learning_rate": 1.4705387205387207e-05,
      "loss": 0.0046,
      "step": 2915
    },
    {
      "epoch": 3.5393939393939395,
      "grad_norm": 0.30830469727516174,
      "learning_rate": 1.4694865319865321e-05,
      "loss": 0.0047,
      "step": 2920
    },
    {
      "epoch": 3.5454545454545454,
      "grad_norm": 0.3492697477340698,
      "learning_rate": 1.4684343434343435e-05,
      "loss": 0.0045,
      "step": 2925
    },
    {
      "epoch": 3.5515151515151517,
      "grad_norm": 0.29227688908576965,
      "learning_rate": 1.467382154882155e-05,
      "loss": 0.0048,
      "step": 2930
    },
    {
      "epoch": 3.5575757575757576,
      "grad_norm": 0.26132652163505554,
      "learning_rate": 1.4663299663299664e-05,
      "loss": 0.0056,
      "step": 2935
    },
    {
      "epoch": 3.5636363636363635,
      "grad_norm": 0.2700287103652954,
      "learning_rate": 1.4652777777777779e-05,
      "loss": 0.0046,
      "step": 2940
    },
    {
      "epoch": 3.56969696969697,
      "grad_norm": 0.3599071502685547,
      "learning_rate": 1.4642255892255895e-05,
      "loss": 0.0052,
      "step": 2945
    },
    {
      "epoch": 3.5757575757575757,
      "grad_norm": 0.240911066532135,
      "learning_rate": 1.4631734006734009e-05,
      "loss": 0.0053,
      "step": 2950
    },
    {
      "epoch": 3.581818181818182,
      "grad_norm": 0.24042782187461853,
      "learning_rate": 1.4621212121212123e-05,
      "loss": 0.0047,
      "step": 2955
    },
    {
      "epoch": 3.587878787878788,
      "grad_norm": 0.285430908203125,
      "learning_rate": 1.4610690235690236e-05,
      "loss": 0.0045,
      "step": 2960
    },
    {
      "epoch": 3.5939393939393938,
      "grad_norm": 0.2818451225757599,
      "learning_rate": 1.460016835016835e-05,
      "loss": 0.005,
      "step": 2965
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.258114218711853,
      "learning_rate": 1.4589646464646465e-05,
      "loss": 0.0047,
      "step": 2970
    },
    {
      "epoch": 3.606060606060606,
      "grad_norm": 0.2998427152633667,
      "learning_rate": 1.457912457912458e-05,
      "loss": 0.0051,
      "step": 2975
    },
    {
      "epoch": 3.6121212121212123,
      "grad_norm": 0.3059210479259491,
      "learning_rate": 1.4568602693602695e-05,
      "loss": 0.0051,
      "step": 2980
    },
    {
      "epoch": 3.618181818181818,
      "grad_norm": 0.3197299838066101,
      "learning_rate": 1.455808080808081e-05,
      "loss": 0.0052,
      "step": 2985
    },
    {
      "epoch": 3.624242424242424,
      "grad_norm": 0.2676868736743927,
      "learning_rate": 1.4547558922558924e-05,
      "loss": 0.0052,
      "step": 2990
    },
    {
      "epoch": 3.6303030303030304,
      "grad_norm": 0.2877224385738373,
      "learning_rate": 1.4537037037037039e-05,
      "loss": 0.0048,
      "step": 2995
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 0.3202628493309021,
      "learning_rate": 1.4526515151515153e-05,
      "loss": 0.0056,
      "step": 3000
    },
    {
      "epoch": 3.6424242424242426,
      "grad_norm": 0.33481359481811523,
      "learning_rate": 1.4515993265993267e-05,
      "loss": 0.0055,
      "step": 3005
    },
    {
      "epoch": 3.6484848484848484,
      "grad_norm": 1.6051639318466187,
      "learning_rate": 1.4505471380471382e-05,
      "loss": 0.0053,
      "step": 3010
    },
    {
      "epoch": 3.6545454545454548,
      "grad_norm": 0.2870620787143707,
      "learning_rate": 1.4494949494949494e-05,
      "loss": 0.0052,
      "step": 3015
    },
    {
      "epoch": 3.6606060606060606,
      "grad_norm": 0.3321783244609833,
      "learning_rate": 1.4484427609427612e-05,
      "loss": 0.0054,
      "step": 3020
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.28342464566230774,
      "learning_rate": 1.4473905723905725e-05,
      "loss": 0.0055,
      "step": 3025
    },
    {
      "epoch": 3.672727272727273,
      "grad_norm": 0.285610169172287,
      "learning_rate": 1.446338383838384e-05,
      "loss": 0.0054,
      "step": 3030
    },
    {
      "epoch": 3.6787878787878787,
      "grad_norm": 0.2538585960865021,
      "learning_rate": 1.4452861952861954e-05,
      "loss": 0.0062,
      "step": 3035
    },
    {
      "epoch": 3.684848484848485,
      "grad_norm": 0.3330533504486084,
      "learning_rate": 1.4442340067340068e-05,
      "loss": 0.0046,
      "step": 3040
    },
    {
      "epoch": 3.690909090909091,
      "grad_norm": 0.32353755831718445,
      "learning_rate": 1.4431818181818182e-05,
      "loss": 0.006,
      "step": 3045
    },
    {
      "epoch": 3.696969696969697,
      "grad_norm": 0.2789161801338196,
      "learning_rate": 1.4421296296296297e-05,
      "loss": 0.0052,
      "step": 3050
    },
    {
      "epoch": 3.703030303030303,
      "grad_norm": 0.3176405727863312,
      "learning_rate": 1.4410774410774413e-05,
      "loss": 0.0058,
      "step": 3055
    },
    {
      "epoch": 3.709090909090909,
      "grad_norm": 0.3641081750392914,
      "learning_rate": 1.4400252525252527e-05,
      "loss": 0.0055,
      "step": 3060
    },
    {
      "epoch": 3.7151515151515153,
      "grad_norm": 0.30698370933532715,
      "learning_rate": 1.4389730639730642e-05,
      "loss": 0.0054,
      "step": 3065
    },
    {
      "epoch": 3.721212121212121,
      "grad_norm": 0.29742202162742615,
      "learning_rate": 1.4379208754208756e-05,
      "loss": 0.0052,
      "step": 3070
    },
    {
      "epoch": 3.727272727272727,
      "grad_norm": 0.29487907886505127,
      "learning_rate": 1.436868686868687e-05,
      "loss": 0.0049,
      "step": 3075
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.3245706558227539,
      "learning_rate": 1.4358164983164983e-05,
      "loss": 0.0055,
      "step": 3080
    },
    {
      "epoch": 3.7393939393939393,
      "grad_norm": 0.28976255655288696,
      "learning_rate": 1.4347643097643098e-05,
      "loss": 0.0057,
      "step": 3085
    },
    {
      "epoch": 3.7454545454545456,
      "grad_norm": 0.32096067070961,
      "learning_rate": 1.4337121212121212e-05,
      "loss": 0.0053,
      "step": 3090
    },
    {
      "epoch": 3.7515151515151515,
      "grad_norm": 0.23264575004577637,
      "learning_rate": 1.4326599326599328e-05,
      "loss": 0.0052,
      "step": 3095
    },
    {
      "epoch": 3.757575757575758,
      "grad_norm": 0.29498016834259033,
      "learning_rate": 1.4316077441077442e-05,
      "loss": 0.0051,
      "step": 3100
    },
    {
      "epoch": 3.7636363636363637,
      "grad_norm": 0.3309749662876129,
      "learning_rate": 1.4305555555555557e-05,
      "loss": 0.0049,
      "step": 3105
    },
    {
      "epoch": 3.7696969696969695,
      "grad_norm": 0.2827751636505127,
      "learning_rate": 1.4295033670033671e-05,
      "loss": 0.0046,
      "step": 3110
    },
    {
      "epoch": 3.775757575757576,
      "grad_norm": 0.2921620309352875,
      "learning_rate": 1.4284511784511786e-05,
      "loss": 0.0058,
      "step": 3115
    },
    {
      "epoch": 3.7818181818181817,
      "grad_norm": 0.26382461190223694,
      "learning_rate": 1.42739898989899e-05,
      "loss": 0.0052,
      "step": 3120
    },
    {
      "epoch": 3.787878787878788,
      "grad_norm": 0.25001949071884155,
      "learning_rate": 1.4263468013468014e-05,
      "loss": 0.0052,
      "step": 3125
    },
    {
      "epoch": 3.793939393939394,
      "grad_norm": 0.2937867343425751,
      "learning_rate": 1.425294612794613e-05,
      "loss": 0.0053,
      "step": 3130
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.2692645192146301,
      "learning_rate": 1.4242424242424245e-05,
      "loss": 0.0047,
      "step": 3135
    },
    {
      "epoch": 3.806060606060606,
      "grad_norm": 0.2591134309768677,
      "learning_rate": 1.423190235690236e-05,
      "loss": 0.0051,
      "step": 3140
    },
    {
      "epoch": 3.812121212121212,
      "grad_norm": 0.2822151482105255,
      "learning_rate": 1.4221380471380472e-05,
      "loss": 0.0051,
      "step": 3145
    },
    {
      "epoch": 3.8181818181818183,
      "grad_norm": 0.2722412645816803,
      "learning_rate": 1.4210858585858586e-05,
      "loss": 0.0054,
      "step": 3150
    },
    {
      "epoch": 3.824242424242424,
      "grad_norm": 0.2647487223148346,
      "learning_rate": 1.42003367003367e-05,
      "loss": 0.0053,
      "step": 3155
    },
    {
      "epoch": 3.83030303030303,
      "grad_norm": 0.3284708261489868,
      "learning_rate": 1.4189814814814815e-05,
      "loss": 0.0055,
      "step": 3160
    },
    {
      "epoch": 3.8363636363636364,
      "grad_norm": 0.26694172620773315,
      "learning_rate": 1.4179292929292931e-05,
      "loss": 0.0059,
      "step": 3165
    },
    {
      "epoch": 3.8424242424242423,
      "grad_norm": 0.29585230350494385,
      "learning_rate": 1.4168771043771045e-05,
      "loss": 0.0051,
      "step": 3170
    },
    {
      "epoch": 3.8484848484848486,
      "grad_norm": 0.30025172233581543,
      "learning_rate": 1.415824915824916e-05,
      "loss": 0.0055,
      "step": 3175
    },
    {
      "epoch": 3.8545454545454545,
      "grad_norm": 0.30941689014434814,
      "learning_rate": 1.4147727272727274e-05,
      "loss": 0.0049,
      "step": 3180
    },
    {
      "epoch": 3.860606060606061,
      "grad_norm": 0.3199971616268158,
      "learning_rate": 1.4137205387205389e-05,
      "loss": 0.0059,
      "step": 3185
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.25569507479667664,
      "learning_rate": 1.4126683501683503e-05,
      "loss": 0.0052,
      "step": 3190
    },
    {
      "epoch": 3.8727272727272726,
      "grad_norm": 0.25401976704597473,
      "learning_rate": 1.4116161616161617e-05,
      "loss": 0.0053,
      "step": 3195
    },
    {
      "epoch": 3.878787878787879,
      "grad_norm": 0.26396799087524414,
      "learning_rate": 1.410563973063973e-05,
      "loss": 0.0056,
      "step": 3200
    },
    {
      "epoch": 3.8848484848484848,
      "grad_norm": 0.3139147460460663,
      "learning_rate": 1.4095117845117848e-05,
      "loss": 0.0051,
      "step": 3205
    },
    {
      "epoch": 3.890909090909091,
      "grad_norm": 0.30207833647727966,
      "learning_rate": 1.408459595959596e-05,
      "loss": 0.0052,
      "step": 3210
    },
    {
      "epoch": 3.896969696969697,
      "grad_norm": 0.3357970118522644,
      "learning_rate": 1.4074074074074075e-05,
      "loss": 0.0056,
      "step": 3215
    },
    {
      "epoch": 3.903030303030303,
      "grad_norm": 0.3067120313644409,
      "learning_rate": 1.406355218855219e-05,
      "loss": 0.005,
      "step": 3220
    },
    {
      "epoch": 3.909090909090909,
      "grad_norm": 0.2612677812576294,
      "learning_rate": 1.4053030303030304e-05,
      "loss": 0.005,
      "step": 3225
    },
    {
      "epoch": 3.915151515151515,
      "grad_norm": 0.24029198288917542,
      "learning_rate": 1.4042508417508418e-05,
      "loss": 0.0047,
      "step": 3230
    },
    {
      "epoch": 3.9212121212121214,
      "grad_norm": 0.28821462392807007,
      "learning_rate": 1.4031986531986533e-05,
      "loss": 0.0048,
      "step": 3235
    },
    {
      "epoch": 3.9272727272727272,
      "grad_norm": 0.3146457076072693,
      "learning_rate": 1.4021464646464649e-05,
      "loss": 0.0049,
      "step": 3240
    },
    {
      "epoch": 3.933333333333333,
      "grad_norm": 0.31158167123794556,
      "learning_rate": 1.4010942760942763e-05,
      "loss": 0.0054,
      "step": 3245
    },
    {
      "epoch": 3.9393939393939394,
      "grad_norm": 0.29406121373176575,
      "learning_rate": 1.4000420875420877e-05,
      "loss": 0.0061,
      "step": 3250
    },
    {
      "epoch": 3.9454545454545453,
      "grad_norm": 0.2501521110534668,
      "learning_rate": 1.3989898989898992e-05,
      "loss": 0.0052,
      "step": 3255
    },
    {
      "epoch": 3.9515151515151516,
      "grad_norm": 0.2848391532897949,
      "learning_rate": 1.3979377104377106e-05,
      "loss": 0.0055,
      "step": 3260
    },
    {
      "epoch": 3.9575757575757575,
      "grad_norm": 0.3289638161659241,
      "learning_rate": 1.3968855218855219e-05,
      "loss": 0.0055,
      "step": 3265
    },
    {
      "epoch": 3.963636363636364,
      "grad_norm": 0.26624003052711487,
      "learning_rate": 1.3958333333333333e-05,
      "loss": 0.0056,
      "step": 3270
    },
    {
      "epoch": 3.9696969696969697,
      "grad_norm": 0.32885047793388367,
      "learning_rate": 1.3947811447811448e-05,
      "loss": 0.005,
      "step": 3275
    },
    {
      "epoch": 3.9757575757575756,
      "grad_norm": 0.2808777093887329,
      "learning_rate": 1.3937289562289564e-05,
      "loss": 0.0059,
      "step": 3280
    },
    {
      "epoch": 3.981818181818182,
      "grad_norm": 0.31186145544052124,
      "learning_rate": 1.3926767676767678e-05,
      "loss": 0.0055,
      "step": 3285
    },
    {
      "epoch": 3.987878787878788,
      "grad_norm": 0.21134361624717712,
      "learning_rate": 1.3916245791245792e-05,
      "loss": 0.0045,
      "step": 3290
    },
    {
      "epoch": 3.993939393939394,
      "grad_norm": 0.2712946832180023,
      "learning_rate": 1.3905723905723907e-05,
      "loss": 0.0054,
      "step": 3295
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.3141225278377533,
      "learning_rate": 1.3895202020202021e-05,
      "loss": 0.0049,
      "step": 3300
    },
    {
      "epoch": 4.0,
      "eval_average": 0.543012133439473,
      "eval_crossner_ai": 0.4818067753577245,
      "eval_crossner_literature": 0.5104895104394135,
      "eval_crossner_music": 0.7093105898575152,
      "eval_crossner_politics": 0.5645161289821856,
      "eval_crossner_science": 0.6046511627405643,
      "eval_mit-movie": 0.5459459458961783,
      "eval_mit-restaurant": 0.38436482080272893,
      "eval_runtime": 20.448,
      "eval_samples_per_second": 34.233,
      "eval_steps_per_second": 0.342,
      "step": 3300
    },
    {
      "epoch": 4.006060606060606,
      "grad_norm": 0.17956776916980743,
      "learning_rate": 1.3884680134680136e-05,
      "loss": 0.0042,
      "step": 3305
    },
    {
      "epoch": 4.012121212121212,
      "grad_norm": 0.2003721445798874,
      "learning_rate": 1.387415824915825e-05,
      "loss": 0.002,
      "step": 3310
    },
    {
      "epoch": 4.0181818181818185,
      "grad_norm": 0.2800138294696808,
      "learning_rate": 1.3863636363636366e-05,
      "loss": 0.0026,
      "step": 3315
    },
    {
      "epoch": 4.024242424242424,
      "grad_norm": 0.38462647795677185,
      "learning_rate": 1.385311447811448e-05,
      "loss": 0.0021,
      "step": 3320
    },
    {
      "epoch": 4.03030303030303,
      "grad_norm": 0.28458672761917114,
      "learning_rate": 1.3842592592592595e-05,
      "loss": 0.0023,
      "step": 3325
    },
    {
      "epoch": 4.036363636363636,
      "grad_norm": 0.2667478621006012,
      "learning_rate": 1.3832070707070708e-05,
      "loss": 0.0022,
      "step": 3330
    },
    {
      "epoch": 4.042424242424242,
      "grad_norm": 0.2976126968860626,
      "learning_rate": 1.3821548821548822e-05,
      "loss": 0.0021,
      "step": 3335
    },
    {
      "epoch": 4.048484848484849,
      "grad_norm": 0.18871527910232544,
      "learning_rate": 1.3811026936026936e-05,
      "loss": 0.0027,
      "step": 3340
    },
    {
      "epoch": 4.054545454545455,
      "grad_norm": 0.22207027673721313,
      "learning_rate": 1.380050505050505e-05,
      "loss": 0.002,
      "step": 3345
    },
    {
      "epoch": 4.0606060606060606,
      "grad_norm": 0.283325731754303,
      "learning_rate": 1.3789983164983165e-05,
      "loss": 0.0024,
      "step": 3350
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 0.23439501225948334,
      "learning_rate": 1.3779461279461281e-05,
      "loss": 0.0022,
      "step": 3355
    },
    {
      "epoch": 4.072727272727272,
      "grad_norm": 0.18762966990470886,
      "learning_rate": 1.3768939393939396e-05,
      "loss": 0.0022,
      "step": 3360
    },
    {
      "epoch": 4.078787878787879,
      "grad_norm": 0.22916734218597412,
      "learning_rate": 1.375841750841751e-05,
      "loss": 0.0021,
      "step": 3365
    },
    {
      "epoch": 4.084848484848485,
      "grad_norm": 0.2540155053138733,
      "learning_rate": 1.3747895622895624e-05,
      "loss": 0.0022,
      "step": 3370
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 0.19171485304832458,
      "learning_rate": 1.3737373737373739e-05,
      "loss": 0.0021,
      "step": 3375
    },
    {
      "epoch": 4.096969696969697,
      "grad_norm": 0.2261577546596527,
      "learning_rate": 1.3726851851851853e-05,
      "loss": 0.0018,
      "step": 3380
    },
    {
      "epoch": 4.1030303030303035,
      "grad_norm": 0.23604777455329895,
      "learning_rate": 1.3716329966329966e-05,
      "loss": 0.0022,
      "step": 3385
    },
    {
      "epoch": 4.109090909090909,
      "grad_norm": 0.26967403292655945,
      "learning_rate": 1.3705808080808084e-05,
      "loss": 0.002,
      "step": 3390
    },
    {
      "epoch": 4.115151515151515,
      "grad_norm": 0.20435146987438202,
      "learning_rate": 1.3695286195286196e-05,
      "loss": 0.0025,
      "step": 3395
    },
    {
      "epoch": 4.121212121212121,
      "grad_norm": 0.2232040911912918,
      "learning_rate": 1.368476430976431e-05,
      "loss": 0.0019,
      "step": 3400
    },
    {
      "epoch": 4.127272727272727,
      "grad_norm": 0.19479893147945404,
      "learning_rate": 1.3674242424242425e-05,
      "loss": 0.002,
      "step": 3405
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 0.262400358915329,
      "learning_rate": 1.366372053872054e-05,
      "loss": 0.0022,
      "step": 3410
    },
    {
      "epoch": 4.13939393939394,
      "grad_norm": 0.2236371487379074,
      "learning_rate": 1.3653198653198654e-05,
      "loss": 0.0021,
      "step": 3415
    },
    {
      "epoch": 4.1454545454545455,
      "grad_norm": 0.17491425573825836,
      "learning_rate": 1.3642676767676768e-05,
      "loss": 0.002,
      "step": 3420
    },
    {
      "epoch": 4.151515151515151,
      "grad_norm": 0.20552273094654083,
      "learning_rate": 1.3632154882154884e-05,
      "loss": 0.0021,
      "step": 3425
    },
    {
      "epoch": 4.157575757575757,
      "grad_norm": 0.23167413473129272,
      "learning_rate": 1.3621632996632999e-05,
      "loss": 0.0024,
      "step": 3430
    },
    {
      "epoch": 4.163636363636364,
      "grad_norm": 0.293396532535553,
      "learning_rate": 1.3611111111111113e-05,
      "loss": 0.002,
      "step": 3435
    },
    {
      "epoch": 4.16969696969697,
      "grad_norm": 0.3108392655849457,
      "learning_rate": 1.3600589225589228e-05,
      "loss": 0.0024,
      "step": 3440
    },
    {
      "epoch": 4.175757575757576,
      "grad_norm": 0.2347637265920639,
      "learning_rate": 1.3590067340067342e-05,
      "loss": 0.0021,
      "step": 3445
    },
    {
      "epoch": 4.181818181818182,
      "grad_norm": 0.2398715317249298,
      "learning_rate": 1.3579545454545455e-05,
      "loss": 0.002,
      "step": 3450
    },
    {
      "epoch": 4.1878787878787875,
      "grad_norm": 0.19489964842796326,
      "learning_rate": 1.3569023569023569e-05,
      "loss": 0.0021,
      "step": 3455
    },
    {
      "epoch": 4.193939393939394,
      "grad_norm": 0.23654423654079437,
      "learning_rate": 1.3558501683501683e-05,
      "loss": 0.0023,
      "step": 3460
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.24163538217544556,
      "learning_rate": 1.35479797979798e-05,
      "loss": 0.0019,
      "step": 3465
    },
    {
      "epoch": 4.206060606060606,
      "grad_norm": 0.23365838825702667,
      "learning_rate": 1.3537457912457914e-05,
      "loss": 0.0027,
      "step": 3470
    },
    {
      "epoch": 4.212121212121212,
      "grad_norm": 0.22097793221473694,
      "learning_rate": 1.3526936026936028e-05,
      "loss": 0.0024,
      "step": 3475
    },
    {
      "epoch": 4.218181818181818,
      "grad_norm": 0.3085990846157074,
      "learning_rate": 1.3516414141414143e-05,
      "loss": 0.0026,
      "step": 3480
    },
    {
      "epoch": 4.224242424242425,
      "grad_norm": 0.22767408192157745,
      "learning_rate": 1.3505892255892257e-05,
      "loss": 0.002,
      "step": 3485
    },
    {
      "epoch": 4.2303030303030305,
      "grad_norm": 0.24224407970905304,
      "learning_rate": 1.3495370370370371e-05,
      "loss": 0.0019,
      "step": 3490
    },
    {
      "epoch": 4.236363636363636,
      "grad_norm": 0.22265899181365967,
      "learning_rate": 1.3484848484848486e-05,
      "loss": 0.0022,
      "step": 3495
    },
    {
      "epoch": 4.242424242424242,
      "grad_norm": 0.24991820752620697,
      "learning_rate": 1.3474326599326602e-05,
      "loss": 0.002,
      "step": 3500
    },
    {
      "epoch": 4.248484848484848,
      "grad_norm": 0.25445684790611267,
      "learning_rate": 1.3463804713804716e-05,
      "loss": 0.0018,
      "step": 3505
    },
    {
      "epoch": 4.254545454545455,
      "grad_norm": 0.3242897391319275,
      "learning_rate": 1.345328282828283e-05,
      "loss": 0.0023,
      "step": 3510
    },
    {
      "epoch": 4.260606060606061,
      "grad_norm": 0.33059161901474,
      "learning_rate": 1.3442760942760943e-05,
      "loss": 0.0027,
      "step": 3515
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.26641854643821716,
      "learning_rate": 1.3432239057239058e-05,
      "loss": 0.0025,
      "step": 3520
    },
    {
      "epoch": 4.2727272727272725,
      "grad_norm": 0.22612498700618744,
      "learning_rate": 1.3421717171717172e-05,
      "loss": 0.0025,
      "step": 3525
    },
    {
      "epoch": 4.278787878787878,
      "grad_norm": 0.19373351335525513,
      "learning_rate": 1.3411195286195286e-05,
      "loss": 0.0026,
      "step": 3530
    },
    {
      "epoch": 4.284848484848485,
      "grad_norm": 0.253212571144104,
      "learning_rate": 1.3400673400673401e-05,
      "loss": 0.0025,
      "step": 3535
    },
    {
      "epoch": 4.290909090909091,
      "grad_norm": 0.35504552721977234,
      "learning_rate": 1.3390151515151517e-05,
      "loss": 0.0024,
      "step": 3540
    },
    {
      "epoch": 4.296969696969697,
      "grad_norm": 0.2377055287361145,
      "learning_rate": 1.3379629629629631e-05,
      "loss": 0.0024,
      "step": 3545
    },
    {
      "epoch": 4.303030303030303,
      "grad_norm": 0.19721227884292603,
      "learning_rate": 1.3369107744107746e-05,
      "loss": 0.0023,
      "step": 3550
    },
    {
      "epoch": 4.3090909090909095,
      "grad_norm": 0.2604713439941406,
      "learning_rate": 1.335858585858586e-05,
      "loss": 0.0024,
      "step": 3555
    },
    {
      "epoch": 4.315151515151515,
      "grad_norm": 0.2719999849796295,
      "learning_rate": 1.3348063973063975e-05,
      "loss": 0.0023,
      "step": 3560
    },
    {
      "epoch": 4.321212121212121,
      "grad_norm": 0.28644174337387085,
      "learning_rate": 1.3337542087542087e-05,
      "loss": 0.0021,
      "step": 3565
    },
    {
      "epoch": 4.327272727272727,
      "grad_norm": 0.24174408614635468,
      "learning_rate": 1.3327020202020202e-05,
      "loss": 0.0024,
      "step": 3570
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.30247360467910767,
      "learning_rate": 1.331649831649832e-05,
      "loss": 0.0024,
      "step": 3575
    },
    {
      "epoch": 4.33939393939394,
      "grad_norm": 0.23021112382411957,
      "learning_rate": 1.3305976430976432e-05,
      "loss": 0.0026,
      "step": 3580
    },
    {
      "epoch": 4.345454545454546,
      "grad_norm": 0.2793614864349365,
      "learning_rate": 1.3295454545454546e-05,
      "loss": 0.0027,
      "step": 3585
    },
    {
      "epoch": 4.351515151515152,
      "grad_norm": 0.21972063183784485,
      "learning_rate": 1.328493265993266e-05,
      "loss": 0.0021,
      "step": 3590
    },
    {
      "epoch": 4.357575757575757,
      "grad_norm": 0.24965202808380127,
      "learning_rate": 1.3274410774410775e-05,
      "loss": 0.0024,
      "step": 3595
    },
    {
      "epoch": 4.363636363636363,
      "grad_norm": 0.3069505989551544,
      "learning_rate": 1.326388888888889e-05,
      "loss": 0.0025,
      "step": 3600
    },
    {
      "epoch": 4.36969696969697,
      "grad_norm": 0.230179563164711,
      "learning_rate": 1.3253367003367004e-05,
      "loss": 0.0024,
      "step": 3605
    },
    {
      "epoch": 4.375757575757576,
      "grad_norm": 0.2145940065383911,
      "learning_rate": 1.3242845117845118e-05,
      "loss": 0.0022,
      "step": 3610
    },
    {
      "epoch": 4.381818181818182,
      "grad_norm": 0.24300242960453033,
      "learning_rate": 1.3232323232323234e-05,
      "loss": 0.0022,
      "step": 3615
    },
    {
      "epoch": 4.387878787878788,
      "grad_norm": 0.3589112162590027,
      "learning_rate": 1.3221801346801349e-05,
      "loss": 0.0024,
      "step": 3620
    },
    {
      "epoch": 4.393939393939394,
      "grad_norm": 0.28424444794654846,
      "learning_rate": 1.3211279461279463e-05,
      "loss": 0.0024,
      "step": 3625
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.24341481924057007,
      "learning_rate": 1.3200757575757576e-05,
      "loss": 0.0023,
      "step": 3630
    },
    {
      "epoch": 4.406060606060606,
      "grad_norm": 0.21926303207874298,
      "learning_rate": 1.319023569023569e-05,
      "loss": 0.002,
      "step": 3635
    },
    {
      "epoch": 4.412121212121212,
      "grad_norm": 0.24359695613384247,
      "learning_rate": 1.3179713804713805e-05,
      "loss": 0.0025,
      "step": 3640
    },
    {
      "epoch": 4.418181818181818,
      "grad_norm": 0.28908872604370117,
      "learning_rate": 1.3169191919191919e-05,
      "loss": 0.0024,
      "step": 3645
    },
    {
      "epoch": 4.424242424242424,
      "grad_norm": 0.2231917828321457,
      "learning_rate": 1.3158670033670035e-05,
      "loss": 0.0026,
      "step": 3650
    },
    {
      "epoch": 4.430303030303031,
      "grad_norm": 0.2816103994846344,
      "learning_rate": 1.314814814814815e-05,
      "loss": 0.0027,
      "step": 3655
    },
    {
      "epoch": 4.4363636363636365,
      "grad_norm": 0.33676570653915405,
      "learning_rate": 1.3137626262626264e-05,
      "loss": 0.0022,
      "step": 3660
    },
    {
      "epoch": 4.442424242424242,
      "grad_norm": 0.2614557147026062,
      "learning_rate": 1.3127104377104378e-05,
      "loss": 0.0027,
      "step": 3665
    },
    {
      "epoch": 4.448484848484848,
      "grad_norm": 0.28484228253364563,
      "learning_rate": 1.3116582491582493e-05,
      "loss": 0.0022,
      "step": 3670
    },
    {
      "epoch": 4.454545454545454,
      "grad_norm": 0.21956878900527954,
      "learning_rate": 1.3106060606060607e-05,
      "loss": 0.0021,
      "step": 3675
    },
    {
      "epoch": 4.460606060606061,
      "grad_norm": 0.20632337033748627,
      "learning_rate": 1.3095538720538722e-05,
      "loss": 0.0028,
      "step": 3680
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 0.3248988687992096,
      "learning_rate": 1.3085016835016838e-05,
      "loss": 0.0023,
      "step": 3685
    },
    {
      "epoch": 4.472727272727273,
      "grad_norm": 0.19486185908317566,
      "learning_rate": 1.3074494949494952e-05,
      "loss": 0.0025,
      "step": 3690
    },
    {
      "epoch": 4.4787878787878785,
      "grad_norm": 0.21175192296504974,
      "learning_rate": 1.3063973063973066e-05,
      "loss": 0.0031,
      "step": 3695
    },
    {
      "epoch": 4.484848484848484,
      "grad_norm": 0.19676125049591064,
      "learning_rate": 1.3053451178451179e-05,
      "loss": 0.0029,
      "step": 3700
    },
    {
      "epoch": 4.490909090909091,
      "grad_norm": 0.1930566281080246,
      "learning_rate": 1.3042929292929293e-05,
      "loss": 0.0024,
      "step": 3705
    },
    {
      "epoch": 4.496969696969697,
      "grad_norm": 0.17947296798229218,
      "learning_rate": 1.3032407407407408e-05,
      "loss": 0.0021,
      "step": 3710
    },
    {
      "epoch": 4.499393939393939,
      "eval_average": 0.5461020760023066,
      "eval_crossner_ai": 0.5298507462186252,
      "eval_crossner_literature": 0.5208085611865858,
      "eval_crossner_music": 0.6835260115106453,
      "eval_crossner_politics": 0.571196094335592,
      "eval_crossner_science": 0.607821229000145,
      "eval_mit-movie": 0.5212765956947658,
      "eval_mit-restaurant": 0.38823529406978724,
      "eval_runtime": 20.9757,
      "eval_samples_per_second": 33.372,
      "eval_steps_per_second": 0.334,
      "step": 3712
    },
    {
      "epoch": 4.503030303030303,
      "grad_norm": 0.18397080898284912,
      "learning_rate": 1.3021885521885522e-05,
      "loss": 0.0023,
      "step": 3715
    },
    {
      "epoch": 4.509090909090909,
      "grad_norm": 0.2602265775203705,
      "learning_rate": 1.3011363636363637e-05,
      "loss": 0.0028,
      "step": 3720
    },
    {
      "epoch": 4.515151515151516,
      "grad_norm": 0.20116128027439117,
      "learning_rate": 1.3000841750841753e-05,
      "loss": 0.0024,
      "step": 3725
    },
    {
      "epoch": 4.5212121212121215,
      "grad_norm": 0.23894132673740387,
      "learning_rate": 1.2990319865319867e-05,
      "loss": 0.0025,
      "step": 3730
    },
    {
      "epoch": 4.527272727272727,
      "grad_norm": 0.1960233598947525,
      "learning_rate": 1.2979797979797981e-05,
      "loss": 0.0025,
      "step": 3735
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.29538989067077637,
      "learning_rate": 1.2969276094276096e-05,
      "loss": 0.0029,
      "step": 3740
    },
    {
      "epoch": 4.539393939393939,
      "grad_norm": 0.26481539011001587,
      "learning_rate": 1.295875420875421e-05,
      "loss": 0.0027,
      "step": 3745
    },
    {
      "epoch": 4.545454545454546,
      "grad_norm": 0.284361869096756,
      "learning_rate": 1.2948232323232323e-05,
      "loss": 0.0022,
      "step": 3750
    },
    {
      "epoch": 4.551515151515152,
      "grad_norm": 0.26904451847076416,
      "learning_rate": 1.2937710437710437e-05,
      "loss": 0.0023,
      "step": 3755
    },
    {
      "epoch": 4.557575757575758,
      "grad_norm": 0.2165660709142685,
      "learning_rate": 1.2927188552188555e-05,
      "loss": 0.0025,
      "step": 3760
    },
    {
      "epoch": 4.5636363636363635,
      "grad_norm": 0.2863002419471741,
      "learning_rate": 1.2916666666666668e-05,
      "loss": 0.0023,
      "step": 3765
    },
    {
      "epoch": 4.569696969696969,
      "grad_norm": 0.3034701347351074,
      "learning_rate": 1.2906144781144782e-05,
      "loss": 0.0026,
      "step": 3770
    },
    {
      "epoch": 4.575757575757576,
      "grad_norm": 0.29291465878486633,
      "learning_rate": 1.2895622895622897e-05,
      "loss": 0.0021,
      "step": 3775
    },
    {
      "epoch": 4.581818181818182,
      "grad_norm": 0.2605939507484436,
      "learning_rate": 1.2885101010101011e-05,
      "loss": 0.0028,
      "step": 3780
    },
    {
      "epoch": 4.587878787878788,
      "grad_norm": 0.2954179644584656,
      "learning_rate": 1.2874579124579125e-05,
      "loss": 0.0024,
      "step": 3785
    },
    {
      "epoch": 4.593939393939394,
      "grad_norm": 0.2623772621154785,
      "learning_rate": 1.286405723905724e-05,
      "loss": 0.0022,
      "step": 3790
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.3086014986038208,
      "learning_rate": 1.2853535353535354e-05,
      "loss": 0.0027,
      "step": 3795
    },
    {
      "epoch": 4.606060606060606,
      "grad_norm": 0.24544833600521088,
      "learning_rate": 1.284301346801347e-05,
      "loss": 0.0026,
      "step": 3800
    },
    {
      "epoch": 4.612121212121212,
      "grad_norm": 0.27274391055107117,
      "learning_rate": 1.2832491582491585e-05,
      "loss": 0.0029,
      "step": 3805
    },
    {
      "epoch": 4.618181818181818,
      "grad_norm": 0.22290045022964478,
      "learning_rate": 1.2821969696969699e-05,
      "loss": 0.0022,
      "step": 3810
    },
    {
      "epoch": 4.624242424242424,
      "grad_norm": 0.29121634364128113,
      "learning_rate": 1.2811447811447812e-05,
      "loss": 0.0026,
      "step": 3815
    },
    {
      "epoch": 4.63030303030303,
      "grad_norm": 0.2899317443370819,
      "learning_rate": 1.2800925925925926e-05,
      "loss": 0.0024,
      "step": 3820
    },
    {
      "epoch": 4.636363636363637,
      "grad_norm": 0.19119973480701447,
      "learning_rate": 1.279040404040404e-05,
      "loss": 0.0023,
      "step": 3825
    },
    {
      "epoch": 4.642424242424243,
      "grad_norm": 0.25180378556251526,
      "learning_rate": 1.2779882154882155e-05,
      "loss": 0.0025,
      "step": 3830
    },
    {
      "epoch": 4.648484848484848,
      "grad_norm": 0.22274576127529144,
      "learning_rate": 1.2769360269360271e-05,
      "loss": 0.0025,
      "step": 3835
    },
    {
      "epoch": 4.654545454545454,
      "grad_norm": 0.3419305980205536,
      "learning_rate": 1.2758838383838385e-05,
      "loss": 0.0026,
      "step": 3840
    },
    {
      "epoch": 4.66060606060606,
      "grad_norm": 0.250006765127182,
      "learning_rate": 1.27483164983165e-05,
      "loss": 0.0027,
      "step": 3845
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.2919210195541382,
      "learning_rate": 1.2737794612794614e-05,
      "loss": 0.0023,
      "step": 3850
    },
    {
      "epoch": 4.672727272727273,
      "grad_norm": 0.26581642031669617,
      "learning_rate": 1.2727272727272728e-05,
      "loss": 0.003,
      "step": 3855
    },
    {
      "epoch": 4.678787878787879,
      "grad_norm": 0.23333001136779785,
      "learning_rate": 1.2716750841750843e-05,
      "loss": 0.0025,
      "step": 3860
    },
    {
      "epoch": 4.684848484848485,
      "grad_norm": 0.28290408849716187,
      "learning_rate": 1.2706228956228957e-05,
      "loss": 0.0025,
      "step": 3865
    },
    {
      "epoch": 4.6909090909090905,
      "grad_norm": 0.22514009475708008,
      "learning_rate": 1.269570707070707e-05,
      "loss": 0.0023,
      "step": 3870
    },
    {
      "epoch": 4.696969696969697,
      "grad_norm": 0.30041763186454773,
      "learning_rate": 1.2685185185185188e-05,
      "loss": 0.0025,
      "step": 3875
    },
    {
      "epoch": 4.703030303030303,
      "grad_norm": 0.19251012802124023,
      "learning_rate": 1.26746632996633e-05,
      "loss": 0.0023,
      "step": 3880
    },
    {
      "epoch": 4.709090909090909,
      "grad_norm": 0.27505797147750854,
      "learning_rate": 1.2664141414141415e-05,
      "loss": 0.0027,
      "step": 3885
    },
    {
      "epoch": 4.715151515151515,
      "grad_norm": 0.23935933411121368,
      "learning_rate": 1.265361952861953e-05,
      "loss": 0.0026,
      "step": 3890
    },
    {
      "epoch": 4.721212121212122,
      "grad_norm": 0.29272764921188354,
      "learning_rate": 1.2643097643097644e-05,
      "loss": 0.0024,
      "step": 3895
    },
    {
      "epoch": 4.7272727272727275,
      "grad_norm": 0.22341406345367432,
      "learning_rate": 1.2632575757575758e-05,
      "loss": 0.0024,
      "step": 3900
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 0.25635191798210144,
      "learning_rate": 1.2622053872053872e-05,
      "loss": 0.0023,
      "step": 3905
    },
    {
      "epoch": 4.739393939393939,
      "grad_norm": 0.27151235938072205,
      "learning_rate": 1.2611531986531988e-05,
      "loss": 0.0025,
      "step": 3910
    },
    {
      "epoch": 4.745454545454545,
      "grad_norm": 0.24838046729564667,
      "learning_rate": 1.2601010101010103e-05,
      "loss": 0.0026,
      "step": 3915
    },
    {
      "epoch": 4.751515151515152,
      "grad_norm": 0.22718128561973572,
      "learning_rate": 1.2590488215488217e-05,
      "loss": 0.0024,
      "step": 3920
    },
    {
      "epoch": 4.757575757575758,
      "grad_norm": 0.2501765787601471,
      "learning_rate": 1.2579966329966332e-05,
      "loss": 0.0024,
      "step": 3925
    },
    {
      "epoch": 4.763636363636364,
      "grad_norm": 0.23187339305877686,
      "learning_rate": 1.2569444444444446e-05,
      "loss": 0.0027,
      "step": 3930
    },
    {
      "epoch": 4.7696969696969695,
      "grad_norm": 0.29657241702079773,
      "learning_rate": 1.2558922558922559e-05,
      "loss": 0.0029,
      "step": 3935
    },
    {
      "epoch": 4.775757575757575,
      "grad_norm": 0.300801157951355,
      "learning_rate": 1.2548400673400673e-05,
      "loss": 0.0028,
      "step": 3940
    },
    {
      "epoch": 4.781818181818182,
      "grad_norm": 0.25453799962997437,
      "learning_rate": 1.2537878787878789e-05,
      "loss": 0.0026,
      "step": 3945
    },
    {
      "epoch": 4.787878787878788,
      "grad_norm": 0.25449034571647644,
      "learning_rate": 1.2527356902356904e-05,
      "loss": 0.0032,
      "step": 3950
    },
    {
      "epoch": 4.793939393939394,
      "grad_norm": 0.17809222638607025,
      "learning_rate": 1.2516835016835018e-05,
      "loss": 0.0024,
      "step": 3955
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.29421573877334595,
      "learning_rate": 1.2506313131313132e-05,
      "loss": 0.0028,
      "step": 3960
    },
    {
      "epoch": 4.806060606060606,
      "grad_norm": 0.39182049036026,
      "learning_rate": 1.2495791245791247e-05,
      "loss": 0.0026,
      "step": 3965
    },
    {
      "epoch": 4.8121212121212125,
      "grad_norm": 0.16380931437015533,
      "learning_rate": 1.2485269360269361e-05,
      "loss": 0.0023,
      "step": 3970
    },
    {
      "epoch": 4.818181818181818,
      "grad_norm": 0.23690950870513916,
      "learning_rate": 1.2474747474747475e-05,
      "loss": 0.0028,
      "step": 3975
    },
    {
      "epoch": 4.824242424242424,
      "grad_norm": 0.2553883492946625,
      "learning_rate": 1.246422558922559e-05,
      "loss": 0.0025,
      "step": 3980
    },
    {
      "epoch": 4.83030303030303,
      "grad_norm": 0.25579410791397095,
      "learning_rate": 1.2453703703703706e-05,
      "loss": 0.0026,
      "step": 3985
    },
    {
      "epoch": 4.836363636363636,
      "grad_norm": 0.22850482165813446,
      "learning_rate": 1.244318181818182e-05,
      "loss": 0.0024,
      "step": 3990
    },
    {
      "epoch": 4.842424242424243,
      "grad_norm": 0.2340640276670456,
      "learning_rate": 1.2432659932659935e-05,
      "loss": 0.0029,
      "step": 3995
    },
    {
      "epoch": 4.848484848484849,
      "grad_norm": 0.2865757942199707,
      "learning_rate": 1.2422138047138047e-05,
      "loss": 0.0025,
      "step": 4000
    },
    {
      "epoch": 4.8545454545454545,
      "grad_norm": 0.22489002346992493,
      "learning_rate": 1.2411616161616162e-05,
      "loss": 0.0023,
      "step": 4005
    },
    {
      "epoch": 4.86060606060606,
      "grad_norm": 0.24139690399169922,
      "learning_rate": 1.2401094276094276e-05,
      "loss": 0.0028,
      "step": 4010
    },
    {
      "epoch": 4.866666666666666,
      "grad_norm": 0.3021042048931122,
      "learning_rate": 1.239057239057239e-05,
      "loss": 0.0027,
      "step": 4015
    },
    {
      "epoch": 4.872727272727273,
      "grad_norm": 0.22636592388153076,
      "learning_rate": 1.2380050505050507e-05,
      "loss": 0.0024,
      "step": 4020
    },
    {
      "epoch": 4.878787878787879,
      "grad_norm": 0.3025597631931305,
      "learning_rate": 1.2369528619528621e-05,
      "loss": 0.0028,
      "step": 4025
    },
    {
      "epoch": 4.884848484848485,
      "grad_norm": 0.2878189980983734,
      "learning_rate": 1.2359006734006735e-05,
      "loss": 0.0026,
      "step": 4030
    },
    {
      "epoch": 4.890909090909091,
      "grad_norm": 0.30324801802635193,
      "learning_rate": 1.234848484848485e-05,
      "loss": 0.0024,
      "step": 4035
    },
    {
      "epoch": 4.8969696969696965,
      "grad_norm": 0.2520323693752289,
      "learning_rate": 1.2337962962962964e-05,
      "loss": 0.0028,
      "step": 4040
    },
    {
      "epoch": 4.903030303030303,
      "grad_norm": 0.3188816010951996,
      "learning_rate": 1.2327441077441079e-05,
      "loss": 0.0026,
      "step": 4045
    },
    {
      "epoch": 4.909090909090909,
      "grad_norm": 0.2983569800853729,
      "learning_rate": 1.2316919191919193e-05,
      "loss": 0.003,
      "step": 4050
    },
    {
      "epoch": 4.915151515151515,
      "grad_norm": 0.2641560137271881,
      "learning_rate": 1.2306397306397306e-05,
      "loss": 0.0028,
      "step": 4055
    },
    {
      "epoch": 4.921212121212121,
      "grad_norm": 0.23905999958515167,
      "learning_rate": 1.2295875420875423e-05,
      "loss": 0.0028,
      "step": 4060
    },
    {
      "epoch": 4.927272727272728,
      "grad_norm": 0.230181023478508,
      "learning_rate": 1.2285353535353536e-05,
      "loss": 0.0027,
      "step": 4065
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 0.22371052205562592,
      "learning_rate": 1.227483164983165e-05,
      "loss": 0.0026,
      "step": 4070
    },
    {
      "epoch": 4.9393939393939394,
      "grad_norm": 0.3322824239730835,
      "learning_rate": 1.2264309764309765e-05,
      "loss": 0.0028,
      "step": 4075
    },
    {
      "epoch": 4.945454545454545,
      "grad_norm": 0.23464983701705933,
      "learning_rate": 1.225378787878788e-05,
      "loss": 0.0025,
      "step": 4080
    },
    {
      "epoch": 4.951515151515151,
      "grad_norm": 0.25122207403182983,
      "learning_rate": 1.2243265993265994e-05,
      "loss": 0.0024,
      "step": 4085
    },
    {
      "epoch": 4.957575757575758,
      "grad_norm": 0.28299370408058167,
      "learning_rate": 1.2232744107744108e-05,
      "loss": 0.0027,
      "step": 4090
    },
    {
      "epoch": 4.963636363636364,
      "grad_norm": 0.265430212020874,
      "learning_rate": 1.2222222222222224e-05,
      "loss": 0.0025,
      "step": 4095
    },
    {
      "epoch": 4.96969696969697,
      "grad_norm": 0.23457783460617065,
      "learning_rate": 1.2211700336700339e-05,
      "loss": 0.0023,
      "step": 4100
    },
    {
      "epoch": 4.975757575757576,
      "grad_norm": 0.33502107858657837,
      "learning_rate": 1.2201178451178453e-05,
      "loss": 0.0025,
      "step": 4105
    },
    {
      "epoch": 4.9818181818181815,
      "grad_norm": 0.2826726734638214,
      "learning_rate": 1.2190656565656567e-05,
      "loss": 0.0028,
      "step": 4110
    },
    {
      "epoch": 4.987878787878788,
      "grad_norm": 0.35158661007881165,
      "learning_rate": 1.2180134680134682e-05,
      "loss": 0.0027,
      "step": 4115
    },
    {
      "epoch": 4.993939393939394,
      "grad_norm": 0.37365493178367615,
      "learning_rate": 1.2169612794612794e-05,
      "loss": 0.0024,
      "step": 4120
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.27466028928756714,
      "learning_rate": 1.2159090909090909e-05,
      "loss": 0.0027,
      "step": 4125
    },
    {
      "epoch": 5.0,
      "eval_average": 0.5395753119459876,
      "eval_crossner_ai": 0.49343832015984734,
      "eval_crossner_literature": 0.47904191611767055,
      "eval_crossner_music": 0.6896057347169542,
      "eval_crossner_politics": 0.6029654035742829,
      "eval_crossner_science": 0.5919282510709476,
      "eval_mit-movie": 0.5239436619226503,
      "eval_mit-restaurant": 0.3961038960595611,
      "eval_runtime": 21.146,
      "eval_samples_per_second": 33.103,
      "eval_steps_per_second": 0.331,
      "step": 4125
    },
    {
      "epoch": 5.006060606060606,
      "grad_norm": 0.22562609612941742,
      "learning_rate": 1.2150673400673401e-05,
      "loss": 0.0027,
      "step": 4130
    },
    {
      "epoch": 5.012121212121212,
      "grad_norm": 0.25982537865638733,
      "learning_rate": 1.2140151515151517e-05,
      "loss": 0.0014,
      "step": 4135
    },
    {
      "epoch": 5.0181818181818185,
      "grad_norm": 0.19192272424697876,
      "learning_rate": 1.2129629629629631e-05,
      "loss": 0.0011,
      "step": 4140
    },
    {
      "epoch": 5.024242424242424,
      "grad_norm": 0.14073088765144348,
      "learning_rate": 1.2119107744107746e-05,
      "loss": 0.0012,
      "step": 4145
    },
    {
      "epoch": 5.03030303030303,
      "grad_norm": 0.31352657079696655,
      "learning_rate": 1.210858585858586e-05,
      "loss": 0.0017,
      "step": 4150
    },
    {
      "epoch": 5.036363636363636,
      "grad_norm": 0.2935698628425598,
      "learning_rate": 1.2098063973063973e-05,
      "loss": 0.001,
      "step": 4155
    },
    {
      "epoch": 5.042424242424242,
      "grad_norm": 0.6182765364646912,
      "learning_rate": 1.2087542087542087e-05,
      "loss": 0.0011,
      "step": 4160
    },
    {
      "epoch": 5.048484848484849,
      "grad_norm": 0.2288237065076828,
      "learning_rate": 1.2077020202020202e-05,
      "loss": 0.0015,
      "step": 4165
    },
    {
      "epoch": 5.054545454545455,
      "grad_norm": 0.3031150698661804,
      "learning_rate": 1.2066498316498318e-05,
      "loss": 0.001,
      "step": 4170
    },
    {
      "epoch": 5.0606060606060606,
      "grad_norm": 0.2376864105463028,
      "learning_rate": 1.2055976430976432e-05,
      "loss": 0.0014,
      "step": 4175
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 0.17536437511444092,
      "learning_rate": 1.2045454545454547e-05,
      "loss": 0.0013,
      "step": 4180
    },
    {
      "epoch": 5.072727272727272,
      "grad_norm": 0.18741801381111145,
      "learning_rate": 1.2034932659932661e-05,
      "loss": 0.0012,
      "step": 4185
    },
    {
      "epoch": 5.078787878787879,
      "grad_norm": 0.20907486975193024,
      "learning_rate": 1.2024410774410775e-05,
      "loss": 0.0015,
      "step": 4190
    },
    {
      "epoch": 5.084848484848485,
      "grad_norm": 0.21957281231880188,
      "learning_rate": 1.201388888888889e-05,
      "loss": 0.0011,
      "step": 4195
    },
    {
      "epoch": 5.090909090909091,
      "grad_norm": 0.2053535133600235,
      "learning_rate": 1.2003367003367004e-05,
      "loss": 0.0012,
      "step": 4200
    },
    {
      "epoch": 5.096969696969697,
      "grad_norm": 0.2788279950618744,
      "learning_rate": 1.1992845117845118e-05,
      "loss": 0.0013,
      "step": 4205
    },
    {
      "epoch": 5.1030303030303035,
      "grad_norm": 0.18861059844493866,
      "learning_rate": 1.1982323232323235e-05,
      "loss": 0.0012,
      "step": 4210
    },
    {
      "epoch": 5.109090909090909,
      "grad_norm": 0.24185727536678314,
      "learning_rate": 1.1971801346801349e-05,
      "loss": 0.0012,
      "step": 4215
    },
    {
      "epoch": 5.115151515151515,
      "grad_norm": 0.24563516676425934,
      "learning_rate": 1.1961279461279462e-05,
      "loss": 0.0012,
      "step": 4220
    },
    {
      "epoch": 5.121212121212121,
      "grad_norm": 0.2089444249868393,
      "learning_rate": 1.1950757575757576e-05,
      "loss": 0.0013,
      "step": 4225
    },
    {
      "epoch": 5.127272727272727,
      "grad_norm": 0.12139490246772766,
      "learning_rate": 1.194023569023569e-05,
      "loss": 0.001,
      "step": 4230
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 0.15086574852466583,
      "learning_rate": 1.1929713804713805e-05,
      "loss": 0.0011,
      "step": 4235
    },
    {
      "epoch": 5.13939393939394,
      "grad_norm": 0.17283116281032562,
      "learning_rate": 1.191919191919192e-05,
      "loss": 0.0009,
      "step": 4240
    },
    {
      "epoch": 5.1454545454545455,
      "grad_norm": 0.2204652577638626,
      "learning_rate": 1.1908670033670035e-05,
      "loss": 0.0012,
      "step": 4245
    },
    {
      "epoch": 5.151515151515151,
      "grad_norm": 0.20188866555690765,
      "learning_rate": 1.189814814814815e-05,
      "loss": 0.0012,
      "step": 4250
    },
    {
      "epoch": 5.157575757575757,
      "grad_norm": 0.2425822913646698,
      "learning_rate": 1.1887626262626264e-05,
      "loss": 0.0011,
      "step": 4255
    },
    {
      "epoch": 5.163636363636364,
      "grad_norm": 0.17936988174915314,
      "learning_rate": 1.1877104377104378e-05,
      "loss": 0.0012,
      "step": 4260
    },
    {
      "epoch": 5.16969696969697,
      "grad_norm": 0.17140240967273712,
      "learning_rate": 1.1866582491582493e-05,
      "loss": 0.001,
      "step": 4265
    },
    {
      "epoch": 5.175757575757576,
      "grad_norm": 0.1528005301952362,
      "learning_rate": 1.1856060606060607e-05,
      "loss": 0.0012,
      "step": 4270
    },
    {
      "epoch": 5.181818181818182,
      "grad_norm": 0.23634372651576996,
      "learning_rate": 1.184553872053872e-05,
      "loss": 0.0012,
      "step": 4275
    },
    {
      "epoch": 5.1878787878787875,
      "grad_norm": 0.22121082246303558,
      "learning_rate": 1.1835016835016838e-05,
      "loss": 0.0011,
      "step": 4280
    },
    {
      "epoch": 5.193939393939394,
      "grad_norm": 0.23847398161888123,
      "learning_rate": 1.1824494949494952e-05,
      "loss": 0.0013,
      "step": 4285
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.21382910013198853,
      "learning_rate": 1.1813973063973065e-05,
      "loss": 0.0011,
      "step": 4290
    },
    {
      "epoch": 5.206060606060606,
      "grad_norm": 0.13902609050273895,
      "learning_rate": 1.180345117845118e-05,
      "loss": 0.0016,
      "step": 4295
    },
    {
      "epoch": 5.212121212121212,
      "grad_norm": 0.22732630372047424,
      "learning_rate": 1.1792929292929294e-05,
      "loss": 0.0011,
      "step": 4300
    },
    {
      "epoch": 5.218181818181818,
      "grad_norm": 0.1637355536222458,
      "learning_rate": 1.1782407407407408e-05,
      "loss": 0.0013,
      "step": 4305
    },
    {
      "epoch": 5.224242424242425,
      "grad_norm": 0.15653415024280548,
      "learning_rate": 1.1771885521885522e-05,
      "loss": 0.0013,
      "step": 4310
    },
    {
      "epoch": 5.2303030303030305,
      "grad_norm": 0.21190369129180908,
      "learning_rate": 1.1761363636363637e-05,
      "loss": 0.0012,
      "step": 4315
    },
    {
      "epoch": 5.236363636363636,
      "grad_norm": 0.2536643445491791,
      "learning_rate": 1.1750841750841753e-05,
      "loss": 0.0014,
      "step": 4320
    },
    {
      "epoch": 5.242424242424242,
      "grad_norm": 0.1670505702495575,
      "learning_rate": 1.1740319865319867e-05,
      "loss": 0.0009,
      "step": 4325
    },
    {
      "epoch": 5.248484848484848,
      "grad_norm": 0.1864774376153946,
      "learning_rate": 1.1729797979797982e-05,
      "loss": 0.0013,
      "step": 4330
    },
    {
      "epoch": 5.254545454545455,
      "grad_norm": 0.2484673112630844,
      "learning_rate": 1.1719276094276096e-05,
      "loss": 0.0011,
      "step": 4335
    },
    {
      "epoch": 5.260606060606061,
      "grad_norm": 0.22805921733379364,
      "learning_rate": 1.1708754208754209e-05,
      "loss": 0.0011,
      "step": 4340
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 0.2170913815498352,
      "learning_rate": 1.1698232323232323e-05,
      "loss": 0.0013,
      "step": 4345
    },
    {
      "epoch": 5.2727272727272725,
      "grad_norm": 0.2410641312599182,
      "learning_rate": 1.1687710437710437e-05,
      "loss": 0.0013,
      "step": 4350
    },
    {
      "epoch": 5.278787878787878,
      "grad_norm": 0.15881888568401337,
      "learning_rate": 1.1677188552188554e-05,
      "loss": 0.0013,
      "step": 4355
    },
    {
      "epoch": 5.284848484848485,
      "grad_norm": 0.2943558990955353,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0014,
      "step": 4360
    },
    {
      "epoch": 5.290909090909091,
      "grad_norm": 0.19856464862823486,
      "learning_rate": 1.1656144781144782e-05,
      "loss": 0.0016,
      "step": 4365
    },
    {
      "epoch": 5.296969696969697,
      "grad_norm": 0.2452751100063324,
      "learning_rate": 1.1645622895622897e-05,
      "loss": 0.0012,
      "step": 4370
    },
    {
      "epoch": 5.303030303030303,
      "grad_norm": 0.2807188332080841,
      "learning_rate": 1.1635101010101011e-05,
      "loss": 0.0014,
      "step": 4375
    },
    {
      "epoch": 5.3090909090909095,
      "grad_norm": 0.18510745465755463,
      "learning_rate": 1.1624579124579125e-05,
      "loss": 0.0014,
      "step": 4380
    },
    {
      "epoch": 5.315151515151515,
      "grad_norm": 0.17750832438468933,
      "learning_rate": 1.161405723905724e-05,
      "loss": 0.001,
      "step": 4385
    },
    {
      "epoch": 5.321212121212121,
      "grad_norm": 0.23325660824775696,
      "learning_rate": 1.1603535353535354e-05,
      "loss": 0.0013,
      "step": 4390
    },
    {
      "epoch": 5.327272727272727,
      "grad_norm": 0.1676105111837387,
      "learning_rate": 1.159301346801347e-05,
      "loss": 0.0012,
      "step": 4395
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.20687179267406464,
      "learning_rate": 1.1582491582491585e-05,
      "loss": 0.0013,
      "step": 4400
    },
    {
      "epoch": 5.33939393939394,
      "grad_norm": 0.2173078954219818,
      "learning_rate": 1.1571969696969697e-05,
      "loss": 0.0015,
      "step": 4405
    },
    {
      "epoch": 5.345454545454546,
      "grad_norm": 0.2650887966156006,
      "learning_rate": 1.1561447811447812e-05,
      "loss": 0.0014,
      "step": 4410
    },
    {
      "epoch": 5.351515151515152,
      "grad_norm": 0.2306755781173706,
      "learning_rate": 1.1550925925925926e-05,
      "loss": 0.0014,
      "step": 4415
    },
    {
      "epoch": 5.357575757575757,
      "grad_norm": 0.17076943814754486,
      "learning_rate": 1.154040404040404e-05,
      "loss": 0.0015,
      "step": 4420
    },
    {
      "epoch": 5.363636363636363,
      "grad_norm": 0.30073457956314087,
      "learning_rate": 1.1529882154882155e-05,
      "loss": 0.0016,
      "step": 4425
    },
    {
      "epoch": 5.36969696969697,
      "grad_norm": 0.2725444734096527,
      "learning_rate": 1.1519360269360271e-05,
      "loss": 0.0013,
      "step": 4430
    },
    {
      "epoch": 5.375757575757576,
      "grad_norm": 0.22780746221542358,
      "learning_rate": 1.1508838383838385e-05,
      "loss": 0.0013,
      "step": 4435
    },
    {
      "epoch": 5.381818181818182,
      "grad_norm": 0.2755204141139984,
      "learning_rate": 1.14983164983165e-05,
      "loss": 0.0014,
      "step": 4440
    },
    {
      "epoch": 5.387878787878788,
      "grad_norm": 0.19038133323192596,
      "learning_rate": 1.1487794612794614e-05,
      "loss": 0.0015,
      "step": 4445
    },
    {
      "epoch": 5.393939393939394,
      "grad_norm": 0.2271558791399002,
      "learning_rate": 1.1477272727272729e-05,
      "loss": 0.0015,
      "step": 4450
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.21082772314548492,
      "learning_rate": 1.1466750841750843e-05,
      "loss": 0.0016,
      "step": 4455
    },
    {
      "epoch": 5.406060606060606,
      "grad_norm": 0.2231510877609253,
      "learning_rate": 1.1456228956228956e-05,
      "loss": 0.0014,
      "step": 4460
    },
    {
      "epoch": 5.412121212121212,
      "grad_norm": 0.19462183117866516,
      "learning_rate": 1.144570707070707e-05,
      "loss": 0.0013,
      "step": 4465
    },
    {
      "epoch": 5.418181818181818,
      "grad_norm": 0.19290731847286224,
      "learning_rate": 1.1435185185185186e-05,
      "loss": 0.0013,
      "step": 4470
    },
    {
      "epoch": 5.424242424242424,
      "grad_norm": 0.2597324848175049,
      "learning_rate": 1.14246632996633e-05,
      "loss": 0.0013,
      "step": 4475
    },
    {
      "epoch": 5.430303030303031,
      "grad_norm": 0.18648377060890198,
      "learning_rate": 1.1414141414141415e-05,
      "loss": 0.0013,
      "step": 4480
    },
    {
      "epoch": 5.4363636363636365,
      "grad_norm": 0.16112923622131348,
      "learning_rate": 1.140361952861953e-05,
      "loss": 0.0012,
      "step": 4485
    },
    {
      "epoch": 5.442424242424242,
      "grad_norm": 0.2731223702430725,
      "learning_rate": 1.1393097643097644e-05,
      "loss": 0.0016,
      "step": 4490
    },
    {
      "epoch": 5.448484848484848,
      "grad_norm": 0.2067812830209732,
      "learning_rate": 1.1382575757575758e-05,
      "loss": 0.0012,
      "step": 4495
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 0.19602498412132263,
      "learning_rate": 1.1372053872053872e-05,
      "loss": 0.0014,
      "step": 4500
    },
    {
      "epoch": 5.460606060606061,
      "grad_norm": 0.22153402864933014,
      "learning_rate": 1.1361531986531989e-05,
      "loss": 0.0014,
      "step": 4505
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 0.2633923888206482,
      "learning_rate": 1.1351010101010103e-05,
      "loss": 0.0016,
      "step": 4510
    },
    {
      "epoch": 5.472727272727273,
      "grad_norm": 0.2143058329820633,
      "learning_rate": 1.1340488215488217e-05,
      "loss": 0.0014,
      "step": 4515
    },
    {
      "epoch": 5.4787878787878785,
      "grad_norm": 0.295933336019516,
      "learning_rate": 1.1329966329966332e-05,
      "loss": 0.0015,
      "step": 4520
    },
    {
      "epoch": 5.484848484848484,
      "grad_norm": 0.1940242052078247,
      "learning_rate": 1.1319444444444444e-05,
      "loss": 0.0012,
      "step": 4525
    },
    {
      "epoch": 5.490909090909091,
      "grad_norm": 0.18072324991226196,
      "learning_rate": 1.1308922558922559e-05,
      "loss": 0.0011,
      "step": 4530
    },
    {
      "epoch": 5.496969696969697,
      "grad_norm": 0.20025494694709778,
      "learning_rate": 1.1298400673400673e-05,
      "loss": 0.0023,
      "step": 4535
    },
    {
      "epoch": 5.500606060606061,
      "eval_average": 0.5401932385056868,
      "eval_crossner_ai": 0.5032938075915081,
      "eval_crossner_literature": 0.5744934445268375,
      "eval_crossner_music": 0.7189542483159771,
      "eval_crossner_politics": 0.6013289036043901,
      "eval_crossner_science": 0.5892857142355837,
      "eval_mit-movie": 0.4309392264699063,
      "eval_mit-restaurant": 0.3630573247956043,
      "eval_runtime": 20.5958,
      "eval_samples_per_second": 33.987,
      "eval_steps_per_second": 0.34,
      "step": 4538
    },
    {
      "epoch": 5.503030303030303,
      "grad_norm": 0.1852727234363556,
      "learning_rate": 1.128787878787879e-05,
      "loss": 0.0012,
      "step": 4540
    },
    {
      "epoch": 5.509090909090909,
      "grad_norm": 0.24335993826389313,
      "learning_rate": 1.1277356902356904e-05,
      "loss": 0.0016,
      "step": 4545
    },
    {
      "epoch": 5.515151515151516,
      "grad_norm": 0.264068603515625,
      "learning_rate": 1.1266835016835018e-05,
      "loss": 0.0016,
      "step": 4550
    },
    {
      "epoch": 5.5212121212121215,
      "grad_norm": 0.23944512009620667,
      "learning_rate": 1.1256313131313132e-05,
      "loss": 0.0013,
      "step": 4555
    },
    {
      "epoch": 5.527272727272727,
      "grad_norm": 0.24200817942619324,
      "learning_rate": 1.1245791245791247e-05,
      "loss": 0.0015,
      "step": 4560
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 0.2878672480583191,
      "learning_rate": 1.1235269360269361e-05,
      "loss": 0.0011,
      "step": 4565
    },
    {
      "epoch": 5.539393939393939,
      "grad_norm": 0.2864210307598114,
      "learning_rate": 1.1224747474747476e-05,
      "loss": 0.0015,
      "step": 4570
    },
    {
      "epoch": 5.545454545454546,
      "grad_norm": 0.18318019807338715,
      "learning_rate": 1.121422558922559e-05,
      "loss": 0.0013,
      "step": 4575
    },
    {
      "epoch": 5.551515151515152,
      "grad_norm": 0.3628430962562561,
      "learning_rate": 1.1203703703703706e-05,
      "loss": 0.0015,
      "step": 4580
    },
    {
      "epoch": 5.557575757575758,
      "grad_norm": 0.22420430183410645,
      "learning_rate": 1.119318181818182e-05,
      "loss": 0.0013,
      "step": 4585
    },
    {
      "epoch": 5.5636363636363635,
      "grad_norm": 0.24879537522792816,
      "learning_rate": 1.1182659932659933e-05,
      "loss": 0.002,
      "step": 4590
    },
    {
      "epoch": 5.569696969696969,
      "grad_norm": 0.2517825961112976,
      "learning_rate": 1.1172138047138048e-05,
      "loss": 0.0014,
      "step": 4595
    },
    {
      "epoch": 5.575757575757576,
      "grad_norm": 0.22597546875476837,
      "learning_rate": 1.1161616161616162e-05,
      "loss": 0.0017,
      "step": 4600
    },
    {
      "epoch": 5.581818181818182,
      "grad_norm": 0.19459494948387146,
      "learning_rate": 1.1151094276094276e-05,
      "loss": 0.0016,
      "step": 4605
    },
    {
      "epoch": 5.587878787878788,
      "grad_norm": 0.28375011682510376,
      "learning_rate": 1.114057239057239e-05,
      "loss": 0.0015,
      "step": 4610
    },
    {
      "epoch": 5.593939393939394,
      "grad_norm": 0.2221837341785431,
      "learning_rate": 1.1130050505050507e-05,
      "loss": 0.0013,
      "step": 4615
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.32428017258644104,
      "learning_rate": 1.1119528619528621e-05,
      "loss": 0.0017,
      "step": 4620
    },
    {
      "epoch": 5.606060606060606,
      "grad_norm": 0.22327546775341034,
      "learning_rate": 1.1109006734006736e-05,
      "loss": 0.0013,
      "step": 4625
    },
    {
      "epoch": 5.612121212121212,
      "grad_norm": 0.27257007360458374,
      "learning_rate": 1.109848484848485e-05,
      "loss": 0.0013,
      "step": 4630
    },
    {
      "epoch": 5.618181818181818,
      "grad_norm": 0.27946737408638,
      "learning_rate": 1.1087962962962964e-05,
      "loss": 0.0014,
      "step": 4635
    },
    {
      "epoch": 5.624242424242424,
      "grad_norm": 0.2541114091873169,
      "learning_rate": 1.1077441077441079e-05,
      "loss": 0.0017,
      "step": 4640
    },
    {
      "epoch": 5.63030303030303,
      "grad_norm": 0.725053608417511,
      "learning_rate": 1.1066919191919191e-05,
      "loss": 0.0024,
      "step": 4645
    },
    {
      "epoch": 5.636363636363637,
      "grad_norm": 0.24392925202846527,
      "learning_rate": 1.1056397306397306e-05,
      "loss": 0.0016,
      "step": 4650
    },
    {
      "epoch": 5.642424242424243,
      "grad_norm": 0.22583159804344177,
      "learning_rate": 1.1045875420875422e-05,
      "loss": 0.0015,
      "step": 4655
    },
    {
      "epoch": 5.648484848484848,
      "grad_norm": 0.22617733478546143,
      "learning_rate": 1.1035353535353536e-05,
      "loss": 0.0015,
      "step": 4660
    },
    {
      "epoch": 5.654545454545454,
      "grad_norm": 0.18605823814868927,
      "learning_rate": 1.102483164983165e-05,
      "loss": 0.0017,
      "step": 4665
    },
    {
      "epoch": 5.66060606060606,
      "grad_norm": 0.1612546294927597,
      "learning_rate": 1.1014309764309765e-05,
      "loss": 0.0014,
      "step": 4670
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 0.2584681212902069,
      "learning_rate": 1.100378787878788e-05,
      "loss": 0.0014,
      "step": 4675
    },
    {
      "epoch": 5.672727272727273,
      "grad_norm": 0.15988099575042725,
      "learning_rate": 1.0993265993265994e-05,
      "loss": 0.0013,
      "step": 4680
    },
    {
      "epoch": 5.678787878787879,
      "grad_norm": 0.18549828231334686,
      "learning_rate": 1.0982744107744108e-05,
      "loss": 0.0017,
      "step": 4685
    },
    {
      "epoch": 5.684848484848485,
      "grad_norm": 0.23306523263454437,
      "learning_rate": 1.0972222222222224e-05,
      "loss": 0.0013,
      "step": 4690
    },
    {
      "epoch": 5.6909090909090905,
      "grad_norm": 0.20770305395126343,
      "learning_rate": 1.0961700336700339e-05,
      "loss": 0.0014,
      "step": 4695
    },
    {
      "epoch": 5.696969696969697,
      "grad_norm": 0.20871654152870178,
      "learning_rate": 1.0951178451178453e-05,
      "loss": 0.0014,
      "step": 4700
    },
    {
      "epoch": 5.703030303030303,
      "grad_norm": 0.23143219947814941,
      "learning_rate": 1.0940656565656567e-05,
      "loss": 0.0016,
      "step": 4705
    },
    {
      "epoch": 5.709090909090909,
      "grad_norm": 0.2290632426738739,
      "learning_rate": 1.093013468013468e-05,
      "loss": 0.0013,
      "step": 4710
    },
    {
      "epoch": 5.715151515151515,
      "grad_norm": 0.26686668395996094,
      "learning_rate": 1.0919612794612795e-05,
      "loss": 0.0018,
      "step": 4715
    },
    {
      "epoch": 5.721212121212122,
      "grad_norm": 0.16355930268764496,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 0.0013,
      "step": 4720
    },
    {
      "epoch": 5.7272727272727275,
      "grad_norm": 0.213740736246109,
      "learning_rate": 1.0898569023569023e-05,
      "loss": 0.0014,
      "step": 4725
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 0.3323938846588135,
      "learning_rate": 1.088804713804714e-05,
      "loss": 0.0018,
      "step": 4730
    },
    {
      "epoch": 5.739393939393939,
      "grad_norm": 0.24027156829833984,
      "learning_rate": 1.0877525252525254e-05,
      "loss": 0.0017,
      "step": 4735
    },
    {
      "epoch": 5.745454545454545,
      "grad_norm": 0.20341511070728302,
      "learning_rate": 1.0867003367003368e-05,
      "loss": 0.0016,
      "step": 4740
    },
    {
      "epoch": 5.751515151515152,
      "grad_norm": 0.1627878099679947,
      "learning_rate": 1.0856481481481483e-05,
      "loss": 0.0014,
      "step": 4745
    },
    {
      "epoch": 5.757575757575758,
      "grad_norm": 0.1932951658964157,
      "learning_rate": 1.0845959595959597e-05,
      "loss": 0.0013,
      "step": 4750
    },
    {
      "epoch": 5.763636363636364,
      "grad_norm": 0.2036845088005066,
      "learning_rate": 1.0835437710437711e-05,
      "loss": 0.0015,
      "step": 4755
    },
    {
      "epoch": 5.7696969696969695,
      "grad_norm": 0.1592068374156952,
      "learning_rate": 1.0824915824915824e-05,
      "loss": 0.0017,
      "step": 4760
    },
    {
      "epoch": 5.775757575757575,
      "grad_norm": 0.4398476481437683,
      "learning_rate": 1.0814393939393942e-05,
      "loss": 0.0022,
      "step": 4765
    },
    {
      "epoch": 5.781818181818182,
      "grad_norm": 0.315223753452301,
      "learning_rate": 1.0803872053872056e-05,
      "loss": 0.0016,
      "step": 4770
    },
    {
      "epoch": 5.787878787878788,
      "grad_norm": 0.17137855291366577,
      "learning_rate": 1.0793350168350169e-05,
      "loss": 0.0013,
      "step": 4775
    },
    {
      "epoch": 5.793939393939394,
      "grad_norm": 0.1621934324502945,
      "learning_rate": 1.0782828282828283e-05,
      "loss": 0.0015,
      "step": 4780
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.1587027609348297,
      "learning_rate": 1.0772306397306398e-05,
      "loss": 0.0056,
      "step": 4785
    },
    {
      "epoch": 5.806060606060606,
      "grad_norm": 0.22360974550247192,
      "learning_rate": 1.0761784511784512e-05,
      "loss": 0.002,
      "step": 4790
    },
    {
      "epoch": 5.8121212121212125,
      "grad_norm": 0.24079623818397522,
      "learning_rate": 1.0751262626262626e-05,
      "loss": 0.0019,
      "step": 4795
    },
    {
      "epoch": 5.818181818181818,
      "grad_norm": 0.25691550970077515,
      "learning_rate": 1.0740740740740742e-05,
      "loss": 0.0016,
      "step": 4800
    },
    {
      "epoch": 5.824242424242424,
      "grad_norm": 0.17953801155090332,
      "learning_rate": 1.0730218855218857e-05,
      "loss": 0.0015,
      "step": 4805
    },
    {
      "epoch": 5.83030303030303,
      "grad_norm": 0.22423449158668518,
      "learning_rate": 1.0719696969696971e-05,
      "loss": 0.0017,
      "step": 4810
    },
    {
      "epoch": 5.836363636363636,
      "grad_norm": 0.19061893224716187,
      "learning_rate": 1.0709175084175086e-05,
      "loss": 0.0013,
      "step": 4815
    },
    {
      "epoch": 5.842424242424243,
      "grad_norm": 0.21504665911197662,
      "learning_rate": 1.06986531986532e-05,
      "loss": 0.0014,
      "step": 4820
    },
    {
      "epoch": 5.848484848484849,
      "grad_norm": 0.23965449631214142,
      "learning_rate": 1.0688131313131313e-05,
      "loss": 0.0014,
      "step": 4825
    },
    {
      "epoch": 5.8545454545454545,
      "grad_norm": 0.256599098443985,
      "learning_rate": 1.0677609427609427e-05,
      "loss": 0.0016,
      "step": 4830
    },
    {
      "epoch": 5.86060606060606,
      "grad_norm": 0.24347060918807983,
      "learning_rate": 1.0667087542087542e-05,
      "loss": 0.0016,
      "step": 4835
    },
    {
      "epoch": 5.866666666666666,
      "grad_norm": 0.3755606710910797,
      "learning_rate": 1.0656565656565658e-05,
      "loss": 0.0018,
      "step": 4840
    },
    {
      "epoch": 5.872727272727273,
      "grad_norm": 0.23541486263275146,
      "learning_rate": 1.0646043771043772e-05,
      "loss": 0.0015,
      "step": 4845
    },
    {
      "epoch": 5.878787878787879,
      "grad_norm": 0.23562724888324738,
      "learning_rate": 1.0635521885521886e-05,
      "loss": 0.0016,
      "step": 4850
    },
    {
      "epoch": 5.884848484848485,
      "grad_norm": 0.24241822957992554,
      "learning_rate": 1.0625e-05,
      "loss": 0.0015,
      "step": 4855
    },
    {
      "epoch": 5.890909090909091,
      "grad_norm": 0.2769075334072113,
      "learning_rate": 1.0614478114478115e-05,
      "loss": 0.0016,
      "step": 4860
    },
    {
      "epoch": 5.8969696969696965,
      "grad_norm": 0.23952694237232208,
      "learning_rate": 1.060395622895623e-05,
      "loss": 0.0016,
      "step": 4865
    },
    {
      "epoch": 5.903030303030303,
      "grad_norm": 0.2427929937839508,
      "learning_rate": 1.0593434343434344e-05,
      "loss": 0.0015,
      "step": 4870
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 0.2499101758003235,
      "learning_rate": 1.058291245791246e-05,
      "loss": 0.0018,
      "step": 4875
    },
    {
      "epoch": 5.915151515151515,
      "grad_norm": 0.2676198482513428,
      "learning_rate": 1.0572390572390574e-05,
      "loss": 0.0017,
      "step": 4880
    },
    {
      "epoch": 5.921212121212121,
      "grad_norm": 0.20102383196353912,
      "learning_rate": 1.0561868686868689e-05,
      "loss": 0.0015,
      "step": 4885
    },
    {
      "epoch": 5.927272727272728,
      "grad_norm": 0.27559733390808105,
      "learning_rate": 1.0551346801346803e-05,
      "loss": 0.0014,
      "step": 4890
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 0.2304730862379074,
      "learning_rate": 1.0540824915824916e-05,
      "loss": 0.0017,
      "step": 4895
    },
    {
      "epoch": 5.9393939393939394,
      "grad_norm": 0.2274383157491684,
      "learning_rate": 1.053030303030303e-05,
      "loss": 0.0017,
      "step": 4900
    },
    {
      "epoch": 5.945454545454545,
      "grad_norm": 0.22922436892986298,
      "learning_rate": 1.0519781144781145e-05,
      "loss": 0.0014,
      "step": 4905
    },
    {
      "epoch": 5.951515151515151,
      "grad_norm": 0.2871556878089905,
      "learning_rate": 1.0509259259259259e-05,
      "loss": 0.0013,
      "step": 4910
    },
    {
      "epoch": 5.957575757575758,
      "grad_norm": 0.2512493431568146,
      "learning_rate": 1.0498737373737375e-05,
      "loss": 0.0015,
      "step": 4915
    },
    {
      "epoch": 5.963636363636364,
      "grad_norm": 0.19950833916664124,
      "learning_rate": 1.048821548821549e-05,
      "loss": 0.0016,
      "step": 4920
    },
    {
      "epoch": 5.96969696969697,
      "grad_norm": 0.21752288937568665,
      "learning_rate": 1.0477693602693604e-05,
      "loss": 0.0017,
      "step": 4925
    },
    {
      "epoch": 5.975757575757576,
      "grad_norm": 0.26633259654045105,
      "learning_rate": 1.0467171717171718e-05,
      "loss": 0.0016,
      "step": 4930
    },
    {
      "epoch": 5.9818181818181815,
      "grad_norm": 0.18663693964481354,
      "learning_rate": 1.0456649831649833e-05,
      "loss": 0.0015,
      "step": 4935
    },
    {
      "epoch": 5.987878787878788,
      "grad_norm": 0.25662392377853394,
      "learning_rate": 1.0446127946127947e-05,
      "loss": 0.0016,
      "step": 4940
    },
    {
      "epoch": 5.993939393939394,
      "grad_norm": 0.27143123745918274,
      "learning_rate": 1.043560606060606e-05,
      "loss": 0.0017,
      "step": 4945
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.2861235439777374,
      "learning_rate": 1.0425084175084178e-05,
      "loss": 0.0017,
      "step": 4950
    },
    {
      "epoch": 6.0,
      "eval_average": 0.5534833962357395,
      "eval_crossner_ai": 0.4860050890084267,
      "eval_crossner_literature": 0.5906362544517921,
      "eval_crossner_music": 0.6939371803736584,
      "eval_crossner_politics": 0.5749792874395513,
      "eval_crossner_science": 0.5912653974862648,
      "eval_mit-movie": 0.5983379500890201,
      "eval_mit-restaurant": 0.3392226148014634,
      "eval_runtime": 20.8828,
      "eval_samples_per_second": 33.52,
      "eval_steps_per_second": 0.335,
      "step": 4950
    },
    {
      "epoch": 6.006060606060606,
      "grad_norm": 0.18422330915927887,
      "learning_rate": 1.0414562289562292e-05,
      "loss": 0.0013,
      "step": 4955
    },
    {
      "epoch": 6.012121212121212,
      "grad_norm": 0.14056958258152008,
      "learning_rate": 1.0404040404040405e-05,
      "loss": 0.0008,
      "step": 4960
    },
    {
      "epoch": 6.0181818181818185,
      "grad_norm": 0.16783952713012695,
      "learning_rate": 1.0393518518518519e-05,
      "loss": 0.0006,
      "step": 4965
    },
    {
      "epoch": 6.024242424242424,
      "grad_norm": 0.12769727408885956,
      "learning_rate": 1.0382996632996633e-05,
      "loss": 0.001,
      "step": 4970
    },
    {
      "epoch": 6.03030303030303,
      "grad_norm": 0.19812393188476562,
      "learning_rate": 1.0372474747474748e-05,
      "loss": 0.0007,
      "step": 4975
    },
    {
      "epoch": 6.036363636363636,
      "grad_norm": 0.10312450677156448,
      "learning_rate": 1.0361952861952862e-05,
      "loss": 0.0009,
      "step": 4980
    },
    {
      "epoch": 6.042424242424242,
      "grad_norm": 0.11559689044952393,
      "learning_rate": 1.0351430976430978e-05,
      "loss": 0.0007,
      "step": 4985
    },
    {
      "epoch": 6.048484848484849,
      "grad_norm": 0.3009748160839081,
      "learning_rate": 1.0340909090909093e-05,
      "loss": 0.0011,
      "step": 4990
    },
    {
      "epoch": 6.054545454545455,
      "grad_norm": 0.15984268486499786,
      "learning_rate": 1.0330387205387207e-05,
      "loss": 0.0006,
      "step": 4995
    },
    {
      "epoch": 6.0606060606060606,
      "grad_norm": 0.15645092725753784,
      "learning_rate": 1.0319865319865321e-05,
      "loss": 0.0008,
      "step": 5000
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 0.13219109177589417,
      "learning_rate": 1.0309343434343436e-05,
      "loss": 0.0008,
      "step": 5005
    },
    {
      "epoch": 6.072727272727272,
      "grad_norm": 0.14525339007377625,
      "learning_rate": 1.0298821548821548e-05,
      "loss": 0.0008,
      "step": 5010
    },
    {
      "epoch": 6.078787878787879,
      "grad_norm": 0.20669998228549957,
      "learning_rate": 1.0288299663299663e-05,
      "loss": 0.0007,
      "step": 5015
    },
    {
      "epoch": 6.084848484848485,
      "grad_norm": 0.14524778723716736,
      "learning_rate": 1.0277777777777777e-05,
      "loss": 0.0007,
      "step": 5020
    },
    {
      "epoch": 6.090909090909091,
      "grad_norm": 0.2160983681678772,
      "learning_rate": 1.0267255892255893e-05,
      "loss": 0.0007,
      "step": 5025
    },
    {
      "epoch": 6.096969696969697,
      "grad_norm": 0.14566417038440704,
      "learning_rate": 1.0256734006734008e-05,
      "loss": 0.0008,
      "step": 5030
    },
    {
      "epoch": 6.1030303030303035,
      "grad_norm": 0.1774812787771225,
      "learning_rate": 1.0246212121212122e-05,
      "loss": 0.0007,
      "step": 5035
    },
    {
      "epoch": 6.109090909090909,
      "grad_norm": 0.18839946389198303,
      "learning_rate": 1.0235690235690236e-05,
      "loss": 0.0008,
      "step": 5040
    },
    {
      "epoch": 6.115151515151515,
      "grad_norm": 0.1990203857421875,
      "learning_rate": 1.0225168350168351e-05,
      "loss": 0.0008,
      "step": 5045
    },
    {
      "epoch": 6.121212121212121,
      "grad_norm": 0.1743031144142151,
      "learning_rate": 1.0214646464646465e-05,
      "loss": 0.0009,
      "step": 5050
    },
    {
      "epoch": 6.127272727272727,
      "grad_norm": 0.12424260377883911,
      "learning_rate": 1.020412457912458e-05,
      "loss": 0.0007,
      "step": 5055
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 0.16401945054531097,
      "learning_rate": 1.0193602693602696e-05,
      "loss": 0.0006,
      "step": 5060
    },
    {
      "epoch": 6.13939393939394,
      "grad_norm": 0.19051215052604675,
      "learning_rate": 1.018308080808081e-05,
      "loss": 0.0007,
      "step": 5065
    },
    {
      "epoch": 6.1454545454545455,
      "grad_norm": 0.213876411318779,
      "learning_rate": 1.0172558922558925e-05,
      "loss": 0.0012,
      "step": 5070
    },
    {
      "epoch": 6.151515151515151,
      "grad_norm": 0.1260700225830078,
      "learning_rate": 1.0162037037037037e-05,
      "loss": 0.0007,
      "step": 5075
    },
    {
      "epoch": 6.157575757575757,
      "grad_norm": 0.14229834079742432,
      "learning_rate": 1.0151515151515152e-05,
      "loss": 0.0006,
      "step": 5080
    },
    {
      "epoch": 6.163636363636364,
      "grad_norm": 0.14826451241970062,
      "learning_rate": 1.0140993265993266e-05,
      "loss": 0.0006,
      "step": 5085
    },
    {
      "epoch": 6.16969696969697,
      "grad_norm": 0.1262344866991043,
      "learning_rate": 1.013047138047138e-05,
      "loss": 0.0007,
      "step": 5090
    },
    {
      "epoch": 6.175757575757576,
      "grad_norm": 0.1713092029094696,
      "learning_rate": 1.0119949494949495e-05,
      "loss": 0.0006,
      "step": 5095
    },
    {
      "epoch": 6.181818181818182,
      "grad_norm": 0.17961785197257996,
      "learning_rate": 1.010942760942761e-05,
      "loss": 0.0011,
      "step": 5100
    },
    {
      "epoch": 6.1878787878787875,
      "grad_norm": 0.5942153334617615,
      "learning_rate": 1.0098905723905725e-05,
      "loss": 0.0008,
      "step": 5105
    },
    {
      "epoch": 6.193939393939394,
      "grad_norm": 0.24103911221027374,
      "learning_rate": 1.008838383838384e-05,
      "loss": 0.0008,
      "step": 5110
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.24254797399044037,
      "learning_rate": 1.0077861952861954e-05,
      "loss": 0.0008,
      "step": 5115
    },
    {
      "epoch": 6.206060606060606,
      "grad_norm": 0.16398224234580994,
      "learning_rate": 1.0067340067340068e-05,
      "loss": 0.0009,
      "step": 5120
    },
    {
      "epoch": 6.212121212121212,
      "grad_norm": 0.21382524073123932,
      "learning_rate": 1.0056818181818183e-05,
      "loss": 0.0005,
      "step": 5125
    },
    {
      "epoch": 6.218181818181818,
      "grad_norm": 0.21300290524959564,
      "learning_rate": 1.0046296296296295e-05,
      "loss": 0.001,
      "step": 5130
    },
    {
      "epoch": 6.224242424242425,
      "grad_norm": 0.1461671143770218,
      "learning_rate": 1.0035774410774413e-05,
      "loss": 0.0009,
      "step": 5135
    },
    {
      "epoch": 6.2303030303030305,
      "grad_norm": 0.16916152834892273,
      "learning_rate": 1.0025252525252526e-05,
      "loss": 0.0007,
      "step": 5140
    },
    {
      "epoch": 6.236363636363636,
      "grad_norm": 0.12049049139022827,
      "learning_rate": 1.001473063973064e-05,
      "loss": 0.0006,
      "step": 5145
    },
    {
      "epoch": 6.242424242424242,
      "grad_norm": 0.18673664331436157,
      "learning_rate": 1.0004208754208755e-05,
      "loss": 0.0009,
      "step": 5150
    },
    {
      "epoch": 6.248484848484848,
      "grad_norm": 0.22308297455310822,
      "learning_rate": 9.993686868686869e-06,
      "loss": 0.001,
      "step": 5155
    },
    {
      "epoch": 6.254545454545455,
      "grad_norm": 0.13387200236320496,
      "learning_rate": 9.983164983164983e-06,
      "loss": 0.0011,
      "step": 5160
    },
    {
      "epoch": 6.260606060606061,
      "grad_norm": 0.13283124566078186,
      "learning_rate": 9.9726430976431e-06,
      "loss": 0.0008,
      "step": 5165
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 0.16015370190143585,
      "learning_rate": 9.962121212121212e-06,
      "loss": 0.0008,
      "step": 5170
    },
    {
      "epoch": 6.2727272727272725,
      "grad_norm": 0.17726470530033112,
      "learning_rate": 9.951599326599327e-06,
      "loss": 0.0011,
      "step": 5175
    },
    {
      "epoch": 6.278787878787878,
      "grad_norm": 0.1782257854938507,
      "learning_rate": 9.941077441077443e-06,
      "loss": 0.001,
      "step": 5180
    },
    {
      "epoch": 6.284848484848485,
      "grad_norm": 0.21460667252540588,
      "learning_rate": 9.930555555555557e-06,
      "loss": 0.0011,
      "step": 5185
    },
    {
      "epoch": 6.290909090909091,
      "grad_norm": 0.35386693477630615,
      "learning_rate": 9.920033670033672e-06,
      "loss": 0.001,
      "step": 5190
    },
    {
      "epoch": 6.296969696969697,
      "grad_norm": 0.16113686561584473,
      "learning_rate": 9.909511784511784e-06,
      "loss": 0.0007,
      "step": 5195
    },
    {
      "epoch": 6.303030303030303,
      "grad_norm": 0.22433169186115265,
      "learning_rate": 9.8989898989899e-06,
      "loss": 0.0009,
      "step": 5200
    },
    {
      "epoch": 6.3090909090909095,
      "grad_norm": 0.2135896384716034,
      "learning_rate": 9.888468013468015e-06,
      "loss": 0.0009,
      "step": 5205
    },
    {
      "epoch": 6.315151515151515,
      "grad_norm": 0.21195636689662933,
      "learning_rate": 9.877946127946129e-06,
      "loss": 0.0009,
      "step": 5210
    },
    {
      "epoch": 6.321212121212121,
      "grad_norm": 0.14722225069999695,
      "learning_rate": 9.867424242424243e-06,
      "loss": 0.001,
      "step": 5215
    },
    {
      "epoch": 6.327272727272727,
      "grad_norm": 0.14003708958625793,
      "learning_rate": 9.856902356902358e-06,
      "loss": 0.0007,
      "step": 5220
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 0.2197216898202896,
      "learning_rate": 9.846380471380472e-06,
      "loss": 0.0012,
      "step": 5225
    },
    {
      "epoch": 6.33939393939394,
      "grad_norm": 0.1246655061841011,
      "learning_rate": 9.835858585858587e-06,
      "loss": 0.0007,
      "step": 5230
    },
    {
      "epoch": 6.345454545454546,
      "grad_norm": 0.21731874346733093,
      "learning_rate": 9.825336700336701e-06,
      "loss": 0.0008,
      "step": 5235
    },
    {
      "epoch": 6.351515151515152,
      "grad_norm": 0.2597884237766266,
      "learning_rate": 9.814814814814815e-06,
      "loss": 0.0009,
      "step": 5240
    },
    {
      "epoch": 6.357575757575757,
      "grad_norm": 0.24429699778556824,
      "learning_rate": 9.80429292929293e-06,
      "loss": 0.001,
      "step": 5245
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 0.16004501283168793,
      "learning_rate": 9.793771043771044e-06,
      "loss": 0.001,
      "step": 5250
    },
    {
      "epoch": 6.36969696969697,
      "grad_norm": 0.21002811193466187,
      "learning_rate": 9.78324915824916e-06,
      "loss": 0.0009,
      "step": 5255
    },
    {
      "epoch": 6.375757575757576,
      "grad_norm": 0.1493871510028839,
      "learning_rate": 9.772727272727273e-06,
      "loss": 0.0011,
      "step": 5260
    },
    {
      "epoch": 6.381818181818182,
      "grad_norm": 0.19602751731872559,
      "learning_rate": 9.762205387205387e-06,
      "loss": 0.0008,
      "step": 5265
    },
    {
      "epoch": 6.387878787878788,
      "grad_norm": 0.22273525595664978,
      "learning_rate": 9.751683501683502e-06,
      "loss": 0.0008,
      "step": 5270
    },
    {
      "epoch": 6.393939393939394,
      "grad_norm": 0.2657351791858673,
      "learning_rate": 9.741161616161618e-06,
      "loss": 0.0008,
      "step": 5275
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.2152516096830368,
      "learning_rate": 9.730639730639732e-06,
      "loss": 0.0011,
      "step": 5280
    },
    {
      "epoch": 6.406060606060606,
      "grad_norm": 0.2197493612766266,
      "learning_rate": 9.720117845117845e-06,
      "loss": 0.0009,
      "step": 5285
    },
    {
      "epoch": 6.412121212121212,
      "grad_norm": 0.16212286055088043,
      "learning_rate": 9.70959595959596e-06,
      "loss": 0.0009,
      "step": 5290
    },
    {
      "epoch": 6.418181818181818,
      "grad_norm": 0.1866850107908249,
      "learning_rate": 9.699074074074075e-06,
      "loss": 0.001,
      "step": 5295
    },
    {
      "epoch": 6.424242424242424,
      "grad_norm": 0.1635623276233673,
      "learning_rate": 9.68855218855219e-06,
      "loss": 0.0008,
      "step": 5300
    },
    {
      "epoch": 6.430303030303031,
      "grad_norm": 0.2032293677330017,
      "learning_rate": 9.678030303030304e-06,
      "loss": 0.001,
      "step": 5305
    },
    {
      "epoch": 6.4363636363636365,
      "grad_norm": 0.24090293049812317,
      "learning_rate": 9.667508417508419e-06,
      "loss": 0.0009,
      "step": 5310
    },
    {
      "epoch": 6.442424242424242,
      "grad_norm": 0.21070653200149536,
      "learning_rate": 9.656986531986533e-06,
      "loss": 0.0011,
      "step": 5315
    },
    {
      "epoch": 6.448484848484848,
      "grad_norm": 0.1973126083612442,
      "learning_rate": 9.646464646464647e-06,
      "loss": 0.0007,
      "step": 5320
    },
    {
      "epoch": 6.454545454545454,
      "grad_norm": 0.1861370950937271,
      "learning_rate": 9.635942760942762e-06,
      "loss": 0.001,
      "step": 5325
    },
    {
      "epoch": 6.460606060606061,
      "grad_norm": 0.2156771570444107,
      "learning_rate": 9.625420875420876e-06,
      "loss": 0.0009,
      "step": 5330
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 0.3159637153148651,
      "learning_rate": 9.61489898989899e-06,
      "loss": 0.0009,
      "step": 5335
    },
    {
      "epoch": 6.472727272727273,
      "grad_norm": 0.2890952527523041,
      "learning_rate": 9.604377104377105e-06,
      "loss": 0.0011,
      "step": 5340
    },
    {
      "epoch": 6.4787878787878785,
      "grad_norm": 0.16973267495632172,
      "learning_rate": 9.59385521885522e-06,
      "loss": 0.0008,
      "step": 5345
    },
    {
      "epoch": 6.484848484848484,
      "grad_norm": 0.21129517257213593,
      "learning_rate": 9.583333333333335e-06,
      "loss": 0.001,
      "step": 5350
    },
    {
      "epoch": 6.490909090909091,
      "grad_norm": 0.25701457262039185,
      "learning_rate": 9.572811447811448e-06,
      "loss": 0.0012,
      "step": 5355
    },
    {
      "epoch": 6.496969696969697,
      "grad_norm": 0.16461803019046783,
      "learning_rate": 9.562289562289562e-06,
      "loss": 0.0009,
      "step": 5360
    },
    {
      "epoch": 6.499393939393939,
      "eval_average": 0.5676683143329196,
      "eval_crossner_ai": 0.5006622516055113,
      "eval_crossner_literature": 0.5958986730501376,
      "eval_crossner_music": 0.6923625980941024,
      "eval_crossner_politics": 0.5893004114725366,
      "eval_crossner_science": 0.6391061452012554,
      "eval_mit-movie": 0.580110497188056,
      "eval_mit-restaurant": 0.376237623718838,
      "eval_runtime": 20.8259,
      "eval_samples_per_second": 33.612,
      "eval_steps_per_second": 0.336,
      "step": 5362
    },
    {
      "epoch": 6.503030303030303,
      "grad_norm": 0.24296800792217255,
      "learning_rate": 9.551767676767677e-06,
      "loss": 0.001,
      "step": 5365
    },
    {
      "epoch": 6.509090909090909,
      "grad_norm": 0.14813823997974396,
      "learning_rate": 9.541245791245793e-06,
      "loss": 0.0009,
      "step": 5370
    },
    {
      "epoch": 6.515151515151516,
      "grad_norm": 0.11296524107456207,
      "learning_rate": 9.530723905723907e-06,
      "loss": 0.0008,
      "step": 5375
    },
    {
      "epoch": 6.5212121212121215,
      "grad_norm": 0.23979146778583527,
      "learning_rate": 9.52020202020202e-06,
      "loss": 0.0008,
      "step": 5380
    },
    {
      "epoch": 6.527272727272727,
      "grad_norm": 0.18985441327095032,
      "learning_rate": 9.509680134680136e-06,
      "loss": 0.0007,
      "step": 5385
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 0.17276953160762787,
      "learning_rate": 9.49915824915825e-06,
      "loss": 0.0008,
      "step": 5390
    },
    {
      "epoch": 6.539393939393939,
      "grad_norm": 0.19360576570034027,
      "learning_rate": 9.488636363636365e-06,
      "loss": 0.0011,
      "step": 5395
    },
    {
      "epoch": 6.545454545454546,
      "grad_norm": 0.2154400497674942,
      "learning_rate": 9.47811447811448e-06,
      "loss": 0.0011,
      "step": 5400
    },
    {
      "epoch": 6.551515151515152,
      "grad_norm": 0.2534883916378021,
      "learning_rate": 9.467592592592594e-06,
      "loss": 0.0009,
      "step": 5405
    },
    {
      "epoch": 6.557575757575758,
      "grad_norm": 0.14150826632976532,
      "learning_rate": 9.457070707070708e-06,
      "loss": 0.0009,
      "step": 5410
    },
    {
      "epoch": 6.5636363636363635,
      "grad_norm": 0.21903516352176666,
      "learning_rate": 9.446548821548822e-06,
      "loss": 0.001,
      "step": 5415
    },
    {
      "epoch": 6.569696969696969,
      "grad_norm": 0.1669965386390686,
      "learning_rate": 9.436026936026937e-06,
      "loss": 0.001,
      "step": 5420
    },
    {
      "epoch": 6.575757575757576,
      "grad_norm": 0.18398164212703705,
      "learning_rate": 9.425505050505051e-06,
      "loss": 0.0011,
      "step": 5425
    },
    {
      "epoch": 6.581818181818182,
      "grad_norm": 0.16166727244853973,
      "learning_rate": 9.414983164983166e-06,
      "loss": 0.001,
      "step": 5430
    },
    {
      "epoch": 6.587878787878788,
      "grad_norm": 0.18672817945480347,
      "learning_rate": 9.40446127946128e-06,
      "loss": 0.001,
      "step": 5435
    },
    {
      "epoch": 6.593939393939394,
      "grad_norm": 0.23970937728881836,
      "learning_rate": 9.393939393939396e-06,
      "loss": 0.0009,
      "step": 5440
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.1672876626253128,
      "learning_rate": 9.383417508417509e-06,
      "loss": 0.0011,
      "step": 5445
    },
    {
      "epoch": 6.606060606060606,
      "grad_norm": 0.18197834491729736,
      "learning_rate": 9.372895622895623e-06,
      "loss": 0.001,
      "step": 5450
    },
    {
      "epoch": 6.612121212121212,
      "grad_norm": 0.16017308831214905,
      "learning_rate": 9.362373737373737e-06,
      "loss": 0.001,
      "step": 5455
    },
    {
      "epoch": 6.618181818181818,
      "grad_norm": 0.20686322450637817,
      "learning_rate": 9.351851851851854e-06,
      "loss": 0.0009,
      "step": 5460
    },
    {
      "epoch": 6.624242424242424,
      "grad_norm": 0.1953241229057312,
      "learning_rate": 9.341329966329968e-06,
      "loss": 0.0007,
      "step": 5465
    },
    {
      "epoch": 6.63030303030303,
      "grad_norm": 0.24467015266418457,
      "learning_rate": 9.33080808080808e-06,
      "loss": 0.0011,
      "step": 5470
    },
    {
      "epoch": 6.636363636363637,
      "grad_norm": 0.11098255217075348,
      "learning_rate": 9.320286195286195e-06,
      "loss": 0.0009,
      "step": 5475
    },
    {
      "epoch": 6.642424242424243,
      "grad_norm": 0.19042260944843292,
      "learning_rate": 9.309764309764311e-06,
      "loss": 0.0006,
      "step": 5480
    },
    {
      "epoch": 6.648484848484848,
      "grad_norm": 0.28235554695129395,
      "learning_rate": 9.299242424242425e-06,
      "loss": 0.0008,
      "step": 5485
    },
    {
      "epoch": 6.654545454545454,
      "grad_norm": 0.267791748046875,
      "learning_rate": 9.28872053872054e-06,
      "loss": 0.001,
      "step": 5490
    },
    {
      "epoch": 6.66060606060606,
      "grad_norm": 0.178019180893898,
      "learning_rate": 9.278198653198654e-06,
      "loss": 0.0011,
      "step": 5495
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.17521628737449646,
      "learning_rate": 9.267676767676769e-06,
      "loss": 0.001,
      "step": 5500
    },
    {
      "epoch": 6.672727272727273,
      "grad_norm": 0.18684807419776917,
      "learning_rate": 9.257154882154883e-06,
      "loss": 0.0011,
      "step": 5505
    },
    {
      "epoch": 6.678787878787879,
      "grad_norm": 0.15090234577655792,
      "learning_rate": 9.246632996632997e-06,
      "loss": 0.001,
      "step": 5510
    },
    {
      "epoch": 6.684848484848485,
      "grad_norm": 0.2754772901535034,
      "learning_rate": 9.236111111111112e-06,
      "loss": 0.0011,
      "step": 5515
    },
    {
      "epoch": 6.6909090909090905,
      "grad_norm": 0.2482859194278717,
      "learning_rate": 9.225589225589226e-06,
      "loss": 0.0013,
      "step": 5520
    },
    {
      "epoch": 6.696969696969697,
      "grad_norm": 0.13910217583179474,
      "learning_rate": 9.21506734006734e-06,
      "loss": 0.0009,
      "step": 5525
    },
    {
      "epoch": 6.703030303030303,
      "grad_norm": 0.15228228271007538,
      "learning_rate": 9.204545454545455e-06,
      "loss": 0.0009,
      "step": 5530
    },
    {
      "epoch": 6.709090909090909,
      "grad_norm": 0.2036060392856598,
      "learning_rate": 9.19402356902357e-06,
      "loss": 0.0009,
      "step": 5535
    },
    {
      "epoch": 6.715151515151515,
      "grad_norm": 0.18003930151462555,
      "learning_rate": 9.183501683501684e-06,
      "loss": 0.001,
      "step": 5540
    },
    {
      "epoch": 6.721212121212122,
      "grad_norm": 0.17244447767734528,
      "learning_rate": 9.172979797979798e-06,
      "loss": 0.0009,
      "step": 5545
    },
    {
      "epoch": 6.7272727272727275,
      "grad_norm": 0.22686417400836945,
      "learning_rate": 9.162457912457913e-06,
      "loss": 0.0009,
      "step": 5550
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 0.23134979605674744,
      "learning_rate": 9.151936026936029e-06,
      "loss": 0.0008,
      "step": 5555
    },
    {
      "epoch": 6.739393939393939,
      "grad_norm": 0.2394936978816986,
      "learning_rate": 9.141414141414143e-06,
      "loss": 0.0011,
      "step": 5560
    },
    {
      "epoch": 6.745454545454545,
      "grad_norm": 0.26218098402023315,
      "learning_rate": 9.130892255892256e-06,
      "loss": 0.0009,
      "step": 5565
    },
    {
      "epoch": 6.751515151515152,
      "grad_norm": 0.1848386526107788,
      "learning_rate": 9.120370370370372e-06,
      "loss": 0.001,
      "step": 5570
    },
    {
      "epoch": 6.757575757575758,
      "grad_norm": 0.18044911324977875,
      "learning_rate": 9.109848484848486e-06,
      "loss": 0.0009,
      "step": 5575
    },
    {
      "epoch": 6.763636363636364,
      "grad_norm": 0.21878059208393097,
      "learning_rate": 9.0993265993266e-06,
      "loss": 0.0012,
      "step": 5580
    },
    {
      "epoch": 6.7696969696969695,
      "grad_norm": 0.23800316452980042,
      "learning_rate": 9.088804713804715e-06,
      "loss": 0.0008,
      "step": 5585
    },
    {
      "epoch": 6.775757575757575,
      "grad_norm": 0.16868376731872559,
      "learning_rate": 9.07828282828283e-06,
      "loss": 0.0012,
      "step": 5590
    },
    {
      "epoch": 6.781818181818182,
      "grad_norm": 0.17646539211273193,
      "learning_rate": 9.067760942760944e-06,
      "loss": 0.0009,
      "step": 5595
    },
    {
      "epoch": 6.787878787878788,
      "grad_norm": 0.19481247663497925,
      "learning_rate": 9.057239057239058e-06,
      "loss": 0.0013,
      "step": 5600
    },
    {
      "epoch": 6.793939393939394,
      "grad_norm": 0.17376668751239777,
      "learning_rate": 9.046717171717172e-06,
      "loss": 0.001,
      "step": 5605
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.14745040237903595,
      "learning_rate": 9.036195286195287e-06,
      "loss": 0.0007,
      "step": 5610
    },
    {
      "epoch": 6.806060606060606,
      "grad_norm": 0.258305162191391,
      "learning_rate": 9.025673400673401e-06,
      "loss": 0.0012,
      "step": 5615
    },
    {
      "epoch": 6.8121212121212125,
      "grad_norm": 0.21481147408485413,
      "learning_rate": 9.015151515151516e-06,
      "loss": 0.0009,
      "step": 5620
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 0.20735931396484375,
      "learning_rate": 9.00462962962963e-06,
      "loss": 0.0011,
      "step": 5625
    },
    {
      "epoch": 6.824242424242424,
      "grad_norm": 0.19048890471458435,
      "learning_rate": 8.994107744107744e-06,
      "loss": 0.0009,
      "step": 5630
    },
    {
      "epoch": 6.83030303030303,
      "grad_norm": 0.15717673301696777,
      "learning_rate": 8.983585858585859e-06,
      "loss": 0.0008,
      "step": 5635
    },
    {
      "epoch": 6.836363636363636,
      "grad_norm": 0.13037140667438507,
      "learning_rate": 8.973063973063973e-06,
      "loss": 0.0011,
      "step": 5640
    },
    {
      "epoch": 6.842424242424243,
      "grad_norm": 0.2537412643432617,
      "learning_rate": 8.96254208754209e-06,
      "loss": 0.0009,
      "step": 5645
    },
    {
      "epoch": 6.848484848484849,
      "grad_norm": 0.22351869940757751,
      "learning_rate": 8.952020202020204e-06,
      "loss": 0.0008,
      "step": 5650
    },
    {
      "epoch": 6.8545454545454545,
      "grad_norm": 0.20539860427379608,
      "learning_rate": 8.941498316498316e-06,
      "loss": 0.0012,
      "step": 5655
    },
    {
      "epoch": 6.86060606060606,
      "grad_norm": 0.2896372973918915,
      "learning_rate": 8.93097643097643e-06,
      "loss": 0.0012,
      "step": 5660
    },
    {
      "epoch": 6.866666666666666,
      "grad_norm": 0.22941096127033234,
      "learning_rate": 8.920454545454547e-06,
      "loss": 0.001,
      "step": 5665
    },
    {
      "epoch": 6.872727272727273,
      "grad_norm": 0.31296971440315247,
      "learning_rate": 8.909932659932661e-06,
      "loss": 0.0012,
      "step": 5670
    },
    {
      "epoch": 6.878787878787879,
      "grad_norm": 0.11804752051830292,
      "learning_rate": 8.899410774410776e-06,
      "loss": 0.0008,
      "step": 5675
    },
    {
      "epoch": 6.884848484848485,
      "grad_norm": 0.23386675119400024,
      "learning_rate": 8.888888888888888e-06,
      "loss": 0.0012,
      "step": 5680
    },
    {
      "epoch": 6.890909090909091,
      "grad_norm": 0.16258572041988373,
      "learning_rate": 8.878367003367004e-06,
      "loss": 0.0009,
      "step": 5685
    },
    {
      "epoch": 6.8969696969696965,
      "grad_norm": 0.18049171566963196,
      "learning_rate": 8.867845117845119e-06,
      "loss": 0.0008,
      "step": 5690
    },
    {
      "epoch": 6.903030303030303,
      "grad_norm": 0.1908106952905655,
      "learning_rate": 8.857323232323233e-06,
      "loss": 0.001,
      "step": 5695
    },
    {
      "epoch": 6.909090909090909,
      "grad_norm": 0.13639546930789948,
      "learning_rate": 8.846801346801348e-06,
      "loss": 0.001,
      "step": 5700
    },
    {
      "epoch": 6.915151515151515,
      "grad_norm": 0.16537778079509735,
      "learning_rate": 8.836279461279462e-06,
      "loss": 0.0009,
      "step": 5705
    },
    {
      "epoch": 6.921212121212121,
      "grad_norm": 0.1283491849899292,
      "learning_rate": 8.825757575757576e-06,
      "loss": 0.0009,
      "step": 5710
    },
    {
      "epoch": 6.927272727272728,
      "grad_norm": 0.21806110441684723,
      "learning_rate": 8.81523569023569e-06,
      "loss": 0.0009,
      "step": 5715
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 0.22761204838752747,
      "learning_rate": 8.804713804713805e-06,
      "loss": 0.0008,
      "step": 5720
    },
    {
      "epoch": 6.9393939393939394,
      "grad_norm": 0.211371511220932,
      "learning_rate": 8.79419191919192e-06,
      "loss": 0.0011,
      "step": 5725
    },
    {
      "epoch": 6.945454545454545,
      "grad_norm": 0.26751506328582764,
      "learning_rate": 8.783670033670034e-06,
      "loss": 0.0011,
      "step": 5730
    },
    {
      "epoch": 6.951515151515151,
      "grad_norm": 0.17574170231819153,
      "learning_rate": 8.773148148148148e-06,
      "loss": 0.0009,
      "step": 5735
    },
    {
      "epoch": 6.957575757575758,
      "grad_norm": 0.21246355772018433,
      "learning_rate": 8.762626262626264e-06,
      "loss": 0.001,
      "step": 5740
    },
    {
      "epoch": 6.963636363636364,
      "grad_norm": 0.25801676511764526,
      "learning_rate": 8.752104377104377e-06,
      "loss": 0.001,
      "step": 5745
    },
    {
      "epoch": 6.96969696969697,
      "grad_norm": 0.3372555375099182,
      "learning_rate": 8.741582491582491e-06,
      "loss": 0.001,
      "step": 5750
    },
    {
      "epoch": 6.975757575757576,
      "grad_norm": 0.17541202902793884,
      "learning_rate": 8.731060606060606e-06,
      "loss": 0.0012,
      "step": 5755
    },
    {
      "epoch": 6.9818181818181815,
      "grad_norm": 0.17772118747234344,
      "learning_rate": 8.720538720538722e-06,
      "loss": 0.0009,
      "step": 5760
    },
    {
      "epoch": 6.987878787878788,
      "grad_norm": 0.17277568578720093,
      "learning_rate": 8.710016835016836e-06,
      "loss": 0.0011,
      "step": 5765
    },
    {
      "epoch": 6.993939393939394,
      "grad_norm": 0.2529706358909607,
      "learning_rate": 8.69949494949495e-06,
      "loss": 0.0011,
      "step": 5770
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.16513386368751526,
      "learning_rate": 8.688973063973065e-06,
      "loss": 0.0007,
      "step": 5775
    },
    {
      "epoch": 7.0,
      "eval_average": 0.5488145524943662,
      "eval_crossner_ai": 0.4754316068556041,
      "eval_crossner_literature": 0.5659928655860994,
      "eval_crossner_music": 0.7123088127686218,
      "eval_crossner_politics": 0.5245628642296872,
      "eval_crossner_science": 0.6439909296550902,
      "eval_mit-movie": 0.5777777777283457,
      "eval_mit-restaurant": 0.34163701063711455,
      "eval_runtime": 20.8061,
      "eval_samples_per_second": 33.644,
      "eval_steps_per_second": 0.336,
      "step": 5775
    },
    {
      "epoch": 7.006060606060606,
      "grad_norm": 0.3439306914806366,
      "learning_rate": 8.67845117845118e-06,
      "loss": 0.0012,
      "step": 5780
    },
    {
      "epoch": 7.012121212121212,
      "grad_norm": 0.11571791768074036,
      "learning_rate": 8.67003367003367e-06,
      "loss": 0.0007,
      "step": 5785
    },
    {
      "epoch": 7.0181818181818185,
      "grad_norm": 0.16322818398475647,
      "learning_rate": 8.659511784511784e-06,
      "loss": 0.0006,
      "step": 5790
    },
    {
      "epoch": 7.024242424242424,
      "grad_norm": 0.1574089676141739,
      "learning_rate": 8.6489898989899e-06,
      "loss": 0.0007,
      "step": 5795
    },
    {
      "epoch": 7.03030303030303,
      "grad_norm": 0.14194710552692413,
      "learning_rate": 8.638468013468015e-06,
      "loss": 0.0005,
      "step": 5800
    },
    {
      "epoch": 7.036363636363636,
      "grad_norm": 0.06260515004396439,
      "learning_rate": 8.62794612794613e-06,
      "loss": 0.0005,
      "step": 5805
    },
    {
      "epoch": 7.042424242424242,
      "grad_norm": 0.10426615178585052,
      "learning_rate": 8.617424242424242e-06,
      "loss": 0.0007,
      "step": 5810
    },
    {
      "epoch": 7.048484848484849,
      "grad_norm": 0.16457192599773407,
      "learning_rate": 8.606902356902358e-06,
      "loss": 0.0005,
      "step": 5815
    },
    {
      "epoch": 7.054545454545455,
      "grad_norm": 0.10622871667146683,
      "learning_rate": 8.596380471380472e-06,
      "loss": 0.0006,
      "step": 5820
    },
    {
      "epoch": 7.0606060606060606,
      "grad_norm": 0.14674609899520874,
      "learning_rate": 8.585858585858587e-06,
      "loss": 0.0005,
      "step": 5825
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 0.11921565979719162,
      "learning_rate": 8.575336700336701e-06,
      "loss": 0.0005,
      "step": 5830
    },
    {
      "epoch": 7.072727272727272,
      "grad_norm": 0.11989126354455948,
      "learning_rate": 8.564814814814816e-06,
      "loss": 0.0006,
      "step": 5835
    },
    {
      "epoch": 7.078787878787879,
      "grad_norm": 0.08586626499891281,
      "learning_rate": 8.55429292929293e-06,
      "loss": 0.0004,
      "step": 5840
    },
    {
      "epoch": 7.084848484848485,
      "grad_norm": 0.12617641687393188,
      "learning_rate": 8.543771043771044e-06,
      "loss": 0.0004,
      "step": 5845
    },
    {
      "epoch": 7.090909090909091,
      "grad_norm": 0.08738400042057037,
      "learning_rate": 8.533249158249159e-06,
      "loss": 0.0006,
      "step": 5850
    },
    {
      "epoch": 7.096969696969697,
      "grad_norm": 0.24909551441669464,
      "learning_rate": 8.522727272727273e-06,
      "loss": 0.0007,
      "step": 5855
    },
    {
      "epoch": 7.1030303030303035,
      "grad_norm": 0.08889155834913254,
      "learning_rate": 8.512205387205387e-06,
      "loss": 0.0006,
      "step": 5860
    },
    {
      "epoch": 7.109090909090909,
      "grad_norm": 0.1822611689567566,
      "learning_rate": 8.501683501683502e-06,
      "loss": 0.0006,
      "step": 5865
    },
    {
      "epoch": 7.115151515151515,
      "grad_norm": 0.14752371609210968,
      "learning_rate": 8.491161616161618e-06,
      "loss": 0.0005,
      "step": 5870
    },
    {
      "epoch": 7.121212121212121,
      "grad_norm": 0.11277687549591064,
      "learning_rate": 8.48063973063973e-06,
      "loss": 0.0005,
      "step": 5875
    },
    {
      "epoch": 7.127272727272727,
      "grad_norm": 0.13463813066482544,
      "learning_rate": 8.470117845117845e-06,
      "loss": 0.0005,
      "step": 5880
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 0.2089211493730545,
      "learning_rate": 8.45959595959596e-06,
      "loss": 0.0006,
      "step": 5885
    },
    {
      "epoch": 7.13939393939394,
      "grad_norm": 0.12283745408058167,
      "learning_rate": 8.449074074074075e-06,
      "loss": 0.0004,
      "step": 5890
    },
    {
      "epoch": 7.1454545454545455,
      "grad_norm": 0.19525083899497986,
      "learning_rate": 8.43855218855219e-06,
      "loss": 0.0008,
      "step": 5895
    },
    {
      "epoch": 7.151515151515151,
      "grad_norm": 0.2226705253124237,
      "learning_rate": 8.428030303030304e-06,
      "loss": 0.0007,
      "step": 5900
    },
    {
      "epoch": 7.157575757575757,
      "grad_norm": 0.12571614980697632,
      "learning_rate": 8.417508417508419e-06,
      "loss": 0.0003,
      "step": 5905
    },
    {
      "epoch": 7.163636363636364,
      "grad_norm": 0.1548597514629364,
      "learning_rate": 8.406986531986533e-06,
      "loss": 0.0005,
      "step": 5910
    },
    {
      "epoch": 7.16969696969697,
      "grad_norm": 0.13763831555843353,
      "learning_rate": 8.396464646464647e-06,
      "loss": 0.0007,
      "step": 5915
    },
    {
      "epoch": 7.175757575757576,
      "grad_norm": 0.14959359169006348,
      "learning_rate": 8.385942760942762e-06,
      "loss": 0.0005,
      "step": 5920
    },
    {
      "epoch": 7.181818181818182,
      "grad_norm": 0.15386079251766205,
      "learning_rate": 8.375420875420876e-06,
      "loss": 0.0007,
      "step": 5925
    },
    {
      "epoch": 7.1878787878787875,
      "grad_norm": 0.18769361078739166,
      "learning_rate": 8.36489898989899e-06,
      "loss": 0.0006,
      "step": 5930
    },
    {
      "epoch": 7.193939393939394,
      "grad_norm": 0.15068693459033966,
      "learning_rate": 8.354377104377105e-06,
      "loss": 0.0005,
      "step": 5935
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.14740249514579773,
      "learning_rate": 8.34385521885522e-06,
      "loss": 0.0006,
      "step": 5940
    },
    {
      "epoch": 7.206060606060606,
      "grad_norm": 0.15571698546409607,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0006,
      "step": 5945
    },
    {
      "epoch": 7.212121212121212,
      "grad_norm": 0.1563580483198166,
      "learning_rate": 8.322811447811448e-06,
      "loss": 0.0005,
      "step": 5950
    },
    {
      "epoch": 7.218181818181818,
      "grad_norm": 0.1339610069990158,
      "learning_rate": 8.312289562289563e-06,
      "loss": 0.0004,
      "step": 5955
    },
    {
      "epoch": 7.224242424242425,
      "grad_norm": 0.17706400156021118,
      "learning_rate": 8.301767676767677e-06,
      "loss": 0.0006,
      "step": 5960
    },
    {
      "epoch": 7.2303030303030305,
      "grad_norm": 0.23897799849510193,
      "learning_rate": 8.291245791245793e-06,
      "loss": 0.0005,
      "step": 5965
    },
    {
      "epoch": 7.236363636363636,
      "grad_norm": 0.12609872221946716,
      "learning_rate": 8.280723905723906e-06,
      "loss": 0.0005,
      "step": 5970
    },
    {
      "epoch": 7.242424242424242,
      "grad_norm": 0.13906769454479218,
      "learning_rate": 8.27020202020202e-06,
      "loss": 0.0004,
      "step": 5975
    },
    {
      "epoch": 7.248484848484848,
      "grad_norm": 0.10832192748785019,
      "learning_rate": 8.259680134680136e-06,
      "loss": 0.0007,
      "step": 5980
    },
    {
      "epoch": 7.254545454545455,
      "grad_norm": 0.15783418715000153,
      "learning_rate": 8.24915824915825e-06,
      "loss": 0.0004,
      "step": 5985
    },
    {
      "epoch": 7.260606060606061,
      "grad_norm": 0.24271512031555176,
      "learning_rate": 8.238636363636365e-06,
      "loss": 0.0004,
      "step": 5990
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 0.14163395762443542,
      "learning_rate": 8.228114478114478e-06,
      "loss": 0.0005,
      "step": 5995
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 0.12016680836677551,
      "learning_rate": 8.217592592592594e-06,
      "loss": 0.0005,
      "step": 6000
    },
    {
      "epoch": 7.278787878787878,
      "grad_norm": 0.18495656549930573,
      "learning_rate": 8.207070707070708e-06,
      "loss": 0.0005,
      "step": 6005
    },
    {
      "epoch": 7.284848484848485,
      "grad_norm": 0.1543472856283188,
      "learning_rate": 8.196548821548822e-06,
      "loss": 0.0003,
      "step": 6010
    },
    {
      "epoch": 7.290909090909091,
      "grad_norm": 0.21425558626651764,
      "learning_rate": 8.186026936026937e-06,
      "loss": 0.0005,
      "step": 6015
    },
    {
      "epoch": 7.296969696969697,
      "grad_norm": 0.1734420508146286,
      "learning_rate": 8.175505050505051e-06,
      "loss": 0.0005,
      "step": 6020
    },
    {
      "epoch": 7.303030303030303,
      "grad_norm": 0.24532926082611084,
      "learning_rate": 8.164983164983166e-06,
      "loss": 0.0005,
      "step": 6025
    },
    {
      "epoch": 7.3090909090909095,
      "grad_norm": 0.07702531665563583,
      "learning_rate": 8.15446127946128e-06,
      "loss": 0.0005,
      "step": 6030
    },
    {
      "epoch": 7.315151515151515,
      "grad_norm": 0.1930897831916809,
      "learning_rate": 8.143939393939394e-06,
      "loss": 0.0006,
      "step": 6035
    },
    {
      "epoch": 7.321212121212121,
      "grad_norm": 0.153777077794075,
      "learning_rate": 8.133417508417509e-06,
      "loss": 0.0006,
      "step": 6040
    },
    {
      "epoch": 7.327272727272727,
      "grad_norm": 0.15410968661308289,
      "learning_rate": 8.122895622895623e-06,
      "loss": 0.0004,
      "step": 6045
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 0.16698391735553741,
      "learning_rate": 8.112373737373738e-06,
      "loss": 0.0005,
      "step": 6050
    },
    {
      "epoch": 7.33939393939394,
      "grad_norm": 0.20770229399204254,
      "learning_rate": 8.101851851851854e-06,
      "loss": 0.0004,
      "step": 6055
    },
    {
      "epoch": 7.345454545454546,
      "grad_norm": 0.17403163015842438,
      "learning_rate": 8.091329966329966e-06,
      "loss": 0.0003,
      "step": 6060
    },
    {
      "epoch": 7.351515151515152,
      "grad_norm": 0.12270697206258774,
      "learning_rate": 8.08080808080808e-06,
      "loss": 0.0004,
      "step": 6065
    },
    {
      "epoch": 7.357575757575757,
      "grad_norm": 0.10102108120918274,
      "learning_rate": 8.070286195286195e-06,
      "loss": 0.0006,
      "step": 6070
    },
    {
      "epoch": 7.363636363636363,
      "grad_norm": 0.2206123322248459,
      "learning_rate": 8.059764309764311e-06,
      "loss": 0.0007,
      "step": 6075
    },
    {
      "epoch": 7.36969696969697,
      "grad_norm": 0.2184811383485794,
      "learning_rate": 8.049242424242426e-06,
      "loss": 0.0005,
      "step": 6080
    },
    {
      "epoch": 7.375757575757576,
      "grad_norm": 0.22208328545093536,
      "learning_rate": 8.03872053872054e-06,
      "loss": 0.0007,
      "step": 6085
    },
    {
      "epoch": 7.381818181818182,
      "grad_norm": 0.19276462495326996,
      "learning_rate": 8.028198653198653e-06,
      "loss": 0.0005,
      "step": 6090
    },
    {
      "epoch": 7.387878787878788,
      "grad_norm": 0.1718061864376068,
      "learning_rate": 8.017676767676769e-06,
      "loss": 0.0006,
      "step": 6095
    },
    {
      "epoch": 7.393939393939394,
      "grad_norm": 0.11843764781951904,
      "learning_rate": 8.007154882154883e-06,
      "loss": 0.0007,
      "step": 6100
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.14105896651744843,
      "learning_rate": 7.996632996632998e-06,
      "loss": 0.0007,
      "step": 6105
    },
    {
      "epoch": 7.406060606060606,
      "grad_norm": 0.13936781883239746,
      "learning_rate": 7.986111111111112e-06,
      "loss": 0.0007,
      "step": 6110
    },
    {
      "epoch": 7.412121212121212,
      "grad_norm": 0.14221777021884918,
      "learning_rate": 7.975589225589226e-06,
      "loss": 0.0006,
      "step": 6115
    },
    {
      "epoch": 7.418181818181818,
      "grad_norm": 0.11432850360870361,
      "learning_rate": 7.96506734006734e-06,
      "loss": 0.0006,
      "step": 6120
    },
    {
      "epoch": 7.424242424242424,
      "grad_norm": 0.13503865897655487,
      "learning_rate": 7.954545454545455e-06,
      "loss": 0.0005,
      "step": 6125
    },
    {
      "epoch": 7.430303030303031,
      "grad_norm": 0.14432072639465332,
      "learning_rate": 7.94402356902357e-06,
      "loss": 0.0004,
      "step": 6130
    },
    {
      "epoch": 7.4363636363636365,
      "grad_norm": 0.22755038738250732,
      "learning_rate": 7.933501683501684e-06,
      "loss": 0.0006,
      "step": 6135
    },
    {
      "epoch": 7.442424242424242,
      "grad_norm": 0.1658903956413269,
      "learning_rate": 7.922979797979798e-06,
      "loss": 0.0003,
      "step": 6140
    },
    {
      "epoch": 7.448484848484848,
      "grad_norm": 0.11640284210443497,
      "learning_rate": 7.912457912457913e-06,
      "loss": 0.0005,
      "step": 6145
    },
    {
      "epoch": 7.454545454545454,
      "grad_norm": 0.13287308812141418,
      "learning_rate": 7.901936026936029e-06,
      "loss": 0.0005,
      "step": 6150
    },
    {
      "epoch": 7.460606060606061,
      "grad_norm": 0.26412078738212585,
      "learning_rate": 7.891414141414141e-06,
      "loss": 0.0005,
      "step": 6155
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 0.12336397171020508,
      "learning_rate": 7.880892255892256e-06,
      "loss": 0.0005,
      "step": 6160
    },
    {
      "epoch": 7.472727272727273,
      "grad_norm": 0.18479856848716736,
      "learning_rate": 7.870370370370372e-06,
      "loss": 0.0007,
      "step": 6165
    },
    {
      "epoch": 7.4787878787878785,
      "grad_norm": 0.11893095821142197,
      "learning_rate": 7.859848484848486e-06,
      "loss": 0.0006,
      "step": 6170
    },
    {
      "epoch": 7.484848484848484,
      "grad_norm": 0.20664562284946442,
      "learning_rate": 7.8493265993266e-06,
      "loss": 0.0005,
      "step": 6175
    },
    {
      "epoch": 7.490909090909091,
      "grad_norm": 0.2222009301185608,
      "learning_rate": 7.838804713804713e-06,
      "loss": 0.0006,
      "step": 6180
    },
    {
      "epoch": 7.496969696969697,
      "grad_norm": 0.1733333021402359,
      "learning_rate": 7.82828282828283e-06,
      "loss": 0.0008,
      "step": 6185
    },
    {
      "epoch": 7.500606060606061,
      "eval_average": 0.5090604583712823,
      "eval_crossner_ai": 0.48249027232341646,
      "eval_crossner_literature": 0.47630331748550403,
      "eval_crossner_music": 0.6374100718923916,
      "eval_crossner_politics": 0.5255354200487603,
      "eval_crossner_science": 0.5627802690081737,
      "eval_mit-movie": 0.5760869564720168,
      "eval_mit-restaurant": 0.30281690136871403,
      "eval_runtime": 20.9187,
      "eval_samples_per_second": 33.463,
      "eval_steps_per_second": 0.335,
      "step": 6188
    },
    {
      "epoch": 7.503030303030303,
      "grad_norm": 0.09399339556694031,
      "learning_rate": 7.817760942760944e-06,
      "loss": 0.0005,
      "step": 6190
    },
    {
      "epoch": 7.509090909090909,
      "grad_norm": 0.2795468866825104,
      "learning_rate": 7.807239057239058e-06,
      "loss": 0.0008,
      "step": 6195
    },
    {
      "epoch": 7.515151515151516,
      "grad_norm": 0.10963388532400131,
      "learning_rate": 7.796717171717173e-06,
      "loss": 0.0005,
      "step": 6200
    },
    {
      "epoch": 7.5212121212121215,
      "grad_norm": 0.17328819632530212,
      "learning_rate": 7.786195286195287e-06,
      "loss": 0.0005,
      "step": 6205
    },
    {
      "epoch": 7.527272727272727,
      "grad_norm": 0.14340421557426453,
      "learning_rate": 7.775673400673401e-06,
      "loss": 0.0006,
      "step": 6210
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 0.1360200047492981,
      "learning_rate": 7.765151515151516e-06,
      "loss": 0.0005,
      "step": 6215
    },
    {
      "epoch": 7.539393939393939,
      "grad_norm": 0.21273848414421082,
      "learning_rate": 7.75462962962963e-06,
      "loss": 0.0007,
      "step": 6220
    },
    {
      "epoch": 7.545454545454546,
      "grad_norm": 0.5340838432312012,
      "learning_rate": 7.744107744107745e-06,
      "loss": 0.0005,
      "step": 6225
    },
    {
      "epoch": 7.551515151515152,
      "grad_norm": 0.1512988954782486,
      "learning_rate": 7.733585858585859e-06,
      "loss": 0.0007,
      "step": 6230
    },
    {
      "epoch": 7.557575757575758,
      "grad_norm": 0.13081003725528717,
      "learning_rate": 7.723063973063973e-06,
      "loss": 0.0014,
      "step": 6235
    },
    {
      "epoch": 7.5636363636363635,
      "grad_norm": 0.23060393333435059,
      "learning_rate": 7.71254208754209e-06,
      "loss": 0.0007,
      "step": 6240
    },
    {
      "epoch": 7.569696969696969,
      "grad_norm": 0.3030624985694885,
      "learning_rate": 7.702020202020202e-06,
      "loss": 0.0006,
      "step": 6245
    },
    {
      "epoch": 7.575757575757576,
      "grad_norm": 0.2119453400373459,
      "learning_rate": 7.691498316498316e-06,
      "loss": 0.0007,
      "step": 6250
    },
    {
      "epoch": 7.581818181818182,
      "grad_norm": 0.2272985577583313,
      "learning_rate": 7.680976430976431e-06,
      "loss": 0.0009,
      "step": 6255
    },
    {
      "epoch": 7.587878787878788,
      "grad_norm": 0.18395398557186127,
      "learning_rate": 7.670454545454547e-06,
      "loss": 0.0006,
      "step": 6260
    },
    {
      "epoch": 7.593939393939394,
      "grad_norm": 0.15699145197868347,
      "learning_rate": 7.659932659932661e-06,
      "loss": 0.0006,
      "step": 6265
    },
    {
      "epoch": 7.6,
      "grad_norm": 0.18417519330978394,
      "learning_rate": 7.649410774410774e-06,
      "loss": 0.0007,
      "step": 6270
    },
    {
      "epoch": 7.606060606060606,
      "grad_norm": 0.15746642649173737,
      "learning_rate": 7.638888888888888e-06,
      "loss": 0.0006,
      "step": 6275
    },
    {
      "epoch": 7.612121212121212,
      "grad_norm": 0.2045716792345047,
      "learning_rate": 7.6283670033670045e-06,
      "loss": 0.0005,
      "step": 6280
    },
    {
      "epoch": 7.618181818181818,
      "grad_norm": 0.15997225046157837,
      "learning_rate": 7.617845117845119e-06,
      "loss": 0.0005,
      "step": 6285
    },
    {
      "epoch": 7.624242424242424,
      "grad_norm": 0.09809745103120804,
      "learning_rate": 7.607323232323232e-06,
      "loss": 0.0004,
      "step": 6290
    },
    {
      "epoch": 7.63030303030303,
      "grad_norm": 0.2133062183856964,
      "learning_rate": 7.596801346801348e-06,
      "loss": 0.0006,
      "step": 6295
    },
    {
      "epoch": 7.636363636363637,
      "grad_norm": 0.13981664180755615,
      "learning_rate": 7.586279461279462e-06,
      "loss": 0.0005,
      "step": 6300
    },
    {
      "epoch": 7.642424242424243,
      "grad_norm": 0.21484817564487457,
      "learning_rate": 7.5757575757575764e-06,
      "loss": 0.0004,
      "step": 6305
    },
    {
      "epoch": 7.648484848484848,
      "grad_norm": 0.0872768834233284,
      "learning_rate": 7.565235690235691e-06,
      "loss": 0.0006,
      "step": 6310
    },
    {
      "epoch": 7.654545454545454,
      "grad_norm": 0.19337402284145355,
      "learning_rate": 7.554713804713806e-06,
      "loss": 0.0009,
      "step": 6315
    },
    {
      "epoch": 7.66060606060606,
      "grad_norm": 0.1472979336977005,
      "learning_rate": 7.54419191919192e-06,
      "loss": 0.0006,
      "step": 6320
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 0.1638840138912201,
      "learning_rate": 7.533670033670034e-06,
      "loss": 0.0006,
      "step": 6325
    },
    {
      "epoch": 7.672727272727273,
      "grad_norm": 0.16495858132839203,
      "learning_rate": 7.523148148148148e-06,
      "loss": 0.0006,
      "step": 6330
    },
    {
      "epoch": 7.678787878787879,
      "grad_norm": 0.15323325991630554,
      "learning_rate": 7.512626262626264e-06,
      "loss": 0.0005,
      "step": 6335
    },
    {
      "epoch": 7.684848484848485,
      "grad_norm": 0.1608111709356308,
      "learning_rate": 7.502104377104378e-06,
      "loss": 0.0008,
      "step": 6340
    },
    {
      "epoch": 7.6909090909090905,
      "grad_norm": 0.16276539862155914,
      "learning_rate": 7.491582491582492e-06,
      "loss": 0.0007,
      "step": 6345
    },
    {
      "epoch": 7.696969696969697,
      "grad_norm": 0.18464595079421997,
      "learning_rate": 7.481060606060606e-06,
      "loss": 0.0007,
      "step": 6350
    },
    {
      "epoch": 7.703030303030303,
      "grad_norm": 0.1896764636039734,
      "learning_rate": 7.470538720538721e-06,
      "loss": 0.0006,
      "step": 6355
    },
    {
      "epoch": 7.709090909090909,
      "grad_norm": 0.1816156804561615,
      "learning_rate": 7.4600168350168355e-06,
      "loss": 0.0006,
      "step": 6360
    },
    {
      "epoch": 7.715151515151515,
      "grad_norm": 0.10316410660743713,
      "learning_rate": 7.44949494949495e-06,
      "loss": 0.0008,
      "step": 6365
    },
    {
      "epoch": 7.721212121212122,
      "grad_norm": 0.16326037049293518,
      "learning_rate": 7.438973063973065e-06,
      "loss": 0.0004,
      "step": 6370
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 0.19753602147102356,
      "learning_rate": 7.4284511784511796e-06,
      "loss": 0.0009,
      "step": 6375
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 0.12466777116060257,
      "learning_rate": 7.417929292929293e-06,
      "loss": 0.0004,
      "step": 6380
    },
    {
      "epoch": 7.739393939393939,
      "grad_norm": 0.21408332884311676,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 0.0007,
      "step": 6385
    },
    {
      "epoch": 7.745454545454545,
      "grad_norm": 0.1353372037410736,
      "learning_rate": 7.396885521885523e-06,
      "loss": 0.0005,
      "step": 6390
    },
    {
      "epoch": 7.751515151515152,
      "grad_norm": 0.07014182955026627,
      "learning_rate": 7.386363636363637e-06,
      "loss": 0.0003,
      "step": 6395
    },
    {
      "epoch": 7.757575757575758,
      "grad_norm": 0.21509094536304474,
      "learning_rate": 7.3758417508417515e-06,
      "loss": 0.0007,
      "step": 6400
    },
    {
      "epoch": 7.763636363636364,
      "grad_norm": 0.2025228589773178,
      "learning_rate": 7.365319865319865e-06,
      "loss": 0.0007,
      "step": 6405
    },
    {
      "epoch": 7.7696969696969695,
      "grad_norm": 0.16941018402576447,
      "learning_rate": 7.354797979797981e-06,
      "loss": 0.0006,
      "step": 6410
    },
    {
      "epoch": 7.775757575757575,
      "grad_norm": 0.14085286855697632,
      "learning_rate": 7.344276094276095e-06,
      "loss": 0.0005,
      "step": 6415
    },
    {
      "epoch": 7.781818181818182,
      "grad_norm": 0.15512636303901672,
      "learning_rate": 7.333754208754209e-06,
      "loss": 0.0007,
      "step": 6420
    },
    {
      "epoch": 7.787878787878788,
      "grad_norm": 0.17273356020450592,
      "learning_rate": 7.323232323232324e-06,
      "loss": 0.0006,
      "step": 6425
    },
    {
      "epoch": 7.793939393939394,
      "grad_norm": 0.1492242068052292,
      "learning_rate": 7.312710437710439e-06,
      "loss": 0.0006,
      "step": 6430
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.17194843292236328,
      "learning_rate": 7.302188552188553e-06,
      "loss": 0.0005,
      "step": 6435
    },
    {
      "epoch": 7.806060606060606,
      "grad_norm": 0.28034546971321106,
      "learning_rate": 7.291666666666667e-06,
      "loss": 0.0008,
      "step": 6440
    },
    {
      "epoch": 7.8121212121212125,
      "grad_norm": 0.19587396085262299,
      "learning_rate": 7.281144781144782e-06,
      "loss": 0.0006,
      "step": 6445
    },
    {
      "epoch": 7.818181818181818,
      "grad_norm": 0.20335650444030762,
      "learning_rate": 7.270622895622896e-06,
      "loss": 0.0006,
      "step": 6450
    },
    {
      "epoch": 7.824242424242424,
      "grad_norm": 0.17001211643218994,
      "learning_rate": 7.260101010101011e-06,
      "loss": 0.0006,
      "step": 6455
    },
    {
      "epoch": 7.83030303030303,
      "grad_norm": 0.20177967846393585,
      "learning_rate": 7.249579124579125e-06,
      "loss": 0.0008,
      "step": 6460
    },
    {
      "epoch": 7.836363636363636,
      "grad_norm": 0.15441252291202545,
      "learning_rate": 7.23905723905724e-06,
      "loss": 0.0004,
      "step": 6465
    },
    {
      "epoch": 7.842424242424243,
      "grad_norm": 0.19464026391506195,
      "learning_rate": 7.228535353535354e-06,
      "loss": 0.0008,
      "step": 6470
    },
    {
      "epoch": 7.848484848484849,
      "grad_norm": 0.14863570034503937,
      "learning_rate": 7.218013468013468e-06,
      "loss": 0.0005,
      "step": 6475
    },
    {
      "epoch": 7.8545454545454545,
      "grad_norm": 0.1870739609003067,
      "learning_rate": 7.2074915824915825e-06,
      "loss": 0.0006,
      "step": 6480
    },
    {
      "epoch": 7.86060606060606,
      "grad_norm": 0.14042820036411285,
      "learning_rate": 7.196969696969698e-06,
      "loss": 0.0004,
      "step": 6485
    },
    {
      "epoch": 7.866666666666666,
      "grad_norm": 0.1221781075000763,
      "learning_rate": 7.186447811447812e-06,
      "loss": 0.0005,
      "step": 6490
    },
    {
      "epoch": 7.872727272727273,
      "grad_norm": 0.2309478372335434,
      "learning_rate": 7.1759259259259266e-06,
      "loss": 0.0006,
      "step": 6495
    },
    {
      "epoch": 7.878787878787879,
      "grad_norm": 0.06624850630760193,
      "learning_rate": 7.165404040404042e-06,
      "loss": 0.0004,
      "step": 6500
    },
    {
      "epoch": 7.884848484848485,
      "grad_norm": 0.10842688381671906,
      "learning_rate": 7.154882154882155e-06,
      "loss": 0.0005,
      "step": 6505
    },
    {
      "epoch": 7.890909090909091,
      "grad_norm": 0.26211413741111755,
      "learning_rate": 7.14436026936027e-06,
      "loss": 0.0006,
      "step": 6510
    },
    {
      "epoch": 7.8969696969696965,
      "grad_norm": 0.18790893256664276,
      "learning_rate": 7.133838383838384e-06,
      "loss": 0.0006,
      "step": 6515
    },
    {
      "epoch": 7.903030303030303,
      "grad_norm": 0.1458234190940857,
      "learning_rate": 7.123316498316499e-06,
      "loss": 0.0005,
      "step": 6520
    },
    {
      "epoch": 7.909090909090909,
      "grad_norm": 0.1362462341785431,
      "learning_rate": 7.112794612794614e-06,
      "loss": 0.0005,
      "step": 6525
    },
    {
      "epoch": 7.915151515151515,
      "grad_norm": 0.212783083319664,
      "learning_rate": 7.102272727272727e-06,
      "loss": 0.0006,
      "step": 6530
    },
    {
      "epoch": 7.921212121212121,
      "grad_norm": 0.1007230132818222,
      "learning_rate": 7.091750841750842e-06,
      "loss": 0.0005,
      "step": 6535
    },
    {
      "epoch": 7.927272727272728,
      "grad_norm": 0.17376327514648438,
      "learning_rate": 7.081228956228957e-06,
      "loss": 0.0005,
      "step": 6540
    },
    {
      "epoch": 7.933333333333334,
      "grad_norm": 0.1173710972070694,
      "learning_rate": 7.070707070707071e-06,
      "loss": 0.0008,
      "step": 6545
    },
    {
      "epoch": 7.9393939393939394,
      "grad_norm": 0.16391974687576294,
      "learning_rate": 7.060185185185186e-06,
      "loss": 0.0007,
      "step": 6550
    },
    {
      "epoch": 7.945454545454545,
      "grad_norm": 0.17069005966186523,
      "learning_rate": 7.049663299663301e-06,
      "loss": 0.0005,
      "step": 6555
    },
    {
      "epoch": 7.951515151515151,
      "grad_norm": 0.25127238035202026,
      "learning_rate": 7.039141414141415e-06,
      "loss": 0.0005,
      "step": 6560
    },
    {
      "epoch": 7.957575757575758,
      "grad_norm": 0.21422846615314484,
      "learning_rate": 7.028619528619529e-06,
      "loss": 0.0005,
      "step": 6565
    },
    {
      "epoch": 7.963636363636364,
      "grad_norm": 0.2584215998649597,
      "learning_rate": 7.018097643097643e-06,
      "loss": 0.0007,
      "step": 6570
    },
    {
      "epoch": 7.96969696969697,
      "grad_norm": 0.25673457980155945,
      "learning_rate": 7.0075757575757585e-06,
      "loss": 0.0007,
      "step": 6575
    },
    {
      "epoch": 7.975757575757576,
      "grad_norm": 0.16416937112808228,
      "learning_rate": 6.997053872053873e-06,
      "loss": 0.0006,
      "step": 6580
    },
    {
      "epoch": 7.9818181818181815,
      "grad_norm": 0.21316386759281158,
      "learning_rate": 6.986531986531987e-06,
      "loss": 0.0006,
      "step": 6585
    },
    {
      "epoch": 7.987878787878788,
      "grad_norm": 0.24085110425949097,
      "learning_rate": 6.976010101010101e-06,
      "loss": 0.0006,
      "step": 6590
    },
    {
      "epoch": 7.993939393939394,
      "grad_norm": 0.15062664449214935,
      "learning_rate": 6.965488215488216e-06,
      "loss": 0.0004,
      "step": 6595
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.15389549732208252,
      "learning_rate": 6.95496632996633e-06,
      "loss": 0.0007,
      "step": 6600
    },
    {
      "epoch": 8.0,
      "eval_average": 0.534195934751888,
      "eval_crossner_ai": 0.4694960211700519,
      "eval_crossner_literature": 0.5274463006659673,
      "eval_crossner_music": 0.6604046242274118,
      "eval_crossner_politics": 0.5336658353613883,
      "eval_crossner_science": 0.6210762331337213,
      "eval_mit-movie": 0.614555256014856,
      "eval_mit-restaurant": 0.31272727268981954,
      "eval_runtime": 20.8873,
      "eval_samples_per_second": 33.513,
      "eval_steps_per_second": 0.335,
      "step": 6600
    },
    {
      "epoch": 8.006060606060606,
      "grad_norm": 0.22257328033447266,
      "learning_rate": 6.944444444444445e-06,
      "loss": 0.0008,
      "step": 6605
    },
    {
      "epoch": 8.012121212121212,
      "grad_norm": 0.11428216844797134,
      "learning_rate": 6.933922558922559e-06,
      "loss": 0.0004,
      "step": 6610
    },
    {
      "epoch": 8.018181818181818,
      "grad_norm": 0.10801858454942703,
      "learning_rate": 6.923400673400674e-06,
      "loss": 0.0003,
      "step": 6615
    },
    {
      "epoch": 8.024242424242424,
      "grad_norm": 0.12091755121946335,
      "learning_rate": 6.912878787878789e-06,
      "loss": 0.0003,
      "step": 6620
    },
    {
      "epoch": 8.030303030303031,
      "grad_norm": 0.13143743574619293,
      "learning_rate": 6.902356902356902e-06,
      "loss": 0.0003,
      "step": 6625
    },
    {
      "epoch": 8.036363636363637,
      "grad_norm": 0.06205231696367264,
      "learning_rate": 6.8918350168350176e-06,
      "loss": 0.0003,
      "step": 6630
    },
    {
      "epoch": 8.042424242424243,
      "grad_norm": 0.07818088680505753,
      "learning_rate": 6.881313131313132e-06,
      "loss": 0.0003,
      "step": 6635
    },
    {
      "epoch": 8.048484848484849,
      "grad_norm": 0.14704222977161407,
      "learning_rate": 6.870791245791246e-06,
      "loss": 0.0004,
      "step": 6640
    },
    {
      "epoch": 8.054545454545455,
      "grad_norm": 0.17122820019721985,
      "learning_rate": 6.860269360269361e-06,
      "loss": 0.0003,
      "step": 6645
    },
    {
      "epoch": 8.06060606060606,
      "grad_norm": 0.09247761219739914,
      "learning_rate": 6.849747474747476e-06,
      "loss": 0.0007,
      "step": 6650
    },
    {
      "epoch": 8.066666666666666,
      "grad_norm": 0.11903630942106247,
      "learning_rate": 6.8392255892255895e-06,
      "loss": 0.0003,
      "step": 6655
    },
    {
      "epoch": 8.072727272727272,
      "grad_norm": 0.18384550511837006,
      "learning_rate": 6.828703703703704e-06,
      "loss": 0.0003,
      "step": 6660
    },
    {
      "epoch": 8.078787878787878,
      "grad_norm": 0.16683603823184967,
      "learning_rate": 6.818181818181818e-06,
      "loss": 0.0002,
      "step": 6665
    },
    {
      "epoch": 8.084848484848484,
      "grad_norm": 0.0771830603480339,
      "learning_rate": 6.8076599326599335e-06,
      "loss": 0.0003,
      "step": 6670
    },
    {
      "epoch": 8.090909090909092,
      "grad_norm": 0.1031530424952507,
      "learning_rate": 6.797138047138048e-06,
      "loss": 0.0004,
      "step": 6675
    },
    {
      "epoch": 8.096969696969698,
      "grad_norm": 0.13896526396274567,
      "learning_rate": 6.786616161616162e-06,
      "loss": 0.0004,
      "step": 6680
    },
    {
      "epoch": 8.103030303030303,
      "grad_norm": 0.23711849749088287,
      "learning_rate": 6.7760942760942775e-06,
      "loss": 0.0003,
      "step": 6685
    },
    {
      "epoch": 8.10909090909091,
      "grad_norm": 0.10939135402441025,
      "learning_rate": 6.765572390572391e-06,
      "loss": 0.0003,
      "step": 6690
    },
    {
      "epoch": 8.115151515151515,
      "grad_norm": 0.09199942648410797,
      "learning_rate": 6.7550505050505055e-06,
      "loss": 0.0004,
      "step": 6695
    },
    {
      "epoch": 8.121212121212121,
      "grad_norm": 0.10935069620609283,
      "learning_rate": 6.74452861952862e-06,
      "loss": 0.0003,
      "step": 6700
    },
    {
      "epoch": 8.127272727272727,
      "grad_norm": 0.13160300254821777,
      "learning_rate": 6.734006734006735e-06,
      "loss": 0.0003,
      "step": 6705
    },
    {
      "epoch": 8.133333333333333,
      "grad_norm": 0.11907986551523209,
      "learning_rate": 6.7234848484848495e-06,
      "loss": 0.0004,
      "step": 6710
    },
    {
      "epoch": 8.139393939393939,
      "grad_norm": 0.13204090297222137,
      "learning_rate": 6.712962962962963e-06,
      "loss": 0.0005,
      "step": 6715
    },
    {
      "epoch": 8.145454545454545,
      "grad_norm": 0.2552894949913025,
      "learning_rate": 6.702441077441077e-06,
      "loss": 0.0004,
      "step": 6720
    },
    {
      "epoch": 8.151515151515152,
      "grad_norm": 0.14876964688301086,
      "learning_rate": 6.691919191919193e-06,
      "loss": 0.0002,
      "step": 6725
    },
    {
      "epoch": 8.157575757575758,
      "grad_norm": 0.20269900560379028,
      "learning_rate": 6.681397306397307e-06,
      "loss": 0.0004,
      "step": 6730
    },
    {
      "epoch": 8.163636363636364,
      "grad_norm": 0.10314740985631943,
      "learning_rate": 6.670875420875421e-06,
      "loss": 0.0004,
      "step": 6735
    },
    {
      "epoch": 8.16969696969697,
      "grad_norm": 0.25084519386291504,
      "learning_rate": 6.660353535353535e-06,
      "loss": 0.0003,
      "step": 6740
    },
    {
      "epoch": 8.175757575757576,
      "grad_norm": 0.12058673799037933,
      "learning_rate": 6.649831649831651e-06,
      "loss": 0.0004,
      "step": 6745
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 0.2532674968242645,
      "learning_rate": 6.6393097643097646e-06,
      "loss": 0.0004,
      "step": 6750
    },
    {
      "epoch": 8.187878787878788,
      "grad_norm": 0.08963508158922195,
      "learning_rate": 6.628787878787879e-06,
      "loss": 0.0003,
      "step": 6755
    },
    {
      "epoch": 8.193939393939393,
      "grad_norm": 0.06881972402334213,
      "learning_rate": 6.618265993265994e-06,
      "loss": 0.0003,
      "step": 6760
    },
    {
      "epoch": 8.2,
      "grad_norm": 0.08904388546943665,
      "learning_rate": 6.6077441077441086e-06,
      "loss": 0.0002,
      "step": 6765
    },
    {
      "epoch": 8.206060606060607,
      "grad_norm": 0.21041308343410492,
      "learning_rate": 6.597222222222223e-06,
      "loss": 0.0003,
      "step": 6770
    },
    {
      "epoch": 8.212121212121213,
      "grad_norm": 0.1841079443693161,
      "learning_rate": 6.5867003367003365e-06,
      "loss": 0.0002,
      "step": 6775
    },
    {
      "epoch": 8.218181818181819,
      "grad_norm": 0.06122398003935814,
      "learning_rate": 6.576178451178452e-06,
      "loss": 0.0003,
      "step": 6780
    },
    {
      "epoch": 8.224242424242425,
      "grad_norm": 0.1619061678647995,
      "learning_rate": 6.565656565656566e-06,
      "loss": 0.0003,
      "step": 6785
    },
    {
      "epoch": 8.23030303030303,
      "grad_norm": 0.09580513089895248,
      "learning_rate": 6.5551346801346805e-06,
      "loss": 0.0003,
      "step": 6790
    },
    {
      "epoch": 8.236363636363636,
      "grad_norm": 0.1059342473745346,
      "learning_rate": 6.544612794612795e-06,
      "loss": 0.0006,
      "step": 6795
    },
    {
      "epoch": 8.242424242424242,
      "grad_norm": 0.17904029786586761,
      "learning_rate": 6.53409090909091e-06,
      "loss": 0.0004,
      "step": 6800
    },
    {
      "epoch": 8.248484848484848,
      "grad_norm": 0.164033442735672,
      "learning_rate": 6.5235690235690245e-06,
      "loss": 0.0004,
      "step": 6805
    },
    {
      "epoch": 8.254545454545454,
      "grad_norm": 0.10464397072792053,
      "learning_rate": 6.513047138047138e-06,
      "loss": 0.0004,
      "step": 6810
    },
    {
      "epoch": 8.26060606060606,
      "grad_norm": 0.13250185549259186,
      "learning_rate": 6.502525252525253e-06,
      "loss": 0.0003,
      "step": 6815
    },
    {
      "epoch": 8.266666666666667,
      "grad_norm": 0.09031572192907333,
      "learning_rate": 6.492003367003368e-06,
      "loss": 0.0002,
      "step": 6820
    },
    {
      "epoch": 8.272727272727273,
      "grad_norm": 0.19277822971343994,
      "learning_rate": 6.481481481481482e-06,
      "loss": 0.0003,
      "step": 6825
    },
    {
      "epoch": 8.27878787878788,
      "grad_norm": 0.11470779031515121,
      "learning_rate": 6.4709595959595965e-06,
      "loss": 0.0002,
      "step": 6830
    },
    {
      "epoch": 8.284848484848485,
      "grad_norm": 0.09194348007440567,
      "learning_rate": 6.460437710437712e-06,
      "loss": 0.0006,
      "step": 6835
    },
    {
      "epoch": 8.290909090909091,
      "grad_norm": 0.17152579128742218,
      "learning_rate": 6.449915824915825e-06,
      "loss": 0.0004,
      "step": 6840
    },
    {
      "epoch": 8.296969696969697,
      "grad_norm": 0.2963064908981323,
      "learning_rate": 6.43939393939394e-06,
      "loss": 0.0002,
      "step": 6845
    },
    {
      "epoch": 8.303030303030303,
      "grad_norm": 0.24367894232273102,
      "learning_rate": 6.428872053872054e-06,
      "loss": 0.0002,
      "step": 6850
    },
    {
      "epoch": 8.309090909090909,
      "grad_norm": 0.20579098165035248,
      "learning_rate": 6.418350168350169e-06,
      "loss": 0.0002,
      "step": 6855
    },
    {
      "epoch": 8.315151515151515,
      "grad_norm": 0.1946471929550171,
      "learning_rate": 6.407828282828284e-06,
      "loss": 0.0004,
      "step": 6860
    },
    {
      "epoch": 8.32121212121212,
      "grad_norm": 0.10405553132295609,
      "learning_rate": 6.397306397306397e-06,
      "loss": 0.0005,
      "step": 6865
    },
    {
      "epoch": 8.327272727272728,
      "grad_norm": 0.1370517462491989,
      "learning_rate": 6.3867845117845116e-06,
      "loss": 0.0005,
      "step": 6870
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 0.07665690779685974,
      "learning_rate": 6.376262626262627e-06,
      "loss": 0.0002,
      "step": 6875
    },
    {
      "epoch": 8.33939393939394,
      "grad_norm": 0.15475501120090485,
      "learning_rate": 6.365740740740741e-06,
      "loss": 0.0004,
      "step": 6880
    },
    {
      "epoch": 8.345454545454546,
      "grad_norm": 0.10606169700622559,
      "learning_rate": 6.3552188552188556e-06,
      "loss": 0.0003,
      "step": 6885
    },
    {
      "epoch": 8.351515151515152,
      "grad_norm": 0.13463076949119568,
      "learning_rate": 6.344696969696971e-06,
      "loss": 0.0003,
      "step": 6890
    },
    {
      "epoch": 8.357575757575757,
      "grad_norm": 0.1336059421300888,
      "learning_rate": 6.334175084175085e-06,
      "loss": 0.0004,
      "step": 6895
    },
    {
      "epoch": 8.363636363636363,
      "grad_norm": 0.19149033725261688,
      "learning_rate": 6.323653198653199e-06,
      "loss": 0.0003,
      "step": 6900
    },
    {
      "epoch": 8.36969696969697,
      "grad_norm": 0.15826688706874847,
      "learning_rate": 6.313131313131313e-06,
      "loss": 0.0003,
      "step": 6905
    },
    {
      "epoch": 8.375757575757575,
      "grad_norm": 0.03769626468420029,
      "learning_rate": 6.302609427609428e-06,
      "loss": 0.0003,
      "step": 6910
    },
    {
      "epoch": 8.381818181818181,
      "grad_norm": 0.06622005999088287,
      "learning_rate": 6.292087542087543e-06,
      "loss": 0.0002,
      "step": 6915
    },
    {
      "epoch": 8.387878787878789,
      "grad_norm": 0.11548507958650589,
      "learning_rate": 6.281565656565657e-06,
      "loss": 0.0002,
      "step": 6920
    },
    {
      "epoch": 8.393939393939394,
      "grad_norm": 0.16334038972854614,
      "learning_rate": 6.271043771043771e-06,
      "loss": 0.0003,
      "step": 6925
    },
    {
      "epoch": 8.4,
      "grad_norm": 0.0774477943778038,
      "learning_rate": 6.260521885521886e-06,
      "loss": 0.0003,
      "step": 6930
    },
    {
      "epoch": 8.406060606060606,
      "grad_norm": 0.08460751175880432,
      "learning_rate": 6.25e-06,
      "loss": 0.0002,
      "step": 6935
    },
    {
      "epoch": 8.412121212121212,
      "grad_norm": 0.15243197977542877,
      "learning_rate": 6.239478114478115e-06,
      "loss": 0.0005,
      "step": 6940
    },
    {
      "epoch": 8.418181818181818,
      "grad_norm": 0.11044280230998993,
      "learning_rate": 6.22895622895623e-06,
      "loss": 0.0003,
      "step": 6945
    },
    {
      "epoch": 8.424242424242424,
      "grad_norm": 0.11916393786668777,
      "learning_rate": 6.218434343434344e-06,
      "loss": 0.0003,
      "step": 6950
    },
    {
      "epoch": 8.43030303030303,
      "grad_norm": 0.12806645035743713,
      "learning_rate": 6.207912457912459e-06,
      "loss": 0.0003,
      "step": 6955
    },
    {
      "epoch": 8.436363636363636,
      "grad_norm": 0.14076651632785797,
      "learning_rate": 6.197390572390572e-06,
      "loss": 0.0002,
      "step": 6960
    },
    {
      "epoch": 8.442424242424243,
      "grad_norm": 0.14946863055229187,
      "learning_rate": 6.1868686868686875e-06,
      "loss": 0.0002,
      "step": 6965
    },
    {
      "epoch": 8.44848484848485,
      "grad_norm": 0.2723134756088257,
      "learning_rate": 6.176346801346802e-06,
      "loss": 0.0002,
      "step": 6970
    },
    {
      "epoch": 8.454545454545455,
      "grad_norm": 0.274990975856781,
      "learning_rate": 6.165824915824916e-06,
      "loss": 0.0005,
      "step": 6975
    },
    {
      "epoch": 8.460606060606061,
      "grad_norm": 0.1059371680021286,
      "learning_rate": 6.155303030303031e-06,
      "loss": 0.0003,
      "step": 6980
    },
    {
      "epoch": 8.466666666666667,
      "grad_norm": 0.06810469180345535,
      "learning_rate": 6.144781144781146e-06,
      "loss": 0.0003,
      "step": 6985
    },
    {
      "epoch": 8.472727272727273,
      "grad_norm": 0.04820111766457558,
      "learning_rate": 6.134259259259259e-06,
      "loss": 0.0004,
      "step": 6990
    },
    {
      "epoch": 8.478787878787879,
      "grad_norm": 0.10676807165145874,
      "learning_rate": 6.123737373737374e-06,
      "loss": 0.0003,
      "step": 6995
    },
    {
      "epoch": 8.484848484848484,
      "grad_norm": 0.16998305916786194,
      "learning_rate": 6.113215488215489e-06,
      "loss": 0.0005,
      "step": 7000
    },
    {
      "epoch": 8.49090909090909,
      "grad_norm": 0.1344037652015686,
      "learning_rate": 6.102693602693603e-06,
      "loss": 0.0003,
      "step": 7005
    },
    {
      "epoch": 8.496969696969696,
      "grad_norm": 0.13866788148880005,
      "learning_rate": 6.092171717171718e-06,
      "loss": 0.0003,
      "step": 7010
    },
    {
      "epoch": 8.49939393939394,
      "eval_average": 0.5443245356415176,
      "eval_crossner_ai": 0.49536423836047966,
      "eval_crossner_literature": 0.5456702253354749,
      "eval_crossner_music": 0.6633020907934952,
      "eval_crossner_politics": 0.5778145694863304,
      "eval_crossner_science": 0.5991091313529849,
      "eval_mit-movie": 0.618666666616724,
      "eval_mit-restaurant": 0.31034482754513437,
      "eval_runtime": 20.8565,
      "eval_samples_per_second": 33.563,
      "eval_steps_per_second": 0.336,
      "step": 7012
    },
    {
      "epoch": 8.503030303030304,
      "grad_norm": 0.07905418425798416,
      "learning_rate": 6.081649831649832e-06,
      "loss": 0.0003,
      "step": 7015
    },
    {
      "epoch": 8.50909090909091,
      "grad_norm": 0.16856804490089417,
      "learning_rate": 6.0711279461279474e-06,
      "loss": 0.0005,
      "step": 7020
    },
    {
      "epoch": 8.515151515151516,
      "grad_norm": 0.08218809217214584,
      "learning_rate": 6.060606060606061e-06,
      "loss": 0.0004,
      "step": 7025
    },
    {
      "epoch": 8.521212121212121,
      "grad_norm": 0.3883891701698303,
      "learning_rate": 6.050084175084175e-06,
      "loss": 0.0004,
      "step": 7030
    },
    {
      "epoch": 8.527272727272727,
      "grad_norm": 0.16065216064453125,
      "learning_rate": 6.03956228956229e-06,
      "loss": 0.0002,
      "step": 7035
    },
    {
      "epoch": 8.533333333333333,
      "grad_norm": 0.1389220952987671,
      "learning_rate": 6.029040404040405e-06,
      "loss": 0.0003,
      "step": 7040
    },
    {
      "epoch": 8.539393939393939,
      "grad_norm": 0.170142263174057,
      "learning_rate": 6.018518518518519e-06,
      "loss": 0.0003,
      "step": 7045
    },
    {
      "epoch": 8.545454545454545,
      "grad_norm": 0.13101539015769958,
      "learning_rate": 6.007996632996633e-06,
      "loss": 0.0003,
      "step": 7050
    },
    {
      "epoch": 8.55151515151515,
      "grad_norm": 0.027442386373877525,
      "learning_rate": 5.997474747474747e-06,
      "loss": 0.0003,
      "step": 7055
    },
    {
      "epoch": 8.557575757575757,
      "grad_norm": 0.08459489792585373,
      "learning_rate": 5.9869528619528625e-06,
      "loss": 0.0003,
      "step": 7060
    },
    {
      "epoch": 8.563636363636364,
      "grad_norm": 0.1063946858048439,
      "learning_rate": 5.976430976430977e-06,
      "loss": 0.0002,
      "step": 7065
    },
    {
      "epoch": 8.56969696969697,
      "grad_norm": 0.10787282884120941,
      "learning_rate": 5.965909090909091e-06,
      "loss": 0.0002,
      "step": 7070
    },
    {
      "epoch": 8.575757575757576,
      "grad_norm": 0.13531340658664703,
      "learning_rate": 5.9553872053872065e-06,
      "loss": 0.0003,
      "step": 7075
    },
    {
      "epoch": 8.581818181818182,
      "grad_norm": 0.19711701571941376,
      "learning_rate": 5.944865319865321e-06,
      "loss": 0.0003,
      "step": 7080
    },
    {
      "epoch": 8.587878787878788,
      "grad_norm": 0.10733102262020111,
      "learning_rate": 5.9343434343434345e-06,
      "loss": 0.0003,
      "step": 7085
    },
    {
      "epoch": 8.593939393939394,
      "grad_norm": 0.036436863243579865,
      "learning_rate": 5.923821548821549e-06,
      "loss": 0.0003,
      "step": 7090
    },
    {
      "epoch": 8.6,
      "grad_norm": 0.0987357571721077,
      "learning_rate": 5.913299663299664e-06,
      "loss": 0.0004,
      "step": 7095
    },
    {
      "epoch": 8.606060606060606,
      "grad_norm": 0.1070714145898819,
      "learning_rate": 5.9027777777777785e-06,
      "loss": 0.0003,
      "step": 7100
    },
    {
      "epoch": 8.612121212121211,
      "grad_norm": 0.1733187884092331,
      "learning_rate": 5.892255892255893e-06,
      "loss": 0.0004,
      "step": 7105
    },
    {
      "epoch": 8.618181818181819,
      "grad_norm": 0.24609102308750153,
      "learning_rate": 5.881734006734006e-06,
      "loss": 0.0002,
      "step": 7110
    },
    {
      "epoch": 8.624242424242425,
      "grad_norm": 0.16578246653079987,
      "learning_rate": 5.871212121212122e-06,
      "loss": 0.0002,
      "step": 7115
    },
    {
      "epoch": 8.63030303030303,
      "grad_norm": 0.124982088804245,
      "learning_rate": 5.860690235690236e-06,
      "loss": 0.0003,
      "step": 7120
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 0.23417727649211884,
      "learning_rate": 5.85016835016835e-06,
      "loss": 0.0003,
      "step": 7125
    },
    {
      "epoch": 8.642424242424243,
      "grad_norm": 0.11704603582620621,
      "learning_rate": 5.839646464646466e-06,
      "loss": 0.0003,
      "step": 7130
    },
    {
      "epoch": 8.648484848484848,
      "grad_norm": 0.17778140306472778,
      "learning_rate": 5.82912457912458e-06,
      "loss": 0.0004,
      "step": 7135
    },
    {
      "epoch": 8.654545454545454,
      "grad_norm": 0.12361817061901093,
      "learning_rate": 5.8186026936026944e-06,
      "loss": 0.0004,
      "step": 7140
    },
    {
      "epoch": 8.66060606060606,
      "grad_norm": 0.14843782782554626,
      "learning_rate": 5.808080808080808e-06,
      "loss": 0.0004,
      "step": 7145
    },
    {
      "epoch": 8.666666666666666,
      "grad_norm": 0.19869965314865112,
      "learning_rate": 5.797558922558923e-06,
      "loss": 0.0003,
      "step": 7150
    },
    {
      "epoch": 8.672727272727272,
      "grad_norm": 0.1461622714996338,
      "learning_rate": 5.787037037037038e-06,
      "loss": 0.0003,
      "step": 7155
    },
    {
      "epoch": 8.67878787878788,
      "grad_norm": 0.06598558276891708,
      "learning_rate": 5.776515151515152e-06,
      "loss": 0.0002,
      "step": 7160
    },
    {
      "epoch": 8.684848484848485,
      "grad_norm": 0.12997844815254211,
      "learning_rate": 5.765993265993266e-06,
      "loss": 0.0003,
      "step": 7165
    },
    {
      "epoch": 8.690909090909091,
      "grad_norm": 0.05064772069454193,
      "learning_rate": 5.755471380471382e-06,
      "loss": 0.0003,
      "step": 7170
    },
    {
      "epoch": 8.696969696969697,
      "grad_norm": 0.1589817851781845,
      "learning_rate": 5.744949494949495e-06,
      "loss": 0.0003,
      "step": 7175
    },
    {
      "epoch": 8.703030303030303,
      "grad_norm": 0.11815505474805832,
      "learning_rate": 5.7344276094276095e-06,
      "loss": 0.0004,
      "step": 7180
    },
    {
      "epoch": 8.709090909090909,
      "grad_norm": 0.08612667769193649,
      "learning_rate": 5.723905723905724e-06,
      "loss": 0.0004,
      "step": 7185
    },
    {
      "epoch": 8.715151515151515,
      "grad_norm": 0.1723886877298355,
      "learning_rate": 5.713383838383839e-06,
      "loss": 0.0004,
      "step": 7190
    },
    {
      "epoch": 8.72121212121212,
      "grad_norm": 0.135548397898674,
      "learning_rate": 5.7028619528619535e-06,
      "loss": 0.0002,
      "step": 7195
    },
    {
      "epoch": 8.727272727272727,
      "grad_norm": 0.1394544541835785,
      "learning_rate": 5.692340067340067e-06,
      "loss": 0.0003,
      "step": 7200
    },
    {
      "epoch": 8.733333333333333,
      "grad_norm": 0.0862080305814743,
      "learning_rate": 5.681818181818183e-06,
      "loss": 0.0003,
      "step": 7205
    },
    {
      "epoch": 8.73939393939394,
      "grad_norm": 0.1386113166809082,
      "learning_rate": 5.671296296296297e-06,
      "loss": 0.0005,
      "step": 7210
    },
    {
      "epoch": 8.745454545454546,
      "grad_norm": 0.10686055570840836,
      "learning_rate": 5.660774410774411e-06,
      "loss": 0.0005,
      "step": 7215
    },
    {
      "epoch": 8.751515151515152,
      "grad_norm": 0.09378552436828613,
      "learning_rate": 5.6502525252525255e-06,
      "loss": 0.0003,
      "step": 7220
    },
    {
      "epoch": 8.757575757575758,
      "grad_norm": 0.11712136119604111,
      "learning_rate": 5.639730639730641e-06,
      "loss": 0.0005,
      "step": 7225
    },
    {
      "epoch": 8.763636363636364,
      "grad_norm": 0.0321890264749527,
      "learning_rate": 5.629208754208755e-06,
      "loss": 0.0003,
      "step": 7230
    },
    {
      "epoch": 8.76969696969697,
      "grad_norm": 0.15507426857948303,
      "learning_rate": 5.618686868686869e-06,
      "loss": 0.0003,
      "step": 7235
    },
    {
      "epoch": 8.775757575757575,
      "grad_norm": 0.16232916712760925,
      "learning_rate": 5.608164983164983e-06,
      "loss": 0.0004,
      "step": 7240
    },
    {
      "epoch": 8.781818181818181,
      "grad_norm": 0.13404494524002075,
      "learning_rate": 5.597643097643098e-06,
      "loss": 0.0002,
      "step": 7245
    },
    {
      "epoch": 8.787878787878787,
      "grad_norm": 0.18672409653663635,
      "learning_rate": 5.587121212121213e-06,
      "loss": 0.0003,
      "step": 7250
    },
    {
      "epoch": 8.793939393939393,
      "grad_norm": 0.15725518763065338,
      "learning_rate": 5.576599326599327e-06,
      "loss": 0.0005,
      "step": 7255
    },
    {
      "epoch": 8.8,
      "grad_norm": 0.08384182304143906,
      "learning_rate": 5.566077441077442e-06,
      "loss": 0.0004,
      "step": 7260
    },
    {
      "epoch": 8.806060606060607,
      "grad_norm": 0.08406341820955276,
      "learning_rate": 5.555555555555557e-06,
      "loss": 0.0003,
      "step": 7265
    },
    {
      "epoch": 8.812121212121212,
      "grad_norm": 0.18762892484664917,
      "learning_rate": 5.54503367003367e-06,
      "loss": 0.0005,
      "step": 7270
    },
    {
      "epoch": 8.818181818181818,
      "grad_norm": 0.15533162653446198,
      "learning_rate": 5.534511784511785e-06,
      "loss": 0.0004,
      "step": 7275
    },
    {
      "epoch": 8.824242424242424,
      "grad_norm": 0.13781291246414185,
      "learning_rate": 5.5239898989899e-06,
      "loss": 0.0005,
      "step": 7280
    },
    {
      "epoch": 8.83030303030303,
      "grad_norm": 0.1454274207353592,
      "learning_rate": 5.513468013468014e-06,
      "loss": 0.0003,
      "step": 7285
    },
    {
      "epoch": 8.836363636363636,
      "grad_norm": 0.17033544182777405,
      "learning_rate": 5.502946127946129e-06,
      "loss": 0.0004,
      "step": 7290
    },
    {
      "epoch": 8.842424242424242,
      "grad_norm": 0.16094061732292175,
      "learning_rate": 5.492424242424242e-06,
      "loss": 0.0004,
      "step": 7295
    },
    {
      "epoch": 8.848484848484848,
      "grad_norm": 0.10045812278985977,
      "learning_rate": 5.481902356902357e-06,
      "loss": 0.0005,
      "step": 7300
    },
    {
      "epoch": 8.854545454545455,
      "grad_norm": 0.10664930194616318,
      "learning_rate": 5.471380471380472e-06,
      "loss": 0.0004,
      "step": 7305
    },
    {
      "epoch": 8.860606060606061,
      "grad_norm": 0.17144295573234558,
      "learning_rate": 5.460858585858586e-06,
      "loss": 0.0003,
      "step": 7310
    },
    {
      "epoch": 8.866666666666667,
      "grad_norm": 0.18983012437820435,
      "learning_rate": 5.4503367003367005e-06,
      "loss": 0.0005,
      "step": 7315
    },
    {
      "epoch": 8.872727272727273,
      "grad_norm": 0.024421920999884605,
      "learning_rate": 5.439814814814816e-06,
      "loss": 0.0002,
      "step": 7320
    },
    {
      "epoch": 8.878787878787879,
      "grad_norm": 0.2039838433265686,
      "learning_rate": 5.429292929292929e-06,
      "loss": 0.0004,
      "step": 7325
    },
    {
      "epoch": 8.884848484848485,
      "grad_norm": 0.06331736594438553,
      "learning_rate": 5.418771043771044e-06,
      "loss": 0.0003,
      "step": 7330
    },
    {
      "epoch": 8.89090909090909,
      "grad_norm": 0.134869784116745,
      "learning_rate": 5.408249158249159e-06,
      "loss": 0.0004,
      "step": 7335
    },
    {
      "epoch": 8.896969696969697,
      "grad_norm": 0.10409750044345856,
      "learning_rate": 5.397727272727273e-06,
      "loss": 0.0003,
      "step": 7340
    },
    {
      "epoch": 8.903030303030302,
      "grad_norm": 0.0998767614364624,
      "learning_rate": 5.387205387205388e-06,
      "loss": 0.0003,
      "step": 7345
    },
    {
      "epoch": 8.909090909090908,
      "grad_norm": 0.11225839704275131,
      "learning_rate": 5.376683501683502e-06,
      "loss": 0.0003,
      "step": 7350
    },
    {
      "epoch": 8.915151515151516,
      "grad_norm": 0.09058273583650589,
      "learning_rate": 5.366161616161617e-06,
      "loss": 0.0005,
      "step": 7355
    },
    {
      "epoch": 8.921212121212122,
      "grad_norm": 0.06406579166650772,
      "learning_rate": 5.355639730639731e-06,
      "loss": 0.0003,
      "step": 7360
    },
    {
      "epoch": 8.927272727272728,
      "grad_norm": 0.12492358684539795,
      "learning_rate": 5.345117845117845e-06,
      "loss": 0.0003,
      "step": 7365
    },
    {
      "epoch": 8.933333333333334,
      "grad_norm": 0.10496778041124344,
      "learning_rate": 5.33459595959596e-06,
      "loss": 0.0002,
      "step": 7370
    },
    {
      "epoch": 8.93939393939394,
      "grad_norm": 0.04899771511554718,
      "learning_rate": 5.324074074074075e-06,
      "loss": 0.0004,
      "step": 7375
    },
    {
      "epoch": 8.945454545454545,
      "grad_norm": 0.10972559452056885,
      "learning_rate": 5.313552188552189e-06,
      "loss": 0.0003,
      "step": 7380
    },
    {
      "epoch": 8.951515151515151,
      "grad_norm": 0.17286527156829834,
      "learning_rate": 5.303030303030303e-06,
      "loss": 0.0002,
      "step": 7385
    },
    {
      "epoch": 8.957575757575757,
      "grad_norm": 0.15333592891693115,
      "learning_rate": 5.292508417508419e-06,
      "loss": 0.0003,
      "step": 7390
    },
    {
      "epoch": 8.963636363636363,
      "grad_norm": 0.12611907720565796,
      "learning_rate": 5.2819865319865324e-06,
      "loss": 0.0003,
      "step": 7395
    },
    {
      "epoch": 8.969696969696969,
      "grad_norm": 0.08832836896181107,
      "learning_rate": 5.271464646464647e-06,
      "loss": 0.0003,
      "step": 7400
    },
    {
      "epoch": 8.975757575757576,
      "grad_norm": 0.09759381413459778,
      "learning_rate": 5.260942760942761e-06,
      "loss": 0.0003,
      "step": 7405
    },
    {
      "epoch": 8.981818181818182,
      "grad_norm": 0.07427719980478287,
      "learning_rate": 5.2504208754208765e-06,
      "loss": 0.0003,
      "step": 7410
    },
    {
      "epoch": 8.987878787878788,
      "grad_norm": 0.2336071878671646,
      "learning_rate": 5.239898989898991e-06,
      "loss": 0.0005,
      "step": 7415
    },
    {
      "epoch": 8.993939393939394,
      "grad_norm": 0.16124527156352997,
      "learning_rate": 5.229377104377104e-06,
      "loss": 0.0005,
      "step": 7420
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.1386621743440628,
      "learning_rate": 5.218855218855219e-06,
      "loss": 0.0003,
      "step": 7425
    },
    {
      "epoch": 9.0,
      "eval_average": 0.5510785777284901,
      "eval_crossner_ai": 0.5058670142914589,
      "eval_crossner_literature": 0.5642857142356718,
      "eval_crossner_music": 0.6709770114441899,
      "eval_crossner_politics": 0.561174551336535,
      "eval_crossner_science": 0.5870535713784414,
      "eval_mit-movie": 0.6212534059448358,
      "eval_mit-restaurant": 0.346938775468298,
      "eval_runtime": 20.8321,
      "eval_samples_per_second": 33.602,
      "eval_steps_per_second": 0.336,
      "step": 7425
    },
    {
      "epoch": 9.006060606060606,
      "grad_norm": 0.17123711109161377,
      "learning_rate": 5.208333333333334e-06,
      "loss": 0.0003,
      "step": 7430
    },
    {
      "epoch": 9.012121212121212,
      "grad_norm": 0.3185924291610718,
      "learning_rate": 5.199915824915825e-06,
      "loss": 0.0004,
      "step": 7435
    },
    {
      "epoch": 9.018181818181818,
      "grad_norm": 0.08810850232839584,
      "learning_rate": 5.18939393939394e-06,
      "loss": 0.0001,
      "step": 7440
    },
    {
      "epoch": 9.024242424242424,
      "grad_norm": 0.14506353437900543,
      "learning_rate": 5.178872053872054e-06,
      "loss": 0.0002,
      "step": 7445
    },
    {
      "epoch": 9.030303030303031,
      "grad_norm": 0.061368998140096664,
      "learning_rate": 5.168350168350169e-06,
      "loss": 0.0001,
      "step": 7450
    },
    {
      "epoch": 9.036363636363637,
      "grad_norm": 0.09084125608205795,
      "learning_rate": 5.157828282828283e-06,
      "loss": 0.0001,
      "step": 7455
    },
    {
      "epoch": 9.042424242424243,
      "grad_norm": 0.12690569460391998,
      "learning_rate": 5.147306397306397e-06,
      "loss": 0.0001,
      "step": 7460
    },
    {
      "epoch": 9.048484848484849,
      "grad_norm": 0.04065140709280968,
      "learning_rate": 5.136784511784512e-06,
      "loss": 0.0002,
      "step": 7465
    },
    {
      "epoch": 9.054545454545455,
      "grad_norm": 0.05664749816060066,
      "learning_rate": 5.126262626262627e-06,
      "loss": 0.0001,
      "step": 7470
    },
    {
      "epoch": 9.06060606060606,
      "grad_norm": 0.019416483119130135,
      "learning_rate": 5.115740740740741e-06,
      "loss": 0.0001,
      "step": 7475
    },
    {
      "epoch": 9.066666666666666,
      "grad_norm": 0.04923650622367859,
      "learning_rate": 5.105218855218856e-06,
      "loss": 0.0001,
      "step": 7480
    },
    {
      "epoch": 9.072727272727272,
      "grad_norm": 0.07802384346723557,
      "learning_rate": 5.094696969696971e-06,
      "loss": 0.0002,
      "step": 7485
    },
    {
      "epoch": 9.078787878787878,
      "grad_norm": 0.0252243522554636,
      "learning_rate": 5.0841750841750845e-06,
      "loss": 0.0001,
      "step": 7490
    },
    {
      "epoch": 9.084848484848484,
      "grad_norm": 0.11956502497196198,
      "learning_rate": 5.073653198653199e-06,
      "loss": 0.0002,
      "step": 7495
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 0.038497794419527054,
      "learning_rate": 5.063131313131313e-06,
      "loss": 0.0001,
      "step": 7500
    },
    {
      "epoch": 9.096969696969698,
      "grad_norm": 0.015179131180047989,
      "learning_rate": 5.0526094276094285e-06,
      "loss": 0.0002,
      "step": 7505
    },
    {
      "epoch": 9.103030303030303,
      "grad_norm": 0.10161971300840378,
      "learning_rate": 5.042087542087543e-06,
      "loss": 0.0001,
      "step": 7510
    },
    {
      "epoch": 9.10909090909091,
      "grad_norm": 0.22277164459228516,
      "learning_rate": 5.031565656565656e-06,
      "loss": 0.0002,
      "step": 7515
    },
    {
      "epoch": 9.115151515151515,
      "grad_norm": 0.055921271443367004,
      "learning_rate": 5.021043771043771e-06,
      "loss": 0.0001,
      "step": 7520
    },
    {
      "epoch": 9.121212121212121,
      "grad_norm": 0.17457324266433716,
      "learning_rate": 5.010521885521886e-06,
      "loss": 0.0002,
      "step": 7525
    },
    {
      "epoch": 9.127272727272727,
      "grad_norm": 0.02254490926861763,
      "learning_rate": 5e-06,
      "loss": 0.0002,
      "step": 7530
    },
    {
      "epoch": 9.133333333333333,
      "grad_norm": 0.062362927943468094,
      "learning_rate": 4.989478114478115e-06,
      "loss": 0.0002,
      "step": 7535
    },
    {
      "epoch": 9.139393939393939,
      "grad_norm": 0.08008982241153717,
      "learning_rate": 4.978956228956229e-06,
      "loss": 0.0002,
      "step": 7540
    },
    {
      "epoch": 9.145454545454545,
      "grad_norm": 0.12930846214294434,
      "learning_rate": 4.968434343434344e-06,
      "loss": 0.0002,
      "step": 7545
    },
    {
      "epoch": 9.151515151515152,
      "grad_norm": 0.06976909190416336,
      "learning_rate": 4.957912457912458e-06,
      "loss": 0.0002,
      "step": 7550
    },
    {
      "epoch": 9.157575757575758,
      "grad_norm": 0.0163851547986269,
      "learning_rate": 4.947390572390573e-06,
      "loss": 0.0002,
      "step": 7555
    },
    {
      "epoch": 9.163636363636364,
      "grad_norm": 0.15436764061450958,
      "learning_rate": 4.936868686868687e-06,
      "loss": 0.0001,
      "step": 7560
    },
    {
      "epoch": 9.16969696969697,
      "grad_norm": 0.0783495083451271,
      "learning_rate": 4.926346801346802e-06,
      "loss": 0.0001,
      "step": 7565
    },
    {
      "epoch": 9.175757575757576,
      "grad_norm": 0.08642543107271194,
      "learning_rate": 4.915824915824916e-06,
      "loss": 0.0002,
      "step": 7570
    },
    {
      "epoch": 9.181818181818182,
      "grad_norm": 0.15086662769317627,
      "learning_rate": 4.905303030303031e-06,
      "loss": 0.0002,
      "step": 7575
    },
    {
      "epoch": 9.187878787878788,
      "grad_norm": 0.013582263141870499,
      "learning_rate": 4.894781144781145e-06,
      "loss": 0.0001,
      "step": 7580
    },
    {
      "epoch": 9.193939393939393,
      "grad_norm": 0.043783336877822876,
      "learning_rate": 4.8842592592592595e-06,
      "loss": 0.0001,
      "step": 7585
    },
    {
      "epoch": 9.2,
      "grad_norm": 0.014183850027620792,
      "learning_rate": 4.873737373737374e-06,
      "loss": 0.0002,
      "step": 7590
    },
    {
      "epoch": 9.206060606060607,
      "grad_norm": 0.09285595268011093,
      "learning_rate": 4.863215488215488e-06,
      "loss": 0.0001,
      "step": 7595
    },
    {
      "epoch": 9.212121212121213,
      "grad_norm": 0.06316018849611282,
      "learning_rate": 4.8526936026936035e-06,
      "loss": 0.0001,
      "step": 7600
    },
    {
      "epoch": 9.218181818181819,
      "grad_norm": 0.03932424262166023,
      "learning_rate": 4.842171717171718e-06,
      "loss": 0.0001,
      "step": 7605
    },
    {
      "epoch": 9.224242424242425,
      "grad_norm": 0.01637052744626999,
      "learning_rate": 4.831649831649832e-06,
      "loss": 0.0002,
      "step": 7610
    },
    {
      "epoch": 9.23030303030303,
      "grad_norm": 0.12328492850065231,
      "learning_rate": 4.821127946127947e-06,
      "loss": 0.0002,
      "step": 7615
    },
    {
      "epoch": 9.236363636363636,
      "grad_norm": 0.11950080096721649,
      "learning_rate": 4.810606060606061e-06,
      "loss": 0.0001,
      "step": 7620
    },
    {
      "epoch": 9.242424242424242,
      "grad_norm": 0.14639532566070557,
      "learning_rate": 4.8000841750841755e-06,
      "loss": 0.0002,
      "step": 7625
    },
    {
      "epoch": 9.248484848484848,
      "grad_norm": 0.08095093071460724,
      "learning_rate": 4.78956228956229e-06,
      "loss": 0.0001,
      "step": 7630
    },
    {
      "epoch": 9.254545454545454,
      "grad_norm": 0.13038115203380585,
      "learning_rate": 4.779040404040404e-06,
      "loss": 0.0001,
      "step": 7635
    },
    {
      "epoch": 9.26060606060606,
      "grad_norm": 0.029899539425969124,
      "learning_rate": 4.768518518518519e-06,
      "loss": 0.0001,
      "step": 7640
    },
    {
      "epoch": 9.266666666666667,
      "grad_norm": 0.05407172814011574,
      "learning_rate": 4.757996632996633e-06,
      "loss": 0.0001,
      "step": 7645
    },
    {
      "epoch": 9.272727272727273,
      "grad_norm": 0.07225222885608673,
      "learning_rate": 4.747474747474748e-06,
      "loss": 0.0001,
      "step": 7650
    },
    {
      "epoch": 9.27878787878788,
      "grad_norm": 0.06313680112361908,
      "learning_rate": 4.736952861952862e-06,
      "loss": 0.0001,
      "step": 7655
    },
    {
      "epoch": 9.284848484848485,
      "grad_norm": 0.021223578602075577,
      "learning_rate": 4.726430976430977e-06,
      "loss": 0.0001,
      "step": 7660
    },
    {
      "epoch": 9.290909090909091,
      "grad_norm": 0.1632373183965683,
      "learning_rate": 4.715909090909091e-06,
      "loss": 0.0002,
      "step": 7665
    },
    {
      "epoch": 9.296969696969697,
      "grad_norm": 0.059082139283418655,
      "learning_rate": 4.705387205387206e-06,
      "loss": 0.0002,
      "step": 7670
    },
    {
      "epoch": 9.303030303030303,
      "grad_norm": 0.06094447523355484,
      "learning_rate": 4.69486531986532e-06,
      "loss": 0.0001,
      "step": 7675
    },
    {
      "epoch": 9.309090909090909,
      "grad_norm": 0.07367128133773804,
      "learning_rate": 4.684343434343435e-06,
      "loss": 0.0001,
      "step": 7680
    },
    {
      "epoch": 9.315151515151515,
      "grad_norm": 0.08547689020633698,
      "learning_rate": 4.673821548821549e-06,
      "loss": 0.0003,
      "step": 7685
    },
    {
      "epoch": 9.32121212121212,
      "grad_norm": 0.14697714149951935,
      "learning_rate": 4.663299663299663e-06,
      "loss": 0.0002,
      "step": 7690
    },
    {
      "epoch": 9.327272727272728,
      "grad_norm": 0.19763053953647614,
      "learning_rate": 4.652777777777779e-06,
      "loss": 0.0003,
      "step": 7695
    },
    {
      "epoch": 9.333333333333334,
      "grad_norm": 0.04401685670018196,
      "learning_rate": 4.642255892255892e-06,
      "loss": 0.0002,
      "step": 7700
    },
    {
      "epoch": 9.33939393939394,
      "grad_norm": 0.09377484768629074,
      "learning_rate": 4.631734006734007e-06,
      "loss": 0.0002,
      "step": 7705
    },
    {
      "epoch": 9.345454545454546,
      "grad_norm": 0.019626999273896217,
      "learning_rate": 4.621212121212122e-06,
      "loss": 0.0001,
      "step": 7710
    },
    {
      "epoch": 9.351515151515152,
      "grad_norm": 0.08712878823280334,
      "learning_rate": 4.610690235690236e-06,
      "loss": 0.0001,
      "step": 7715
    },
    {
      "epoch": 9.357575757575757,
      "grad_norm": 0.18966761231422424,
      "learning_rate": 4.6001683501683505e-06,
      "loss": 0.0003,
      "step": 7720
    },
    {
      "epoch": 9.363636363636363,
      "grad_norm": 0.1299673467874527,
      "learning_rate": 4.589646464646465e-06,
      "loss": 0.0002,
      "step": 7725
    },
    {
      "epoch": 9.36969696969697,
      "grad_norm": 0.043742138892412186,
      "learning_rate": 4.57912457912458e-06,
      "loss": 0.0001,
      "step": 7730
    },
    {
      "epoch": 9.375757575757575,
      "grad_norm": 0.09859153628349304,
      "learning_rate": 4.568602693602694e-06,
      "loss": 0.0001,
      "step": 7735
    },
    {
      "epoch": 9.381818181818181,
      "grad_norm": 0.058554232120513916,
      "learning_rate": 4.558080808080809e-06,
      "loss": 0.0002,
      "step": 7740
    },
    {
      "epoch": 9.387878787878789,
      "grad_norm": 0.10187634825706482,
      "learning_rate": 4.5475589225589225e-06,
      "loss": 0.0001,
      "step": 7745
    },
    {
      "epoch": 9.393939393939394,
      "grad_norm": 0.08548035472631454,
      "learning_rate": 4.537037037037038e-06,
      "loss": 0.0003,
      "step": 7750
    },
    {
      "epoch": 9.4,
      "grad_norm": 0.11767134815454483,
      "learning_rate": 4.526515151515152e-06,
      "loss": 0.0003,
      "step": 7755
    },
    {
      "epoch": 9.406060606060606,
      "grad_norm": 0.10098803788423538,
      "learning_rate": 4.5159932659932665e-06,
      "loss": 0.0001,
      "step": 7760
    },
    {
      "epoch": 9.412121212121212,
      "grad_norm": 0.018764683976769447,
      "learning_rate": 4.505471380471381e-06,
      "loss": 0.0001,
      "step": 7765
    },
    {
      "epoch": 9.418181818181818,
      "grad_norm": 0.06450450420379639,
      "learning_rate": 4.494949494949495e-06,
      "loss": 0.0001,
      "step": 7770
    },
    {
      "epoch": 9.424242424242424,
      "grad_norm": 0.08420657366514206,
      "learning_rate": 4.48442760942761e-06,
      "loss": 0.0001,
      "step": 7775
    },
    {
      "epoch": 9.43030303030303,
      "grad_norm": 0.07298596948385239,
      "learning_rate": 4.473905723905724e-06,
      "loss": 0.0002,
      "step": 7780
    },
    {
      "epoch": 9.436363636363636,
      "grad_norm": 0.0330691784620285,
      "learning_rate": 4.463383838383838e-06,
      "loss": 0.0001,
      "step": 7785
    },
    {
      "epoch": 9.442424242424243,
      "grad_norm": 0.04625887796282768,
      "learning_rate": 4.452861952861953e-06,
      "loss": 0.0001,
      "step": 7790
    },
    {
      "epoch": 9.44848484848485,
      "grad_norm": 0.1128968745470047,
      "learning_rate": 4.442340067340068e-06,
      "loss": 0.0001,
      "step": 7795
    },
    {
      "epoch": 9.454545454545455,
      "grad_norm": 0.1611141413450241,
      "learning_rate": 4.4318181818181824e-06,
      "loss": 0.0001,
      "step": 7800
    },
    {
      "epoch": 9.460606060606061,
      "grad_norm": 0.04264967888593674,
      "learning_rate": 4.421296296296297e-06,
      "loss": 0.0001,
      "step": 7805
    },
    {
      "epoch": 9.466666666666667,
      "grad_norm": 0.20090468227863312,
      "learning_rate": 4.410774410774411e-06,
      "loss": 0.0002,
      "step": 7810
    },
    {
      "epoch": 9.472727272727273,
      "grad_norm": 0.1014363244175911,
      "learning_rate": 4.400252525252526e-06,
      "loss": 0.0001,
      "step": 7815
    },
    {
      "epoch": 9.478787878787879,
      "grad_norm": 0.08346741646528244,
      "learning_rate": 4.38973063973064e-06,
      "loss": 0.0001,
      "step": 7820
    },
    {
      "epoch": 9.484848484848484,
      "grad_norm": 0.026013648137450218,
      "learning_rate": 4.379208754208754e-06,
      "loss": 0.0001,
      "step": 7825
    },
    {
      "epoch": 9.49090909090909,
      "grad_norm": 0.11418472230434418,
      "learning_rate": 4.368686868686869e-06,
      "loss": 0.0001,
      "step": 7830
    },
    {
      "epoch": 9.496969696969696,
      "grad_norm": 0.06767604500055313,
      "learning_rate": 4.358164983164984e-06,
      "loss": 0.0002,
      "step": 7835
    },
    {
      "epoch": 9.50060606060606,
      "eval_average": 0.5405051669719053,
      "eval_crossner_ai": 0.5261744965942017,
      "eval_crossner_literature": 0.5456702253354749,
      "eval_crossner_music": 0.6619013580628645,
      "eval_crossner_politics": 0.5342126957454604,
      "eval_crossner_science": 0.5846501128166978,
      "eval_mit-movie": 0.592797783884037,
      "eval_mit-restaurant": 0.3381294963646007,
      "eval_runtime": 20.6519,
      "eval_samples_per_second": 33.895,
      "eval_steps_per_second": 0.339,
      "step": 7838
    },
    {
      "epoch": 9.503030303030304,
      "grad_norm": 0.09493155032396317,
      "learning_rate": 4.3476430976430975e-06,
      "loss": 0.0001,
      "step": 7840
    },
    {
      "epoch": 9.50909090909091,
      "grad_norm": 0.05259111151099205,
      "learning_rate": 4.337121212121213e-06,
      "loss": 0.0002,
      "step": 7845
    },
    {
      "epoch": 9.515151515151516,
      "grad_norm": 0.11560942977666855,
      "learning_rate": 4.326599326599326e-06,
      "loss": 0.0002,
      "step": 7850
    },
    {
      "epoch": 9.521212121212121,
      "grad_norm": 0.08267739415168762,
      "learning_rate": 4.3160774410774415e-06,
      "loss": 0.0003,
      "step": 7855
    },
    {
      "epoch": 9.527272727272727,
      "grad_norm": 0.017724372446537018,
      "learning_rate": 4.305555555555556e-06,
      "loss": 0.0001,
      "step": 7860
    },
    {
      "epoch": 9.533333333333333,
      "grad_norm": 0.11691699922084808,
      "learning_rate": 4.29503367003367e-06,
      "loss": 0.0002,
      "step": 7865
    },
    {
      "epoch": 9.539393939393939,
      "grad_norm": 0.05814595893025398,
      "learning_rate": 4.284511784511785e-06,
      "loss": 0.0001,
      "step": 7870
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 0.09084168821573257,
      "learning_rate": 4.273989898989899e-06,
      "loss": 0.0001,
      "step": 7875
    },
    {
      "epoch": 9.55151515151515,
      "grad_norm": 0.08822810649871826,
      "learning_rate": 4.263468013468014e-06,
      "loss": 0.0003,
      "step": 7880
    },
    {
      "epoch": 9.557575757575757,
      "grad_norm": 0.05598524585366249,
      "learning_rate": 4.252946127946128e-06,
      "loss": 0.0,
      "step": 7885
    },
    {
      "epoch": 9.563636363636364,
      "grad_norm": 0.03452775627374649,
      "learning_rate": 4.242424242424243e-06,
      "loss": 0.0001,
      "step": 7890
    },
    {
      "epoch": 9.56969696969697,
      "grad_norm": 0.027981257066130638,
      "learning_rate": 4.231902356902357e-06,
      "loss": 0.0001,
      "step": 7895
    },
    {
      "epoch": 9.575757575757576,
      "grad_norm": 0.017058780416846275,
      "learning_rate": 4.221380471380472e-06,
      "loss": 0.0002,
      "step": 7900
    },
    {
      "epoch": 9.581818181818182,
      "grad_norm": 0.0272076316177845,
      "learning_rate": 4.210858585858586e-06,
      "loss": 0.0002,
      "step": 7905
    },
    {
      "epoch": 9.587878787878788,
      "grad_norm": 0.1295546591281891,
      "learning_rate": 4.200336700336701e-06,
      "loss": 0.0003,
      "step": 7910
    },
    {
      "epoch": 9.593939393939394,
      "grad_norm": 0.08877643942832947,
      "learning_rate": 4.189814814814815e-06,
      "loss": 0.0002,
      "step": 7915
    },
    {
      "epoch": 9.6,
      "grad_norm": 0.07770728319883347,
      "learning_rate": 4.1792929292929294e-06,
      "loss": 0.0001,
      "step": 7920
    },
    {
      "epoch": 9.606060606060606,
      "grad_norm": 0.10260043293237686,
      "learning_rate": 4.168771043771045e-06,
      "loss": 0.0002,
      "step": 7925
    },
    {
      "epoch": 9.612121212121211,
      "grad_norm": 0.016464127227663994,
      "learning_rate": 4.158249158249158e-06,
      "loss": 0.0003,
      "step": 7930
    },
    {
      "epoch": 9.618181818181819,
      "grad_norm": 0.1399184763431549,
      "learning_rate": 4.1477272727272734e-06,
      "loss": 0.0001,
      "step": 7935
    },
    {
      "epoch": 9.624242424242425,
      "grad_norm": 0.03729972243309021,
      "learning_rate": 4.137205387205388e-06,
      "loss": 0.0001,
      "step": 7940
    },
    {
      "epoch": 9.63030303030303,
      "grad_norm": 0.10031517595052719,
      "learning_rate": 4.126683501683502e-06,
      "loss": 0.0001,
      "step": 7945
    },
    {
      "epoch": 9.636363636363637,
      "grad_norm": 0.07509736716747284,
      "learning_rate": 4.116161616161617e-06,
      "loss": 0.0001,
      "step": 7950
    },
    {
      "epoch": 9.642424242424243,
      "grad_norm": 0.09537330269813538,
      "learning_rate": 4.105639730639731e-06,
      "loss": 0.0001,
      "step": 7955
    },
    {
      "epoch": 9.648484848484848,
      "grad_norm": 0.15670689940452576,
      "learning_rate": 4.095117845117845e-06,
      "loss": 0.0002,
      "step": 7960
    },
    {
      "epoch": 9.654545454545454,
      "grad_norm": 0.018649358302354813,
      "learning_rate": 4.08459595959596e-06,
      "loss": 0.0003,
      "step": 7965
    },
    {
      "epoch": 9.66060606060606,
      "grad_norm": 0.030506432056427002,
      "learning_rate": 4.074074074074074e-06,
      "loss": 0.0002,
      "step": 7970
    },
    {
      "epoch": 9.666666666666666,
      "grad_norm": 0.09719706326723099,
      "learning_rate": 4.0635521885521885e-06,
      "loss": 0.0002,
      "step": 7975
    },
    {
      "epoch": 9.672727272727272,
      "grad_norm": 0.01904623955488205,
      "learning_rate": 4.053030303030303e-06,
      "loss": 0.0002,
      "step": 7980
    },
    {
      "epoch": 9.67878787878788,
      "grad_norm": 0.027624208480119705,
      "learning_rate": 4.042508417508418e-06,
      "loss": 0.0001,
      "step": 7985
    },
    {
      "epoch": 9.684848484848485,
      "grad_norm": 0.07336344569921494,
      "learning_rate": 4.0319865319865326e-06,
      "loss": 0.0001,
      "step": 7990
    },
    {
      "epoch": 9.690909090909091,
      "grad_norm": 0.16165289282798767,
      "learning_rate": 4.021464646464647e-06,
      "loss": 0.0002,
      "step": 7995
    },
    {
      "epoch": 9.696969696969697,
      "grad_norm": 0.11631940305233002,
      "learning_rate": 4.010942760942761e-06,
      "loss": 0.0001,
      "step": 8000
    },
    {
      "epoch": 9.703030303030303,
      "grad_norm": 0.1287468820810318,
      "learning_rate": 4.000420875420876e-06,
      "loss": 0.0001,
      "step": 8005
    },
    {
      "epoch": 9.709090909090909,
      "grad_norm": 0.0573677122592926,
      "learning_rate": 3.98989898989899e-06,
      "loss": 0.0001,
      "step": 8010
    },
    {
      "epoch": 9.715151515151515,
      "grad_norm": 0.11221684515476227,
      "learning_rate": 3.9793771043771045e-06,
      "loss": 0.0001,
      "step": 8015
    },
    {
      "epoch": 9.72121212121212,
      "grad_norm": 0.0458851233124733,
      "learning_rate": 3.968855218855219e-06,
      "loss": 0.0002,
      "step": 8020
    },
    {
      "epoch": 9.727272727272727,
      "grad_norm": 0.16521289944648743,
      "learning_rate": 3.958333333333333e-06,
      "loss": 0.0001,
      "step": 8025
    },
    {
      "epoch": 9.733333333333333,
      "grad_norm": 0.09516256302595139,
      "learning_rate": 3.9478114478114485e-06,
      "loss": 0.0001,
      "step": 8030
    },
    {
      "epoch": 9.73939393939394,
      "grad_norm": 0.016009746119379997,
      "learning_rate": 3.937289562289562e-06,
      "loss": 0.0001,
      "step": 8035
    },
    {
      "epoch": 9.745454545454546,
      "grad_norm": 0.5818037390708923,
      "learning_rate": 3.926767676767677e-06,
      "loss": 0.0012,
      "step": 8040
    },
    {
      "epoch": 9.751515151515152,
      "grad_norm": 0.019375236704945564,
      "learning_rate": 3.916245791245792e-06,
      "loss": 0.0001,
      "step": 8045
    },
    {
      "epoch": 9.757575757575758,
      "grad_norm": 0.024952784180641174,
      "learning_rate": 3.905723905723906e-06,
      "loss": 0.0002,
      "step": 8050
    },
    {
      "epoch": 9.763636363636364,
      "grad_norm": 0.13823580741882324,
      "learning_rate": 3.8952020202020204e-06,
      "loss": 0.0001,
      "step": 8055
    },
    {
      "epoch": 9.76969696969697,
      "grad_norm": 0.21114031970500946,
      "learning_rate": 3.884680134680135e-06,
      "loss": 0.0001,
      "step": 8060
    },
    {
      "epoch": 9.775757575757575,
      "grad_norm": 0.060075610876083374,
      "learning_rate": 3.87415824915825e-06,
      "loss": 0.0001,
      "step": 8065
    },
    {
      "epoch": 9.781818181818181,
      "grad_norm": 0.03370434790849686,
      "learning_rate": 3.863636363636364e-06,
      "loss": 0.0001,
      "step": 8070
    },
    {
      "epoch": 9.787878787878787,
      "grad_norm": 0.3313922882080078,
      "learning_rate": 3.853114478114479e-06,
      "loss": 0.0003,
      "step": 8075
    },
    {
      "epoch": 9.793939393939393,
      "grad_norm": 0.14459694921970367,
      "learning_rate": 3.842592592592592e-06,
      "loss": 0.0001,
      "step": 8080
    },
    {
      "epoch": 9.8,
      "grad_norm": 0.05610821396112442,
      "learning_rate": 3.832070707070708e-06,
      "loss": 0.0002,
      "step": 8085
    },
    {
      "epoch": 9.806060606060607,
      "grad_norm": 0.09393851459026337,
      "learning_rate": 3.821548821548822e-06,
      "loss": 0.0002,
      "step": 8090
    },
    {
      "epoch": 9.812121212121212,
      "grad_norm": 0.12572816014289856,
      "learning_rate": 3.8110269360269364e-06,
      "loss": 0.0002,
      "step": 8095
    },
    {
      "epoch": 9.818181818181818,
      "grad_norm": 0.09365347772836685,
      "learning_rate": 3.8005050505050508e-06,
      "loss": 0.0001,
      "step": 8100
    },
    {
      "epoch": 9.824242424242424,
      "grad_norm": 0.02078179270029068,
      "learning_rate": 3.789983164983165e-06,
      "loss": 0.0001,
      "step": 8105
    },
    {
      "epoch": 9.83030303030303,
      "grad_norm": 0.07970400899648666,
      "learning_rate": 3.7794612794612796e-06,
      "loss": 0.0001,
      "step": 8110
    },
    {
      "epoch": 9.836363636363636,
      "grad_norm": 0.10332315415143967,
      "learning_rate": 3.7689393939393944e-06,
      "loss": 0.0002,
      "step": 8115
    },
    {
      "epoch": 9.842424242424242,
      "grad_norm": 0.1604095697402954,
      "learning_rate": 3.7584175084175088e-06,
      "loss": 0.0002,
      "step": 8120
    },
    {
      "epoch": 9.848484848484848,
      "grad_norm": 0.17979486286640167,
      "learning_rate": 3.747895622895623e-06,
      "loss": 0.0001,
      "step": 8125
    },
    {
      "epoch": 9.854545454545455,
      "grad_norm": 0.03764649108052254,
      "learning_rate": 3.737373737373738e-06,
      "loss": 0.0001,
      "step": 8130
    },
    {
      "epoch": 9.860606060606061,
      "grad_norm": 0.1143823117017746,
      "learning_rate": 3.726851851851852e-06,
      "loss": 0.0001,
      "step": 8135
    },
    {
      "epoch": 9.866666666666667,
      "grad_norm": 0.06404323875904083,
      "learning_rate": 3.7163299663299667e-06,
      "loss": 0.0001,
      "step": 8140
    },
    {
      "epoch": 9.872727272727273,
      "grad_norm": 0.01569780521094799,
      "learning_rate": 3.705808080808081e-06,
      "loss": 0.0001,
      "step": 8145
    },
    {
      "epoch": 9.878787878787879,
      "grad_norm": 0.06299266219139099,
      "learning_rate": 3.6952861952861955e-06,
      "loss": 0.0002,
      "step": 8150
    },
    {
      "epoch": 9.884848484848485,
      "grad_norm": 0.07634847611188889,
      "learning_rate": 3.68476430976431e-06,
      "loss": 0.0003,
      "step": 8155
    },
    {
      "epoch": 9.89090909090909,
      "grad_norm": 0.06681358069181442,
      "learning_rate": 3.6742424242424247e-06,
      "loss": 0.0004,
      "step": 8160
    },
    {
      "epoch": 9.896969696969697,
      "grad_norm": 0.03212598338723183,
      "learning_rate": 3.6637205387205387e-06,
      "loss": 0.0003,
      "step": 8165
    },
    {
      "epoch": 9.903030303030302,
      "grad_norm": 0.0851227268576622,
      "learning_rate": 3.6531986531986535e-06,
      "loss": 0.0002,
      "step": 8170
    },
    {
      "epoch": 9.909090909090908,
      "grad_norm": 0.03526966646313667,
      "learning_rate": 3.642676767676768e-06,
      "loss": 0.0001,
      "step": 8175
    },
    {
      "epoch": 9.915151515151516,
      "grad_norm": 0.030001481994986534,
      "learning_rate": 3.6321548821548823e-06,
      "loss": 0.0001,
      "step": 8180
    },
    {
      "epoch": 9.921212121212122,
      "grad_norm": 0.06843395531177521,
      "learning_rate": 3.621632996632997e-06,
      "loss": 0.0001,
      "step": 8185
    },
    {
      "epoch": 9.927272727272728,
      "grad_norm": 0.08003208041191101,
      "learning_rate": 3.6111111111111115e-06,
      "loss": 0.0002,
      "step": 8190
    },
    {
      "epoch": 9.933333333333334,
      "grad_norm": 0.10491539537906647,
      "learning_rate": 3.6005892255892263e-06,
      "loss": 0.0002,
      "step": 8195
    },
    {
      "epoch": 9.93939393939394,
      "grad_norm": 0.1491946130990982,
      "learning_rate": 3.5900673400673402e-06,
      "loss": 0.0001,
      "step": 8200
    },
    {
      "epoch": 9.945454545454545,
      "grad_norm": 0.1216403916478157,
      "learning_rate": 3.579545454545455e-06,
      "loss": 0.0002,
      "step": 8205
    },
    {
      "epoch": 9.951515151515151,
      "grad_norm": 0.02591775543987751,
      "learning_rate": 3.569023569023569e-06,
      "loss": 0.0002,
      "step": 8210
    },
    {
      "epoch": 9.957575757575757,
      "grad_norm": 0.27324068546295166,
      "learning_rate": 3.558501683501684e-06,
      "loss": 0.0004,
      "step": 8215
    },
    {
      "epoch": 9.963636363636363,
      "grad_norm": 0.15504981577396393,
      "learning_rate": 3.547979797979798e-06,
      "loss": 0.0002,
      "step": 8220
    },
    {
      "epoch": 9.969696969696969,
      "grad_norm": 0.17608128488063812,
      "learning_rate": 3.537457912457913e-06,
      "loss": 0.0001,
      "step": 8225
    },
    {
      "epoch": 9.975757575757576,
      "grad_norm": 0.07761193066835403,
      "learning_rate": 3.526936026936027e-06,
      "loss": 0.0001,
      "step": 8230
    },
    {
      "epoch": 9.981818181818182,
      "grad_norm": 0.11189279705286026,
      "learning_rate": 3.516414141414142e-06,
      "loss": 0.0002,
      "step": 8235
    },
    {
      "epoch": 9.987878787878788,
      "grad_norm": 0.04344876483082771,
      "learning_rate": 3.5058922558922558e-06,
      "loss": 0.0001,
      "step": 8240
    },
    {
      "epoch": 9.993939393939394,
      "grad_norm": 0.04233266040682793,
      "learning_rate": 3.4953703703703706e-06,
      "loss": 0.0001,
      "step": 8245
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.03644050285220146,
      "learning_rate": 3.4848484848484854e-06,
      "loss": 0.0004,
      "step": 8250
    },
    {
      "epoch": 10.0,
      "eval_average": 0.5449484368261078,
      "eval_crossner_ai": 0.5142118862547796,
      "eval_crossner_literature": 0.5120192307192475,
      "eval_crossner_music": 0.6753623187905423,
      "eval_crossner_politics": 0.5579470198174594,
      "eval_crossner_science": 0.5923423422922179,
      "eval_mit-movie": 0.6236559139286277,
      "eval_mit-restaurant": 0.3391003459798805,
      "eval_runtime": 20.6761,
      "eval_samples_per_second": 33.856,
      "eval_steps_per_second": 0.339,
      "step": 8250
    },
    {
      "epoch": 10.006060606060606,
      "grad_norm": 0.07665740698575974,
      "learning_rate": 3.4743265993265993e-06,
      "loss": 0.0001,
      "step": 8255
    },
    {
      "epoch": 10.012121212121212,
      "grad_norm": 0.08985578268766403,
      "learning_rate": 3.463804713804714e-06,
      "loss": 0.0002,
      "step": 8260
    },
    {
      "epoch": 10.018181818181818,
      "grad_norm": 0.10849375277757645,
      "learning_rate": 3.4532828282828285e-06,
      "loss": 0.0,
      "step": 8265
    },
    {
      "epoch": 10.024242424242424,
      "grad_norm": 0.01667393185198307,
      "learning_rate": 3.4427609427609434e-06,
      "loss": 0.0001,
      "step": 8270
    },
    {
      "epoch": 10.030303030303031,
      "grad_norm": 0.018039245158433914,
      "learning_rate": 3.4322390572390573e-06,
      "loss": 0.0001,
      "step": 8275
    },
    {
      "epoch": 10.036363636363637,
      "grad_norm": 0.06243037059903145,
      "learning_rate": 3.421717171717172e-06,
      "loss": 0.0001,
      "step": 8280
    },
    {
      "epoch": 10.042424242424243,
      "grad_norm": 0.10541979968547821,
      "learning_rate": 3.411195286195286e-06,
      "loss": 0.0,
      "step": 8285
    },
    {
      "epoch": 10.048484848484849,
      "grad_norm": 0.17075307667255402,
      "learning_rate": 3.400673400673401e-06,
      "loss": 0.0002,
      "step": 8290
    },
    {
      "epoch": 10.054545454545455,
      "grad_norm": 0.047559671103954315,
      "learning_rate": 3.3901515151515153e-06,
      "loss": 0.0,
      "step": 8295
    },
    {
      "epoch": 10.06060606060606,
      "grad_norm": 0.07439291477203369,
      "learning_rate": 3.37962962962963e-06,
      "loss": 0.0001,
      "step": 8300
    },
    {
      "epoch": 10.066666666666666,
      "grad_norm": 0.016534941270947456,
      "learning_rate": 3.3691077441077445e-06,
      "loss": 0.0,
      "step": 8305
    },
    {
      "epoch": 10.072727272727272,
      "grad_norm": 0.0997205302119255,
      "learning_rate": 3.358585858585859e-06,
      "loss": 0.0001,
      "step": 8310
    },
    {
      "epoch": 10.078787878787878,
      "grad_norm": 0.04175121709704399,
      "learning_rate": 3.3480639730639737e-06,
      "loss": 0.0,
      "step": 8315
    },
    {
      "epoch": 10.084848484848484,
      "grad_norm": 0.028404684737324715,
      "learning_rate": 3.3375420875420877e-06,
      "loss": 0.0001,
      "step": 8320
    },
    {
      "epoch": 10.090909090909092,
      "grad_norm": 0.054506104439496994,
      "learning_rate": 3.3270202020202025e-06,
      "loss": 0.0001,
      "step": 8325
    },
    {
      "epoch": 10.096969696969698,
      "grad_norm": 0.041834648698568344,
      "learning_rate": 3.316498316498317e-06,
      "loss": 0.0,
      "step": 8330
    },
    {
      "epoch": 10.103030303030303,
      "grad_norm": 0.0064432332292199135,
      "learning_rate": 3.3059764309764312e-06,
      "loss": 0.0002,
      "step": 8335
    },
    {
      "epoch": 10.10909090909091,
      "grad_norm": 0.06987804919481277,
      "learning_rate": 3.2954545454545456e-06,
      "loss": 0.0001,
      "step": 8340
    },
    {
      "epoch": 10.115151515151515,
      "grad_norm": 0.025888526812195778,
      "learning_rate": 3.2849326599326604e-06,
      "loss": 0.0001,
      "step": 8345
    },
    {
      "epoch": 10.121212121212121,
      "grad_norm": 0.04733333736658096,
      "learning_rate": 3.2744107744107744e-06,
      "loss": 0.0002,
      "step": 8350
    },
    {
      "epoch": 10.127272727272727,
      "grad_norm": 0.11199288070201874,
      "learning_rate": 3.2638888888888892e-06,
      "loss": 0.0001,
      "step": 8355
    },
    {
      "epoch": 10.133333333333333,
      "grad_norm": 0.029408684000372887,
      "learning_rate": 3.2533670033670036e-06,
      "loss": 0.0,
      "step": 8360
    },
    {
      "epoch": 10.139393939393939,
      "grad_norm": 0.029087519273161888,
      "learning_rate": 3.242845117845118e-06,
      "loss": 0.0002,
      "step": 8365
    },
    {
      "epoch": 10.145454545454545,
      "grad_norm": 0.085931695997715,
      "learning_rate": 3.232323232323233e-06,
      "loss": 0.0001,
      "step": 8370
    },
    {
      "epoch": 10.151515151515152,
      "grad_norm": 0.01356127392500639,
      "learning_rate": 3.221801346801347e-06,
      "loss": 0.0,
      "step": 8375
    },
    {
      "epoch": 10.157575757575758,
      "grad_norm": 0.011274058371782303,
      "learning_rate": 3.2112794612794616e-06,
      "loss": 0.0,
      "step": 8380
    },
    {
      "epoch": 10.163636363636364,
      "grad_norm": 0.11589008569717407,
      "learning_rate": 3.200757575757576e-06,
      "loss": 0.0002,
      "step": 8385
    },
    {
      "epoch": 10.16969696969697,
      "grad_norm": 0.03884688392281532,
      "learning_rate": 3.1902356902356908e-06,
      "loss": 0.0,
      "step": 8390
    },
    {
      "epoch": 10.175757575757576,
      "grad_norm": 0.040463414043188095,
      "learning_rate": 3.1797138047138047e-06,
      "loss": 0.0002,
      "step": 8395
    },
    {
      "epoch": 10.181818181818182,
      "grad_norm": 0.04114925116300583,
      "learning_rate": 3.1691919191919196e-06,
      "loss": 0.0,
      "step": 8400
    },
    {
      "epoch": 10.187878787878788,
      "grad_norm": 0.048291727900505066,
      "learning_rate": 3.158670033670034e-06,
      "loss": 0.0001,
      "step": 8405
    },
    {
      "epoch": 10.193939393939393,
      "grad_norm": 0.15816907584667206,
      "learning_rate": 3.1481481481481483e-06,
      "loss": 0.0,
      "step": 8410
    },
    {
      "epoch": 10.2,
      "grad_norm": 0.03263384476304054,
      "learning_rate": 3.1376262626262627e-06,
      "loss": 0.0001,
      "step": 8415
    },
    {
      "epoch": 10.206060606060607,
      "grad_norm": 0.055385153740644455,
      "learning_rate": 3.1271043771043775e-06,
      "loss": 0.0,
      "step": 8420
    },
    {
      "epoch": 10.212121212121213,
      "grad_norm": 0.07436476647853851,
      "learning_rate": 3.1165824915824915e-06,
      "loss": 0.0001,
      "step": 8425
    },
    {
      "epoch": 10.218181818181819,
      "grad_norm": 0.03281534090638161,
      "learning_rate": 3.1060606060606063e-06,
      "loss": 0.0001,
      "step": 8430
    },
    {
      "epoch": 10.224242424242425,
      "grad_norm": 0.032384857535362244,
      "learning_rate": 3.095538720538721e-06,
      "loss": 0.0001,
      "step": 8435
    },
    {
      "epoch": 10.23030303030303,
      "grad_norm": 0.10826495289802551,
      "learning_rate": 3.085016835016835e-06,
      "loss": 0.0001,
      "step": 8440
    },
    {
      "epoch": 10.236363636363636,
      "grad_norm": 0.017374584451317787,
      "learning_rate": 3.07449494949495e-06,
      "loss": 0.0001,
      "step": 8445
    },
    {
      "epoch": 10.242424242424242,
      "grad_norm": 0.107375867664814,
      "learning_rate": 3.0639730639730643e-06,
      "loss": 0.0001,
      "step": 8450
    },
    {
      "epoch": 10.248484848484848,
      "grad_norm": 0.008681375533342361,
      "learning_rate": 3.053451178451179e-06,
      "loss": 0.0001,
      "step": 8455
    },
    {
      "epoch": 10.254545454545454,
      "grad_norm": 0.045354973524808884,
      "learning_rate": 3.042929292929293e-06,
      "loss": 0.0002,
      "step": 8460
    },
    {
      "epoch": 10.26060606060606,
      "grad_norm": 0.06913230568170547,
      "learning_rate": 3.032407407407408e-06,
      "loss": 0.0001,
      "step": 8465
    },
    {
      "epoch": 10.266666666666667,
      "grad_norm": 0.011440541595220566,
      "learning_rate": 3.021885521885522e-06,
      "loss": 0.0001,
      "step": 8470
    },
    {
      "epoch": 10.272727272727273,
      "grad_norm": 0.019141707569360733,
      "learning_rate": 3.0113636363636366e-06,
      "loss": 0.0,
      "step": 8475
    },
    {
      "epoch": 10.27878787878788,
      "grad_norm": 0.00568960327655077,
      "learning_rate": 3.000841750841751e-06,
      "loss": 0.0,
      "step": 8480
    },
    {
      "epoch": 10.284848484848485,
      "grad_norm": 0.03166409954428673,
      "learning_rate": 2.9903198653198654e-06,
      "loss": 0.0001,
      "step": 8485
    },
    {
      "epoch": 10.290909090909091,
      "grad_norm": 0.005270944908261299,
      "learning_rate": 2.97979797979798e-06,
      "loss": 0.0,
      "step": 8490
    },
    {
      "epoch": 10.296969696969697,
      "grad_norm": 0.007545846980065107,
      "learning_rate": 2.9692760942760946e-06,
      "loss": 0.0,
      "step": 8495
    },
    {
      "epoch": 10.303030303030303,
      "grad_norm": 0.012856402434408665,
      "learning_rate": 2.9587542087542094e-06,
      "loss": 0.0,
      "step": 8500
    },
    {
      "epoch": 10.309090909090909,
      "grad_norm": 0.08976524323225021,
      "learning_rate": 2.9482323232323234e-06,
      "loss": 0.0,
      "step": 8505
    },
    {
      "epoch": 10.315151515151515,
      "grad_norm": 0.09298450499773026,
      "learning_rate": 2.937710437710438e-06,
      "loss": 0.0,
      "step": 8510
    },
    {
      "epoch": 10.32121212121212,
      "grad_norm": 0.1155351996421814,
      "learning_rate": 2.927188552188552e-06,
      "loss": 0.0001,
      "step": 8515
    },
    {
      "epoch": 10.327272727272728,
      "grad_norm": 0.01223837025463581,
      "learning_rate": 2.916666666666667e-06,
      "loss": 0.0001,
      "step": 8520
    },
    {
      "epoch": 10.333333333333334,
      "grad_norm": 0.04856887087225914,
      "learning_rate": 2.9061447811447814e-06,
      "loss": 0.0001,
      "step": 8525
    },
    {
      "epoch": 10.33939393939394,
      "grad_norm": 0.003750291420146823,
      "learning_rate": 2.895622895622896e-06,
      "loss": 0.0001,
      "step": 8530
    },
    {
      "epoch": 10.345454545454546,
      "grad_norm": 0.04595497250556946,
      "learning_rate": 2.88510101010101e-06,
      "loss": 0.0001,
      "step": 8535
    },
    {
      "epoch": 10.351515151515152,
      "grad_norm": 0.06513520330190659,
      "learning_rate": 2.874579124579125e-06,
      "loss": 0.0,
      "step": 8540
    },
    {
      "epoch": 10.357575757575757,
      "grad_norm": 0.010958036407828331,
      "learning_rate": 2.864057239057239e-06,
      "loss": 0.0001,
      "step": 8545
    },
    {
      "epoch": 10.363636363636363,
      "grad_norm": 0.03273642063140869,
      "learning_rate": 2.8535353535353537e-06,
      "loss": 0.0,
      "step": 8550
    },
    {
      "epoch": 10.36969696969697,
      "grad_norm": 0.016724081709980965,
      "learning_rate": 2.843013468013468e-06,
      "loss": 0.0,
      "step": 8555
    },
    {
      "epoch": 10.375757575757575,
      "grad_norm": 0.008878173306584358,
      "learning_rate": 2.832491582491583e-06,
      "loss": 0.0,
      "step": 8560
    },
    {
      "epoch": 10.381818181818181,
      "grad_norm": 0.15357746183872223,
      "learning_rate": 2.8219696969696973e-06,
      "loss": 0.0001,
      "step": 8565
    },
    {
      "epoch": 10.387878787878789,
      "grad_norm": 0.03451823815703392,
      "learning_rate": 2.8114478114478117e-06,
      "loss": 0.0,
      "step": 8570
    },
    {
      "epoch": 10.393939393939394,
      "grad_norm": 0.13152551651000977,
      "learning_rate": 2.8009259259259265e-06,
      "loss": 0.0001,
      "step": 8575
    },
    {
      "epoch": 10.4,
      "grad_norm": 0.011400517076253891,
      "learning_rate": 2.7904040404040405e-06,
      "loss": 0.0,
      "step": 8580
    },
    {
      "epoch": 10.406060606060606,
      "grad_norm": 0.005245530512183905,
      "learning_rate": 2.7798821548821553e-06,
      "loss": 0.0,
      "step": 8585
    },
    {
      "epoch": 10.412121212121212,
      "grad_norm": 0.11146744340658188,
      "learning_rate": 2.7693602693602697e-06,
      "loss": 0.0001,
      "step": 8590
    },
    {
      "epoch": 10.418181818181818,
      "grad_norm": 0.02737724408507347,
      "learning_rate": 2.758838383838384e-06,
      "loss": 0.0,
      "step": 8595
    },
    {
      "epoch": 10.424242424242424,
      "grad_norm": 0.08788307756185532,
      "learning_rate": 2.7483164983164985e-06,
      "loss": 0.0002,
      "step": 8600
    },
    {
      "epoch": 10.43030303030303,
      "grad_norm": 0.0026875154580920935,
      "learning_rate": 2.7377946127946133e-06,
      "loss": 0.0001,
      "step": 8605
    },
    {
      "epoch": 10.436363636363636,
      "grad_norm": 0.00294461939483881,
      "learning_rate": 2.7272727272727272e-06,
      "loss": 0.0,
      "step": 8610
    },
    {
      "epoch": 10.442424242424243,
      "grad_norm": 0.010390506125986576,
      "learning_rate": 2.716750841750842e-06,
      "loss": 0.0,
      "step": 8615
    },
    {
      "epoch": 10.44848484848485,
      "grad_norm": 0.1433781534433365,
      "learning_rate": 2.706228956228956e-06,
      "loss": 0.0001,
      "step": 8620
    },
    {
      "epoch": 10.454545454545455,
      "grad_norm": 0.049132928252220154,
      "learning_rate": 2.695707070707071e-06,
      "loss": 0.0,
      "step": 8625
    },
    {
      "epoch": 10.460606060606061,
      "grad_norm": 0.059735190123319626,
      "learning_rate": 2.6851851851851856e-06,
      "loss": 0.0,
      "step": 8630
    },
    {
      "epoch": 10.466666666666667,
      "grad_norm": 0.006447255611419678,
      "learning_rate": 2.6746632996633e-06,
      "loss": 0.0001,
      "step": 8635
    },
    {
      "epoch": 10.472727272727273,
      "grad_norm": 0.00269758771173656,
      "learning_rate": 2.6641414141414144e-06,
      "loss": 0.0,
      "step": 8640
    },
    {
      "epoch": 10.478787878787879,
      "grad_norm": 0.038285303860902786,
      "learning_rate": 2.6536195286195288e-06,
      "loss": 0.0,
      "step": 8645
    },
    {
      "epoch": 10.484848484848484,
      "grad_norm": 0.020623499527573586,
      "learning_rate": 2.6430976430976436e-06,
      "loss": 0.0,
      "step": 8650
    },
    {
      "epoch": 10.49090909090909,
      "grad_norm": 0.011888482607901096,
      "learning_rate": 2.6325757575757576e-06,
      "loss": 0.0,
      "step": 8655
    },
    {
      "epoch": 10.496969696969696,
      "grad_norm": 0.011651856824755669,
      "learning_rate": 2.6220538720538724e-06,
      "loss": 0.0,
      "step": 8660
    },
    {
      "epoch": 10.49939393939394,
      "eval_average": 0.5339481533978859,
      "eval_crossner_ai": 0.5085638998181208,
      "eval_crossner_literature": 0.5107398567518903,
      "eval_crossner_music": 0.6676258992305167,
      "eval_crossner_politics": 0.5392886682708388,
      "eval_crossner_science": 0.5838926173995361,
      "eval_mit-movie": 0.5999999999505556,
      "eval_mit-restaurant": 0.32752613236374367,
      "eval_runtime": 20.8049,
      "eval_samples_per_second": 33.646,
      "eval_steps_per_second": 0.336,
      "step": 8662
    },
    {
      "epoch": 10.503030303030304,
      "grad_norm": 0.008910301141440868,
      "learning_rate": 2.6115319865319868e-06,
      "loss": 0.0,
      "step": 8665
    },
    {
      "epoch": 10.50909090909091,
      "grad_norm": 0.02093292959034443,
      "learning_rate": 2.601010101010101e-06,
      "loss": 0.0001,
      "step": 8670
    },
    {
      "epoch": 10.515151515151516,
      "grad_norm": 0.06484504044055939,
      "learning_rate": 2.5904882154882155e-06,
      "loss": 0.0,
      "step": 8675
    },
    {
      "epoch": 10.521212121212121,
      "grad_norm": 0.08498647809028625,
      "learning_rate": 2.5799663299663303e-06,
      "loss": 0.0001,
      "step": 8680
    },
    {
      "epoch": 10.527272727272727,
      "grad_norm": 0.020785316824913025,
      "learning_rate": 2.5694444444444443e-06,
      "loss": 0.0,
      "step": 8685
    },
    {
      "epoch": 10.533333333333333,
      "grad_norm": 0.01953372359275818,
      "learning_rate": 2.558922558922559e-06,
      "loss": 0.0001,
      "step": 8690
    },
    {
      "epoch": 10.539393939393939,
      "grad_norm": 0.02679799683392048,
      "learning_rate": 2.548400673400674e-06,
      "loss": 0.0,
      "step": 8695
    },
    {
      "epoch": 10.545454545454545,
      "grad_norm": 0.02467041090130806,
      "learning_rate": 2.537878787878788e-06,
      "loss": 0.0001,
      "step": 8700
    },
    {
      "epoch": 10.55151515151515,
      "grad_norm": 0.010369645431637764,
      "learning_rate": 2.5273569023569027e-06,
      "loss": 0.0001,
      "step": 8705
    },
    {
      "epoch": 10.557575757575757,
      "grad_norm": 0.021178236231207848,
      "learning_rate": 2.516835016835017e-06,
      "loss": 0.0,
      "step": 8710
    },
    {
      "epoch": 10.563636363636364,
      "grad_norm": 0.11153672635555267,
      "learning_rate": 2.5063131313131315e-06,
      "loss": 0.0,
      "step": 8715
    },
    {
      "epoch": 10.56969696969697,
      "grad_norm": 0.017151175066828728,
      "learning_rate": 2.495791245791246e-06,
      "loss": 0.0001,
      "step": 8720
    },
    {
      "epoch": 10.575757575757576,
      "grad_norm": 0.015144112519919872,
      "learning_rate": 2.4852693602693607e-06,
      "loss": 0.0001,
      "step": 8725
    },
    {
      "epoch": 10.581818181818182,
      "grad_norm": 0.06914065778255463,
      "learning_rate": 2.474747474747475e-06,
      "loss": 0.0001,
      "step": 8730
    },
    {
      "epoch": 10.587878787878788,
      "grad_norm": 0.06694386154413223,
      "learning_rate": 2.4642255892255895e-06,
      "loss": 0.0002,
      "step": 8735
    },
    {
      "epoch": 10.593939393939394,
      "grad_norm": 0.005018104799091816,
      "learning_rate": 2.453703703703704e-06,
      "loss": 0.0,
      "step": 8740
    },
    {
      "epoch": 10.6,
      "grad_norm": 0.007211660034954548,
      "learning_rate": 2.4431818181818182e-06,
      "loss": 0.0,
      "step": 8745
    },
    {
      "epoch": 10.606060606060606,
      "grad_norm": 0.1811111867427826,
      "learning_rate": 2.432659932659933e-06,
      "loss": 0.0001,
      "step": 8750
    },
    {
      "epoch": 10.612121212121211,
      "grad_norm": 0.0972726047039032,
      "learning_rate": 2.4221380471380474e-06,
      "loss": 0.0001,
      "step": 8755
    },
    {
      "epoch": 10.618181818181819,
      "grad_norm": 0.006508077494800091,
      "learning_rate": 2.411616161616162e-06,
      "loss": 0.0,
      "step": 8760
    },
    {
      "epoch": 10.624242424242425,
      "grad_norm": 0.019732272252440453,
      "learning_rate": 2.4010942760942762e-06,
      "loss": 0.0001,
      "step": 8765
    },
    {
      "epoch": 10.63030303030303,
      "grad_norm": 0.011335235089063644,
      "learning_rate": 2.3905723905723906e-06,
      "loss": 0.0,
      "step": 8770
    },
    {
      "epoch": 10.636363636363637,
      "grad_norm": 0.027996212244033813,
      "learning_rate": 2.380050505050505e-06,
      "loss": 0.0,
      "step": 8775
    },
    {
      "epoch": 10.642424242424243,
      "grad_norm": 0.10683491826057434,
      "learning_rate": 2.36952861952862e-06,
      "loss": 0.0001,
      "step": 8780
    },
    {
      "epoch": 10.648484848484848,
      "grad_norm": 0.02104366384446621,
      "learning_rate": 2.359006734006734e-06,
      "loss": 0.0001,
      "step": 8785
    },
    {
      "epoch": 10.654545454545454,
      "grad_norm": 0.08599469065666199,
      "learning_rate": 2.348484848484849e-06,
      "loss": 0.0002,
      "step": 8790
    },
    {
      "epoch": 10.66060606060606,
      "grad_norm": 0.13021287322044373,
      "learning_rate": 2.3379629629629634e-06,
      "loss": 0.0001,
      "step": 8795
    },
    {
      "epoch": 10.666666666666666,
      "grad_norm": 0.08240316063165665,
      "learning_rate": 2.3274410774410778e-06,
      "loss": 0.0001,
      "step": 8800
    },
    {
      "epoch": 10.672727272727272,
      "grad_norm": 0.013751420192420483,
      "learning_rate": 2.316919191919192e-06,
      "loss": 0.0,
      "step": 8805
    },
    {
      "epoch": 10.67878787878788,
      "grad_norm": 0.003521033562719822,
      "learning_rate": 2.3063973063973065e-06,
      "loss": 0.0,
      "step": 8810
    },
    {
      "epoch": 10.684848484848485,
      "grad_norm": 0.0358043871819973,
      "learning_rate": 2.295875420875421e-06,
      "loss": 0.0,
      "step": 8815
    },
    {
      "epoch": 10.690909090909091,
      "grad_norm": 0.03953935578465462,
      "learning_rate": 2.2853535353535357e-06,
      "loss": 0.0,
      "step": 8820
    },
    {
      "epoch": 10.696969696969697,
      "grad_norm": 0.10702595859766006,
      "learning_rate": 2.27483164983165e-06,
      "loss": 0.0001,
      "step": 8825
    },
    {
      "epoch": 10.703030303030303,
      "grad_norm": 0.06752809882164001,
      "learning_rate": 2.2643097643097645e-06,
      "loss": 0.0001,
      "step": 8830
    },
    {
      "epoch": 10.709090909090909,
      "grad_norm": 0.025657445192337036,
      "learning_rate": 2.253787878787879e-06,
      "loss": 0.0,
      "step": 8835
    },
    {
      "epoch": 10.715151515151515,
      "grad_norm": 0.04922390729188919,
      "learning_rate": 2.2432659932659933e-06,
      "loss": 0.0001,
      "step": 8840
    },
    {
      "epoch": 10.72121212121212,
      "grad_norm": 0.005692142061889172,
      "learning_rate": 2.2327441077441077e-06,
      "loss": 0.0,
      "step": 8845
    },
    {
      "epoch": 10.727272727272727,
      "grad_norm": 0.09586729854345322,
      "learning_rate": 2.222222222222222e-06,
      "loss": 0.0001,
      "step": 8850
    },
    {
      "epoch": 10.733333333333333,
      "grad_norm": 0.1487734168767929,
      "learning_rate": 2.211700336700337e-06,
      "loss": 0.0002,
      "step": 8855
    },
    {
      "epoch": 10.73939393939394,
      "grad_norm": 0.008980446495115757,
      "learning_rate": 2.2011784511784513e-06,
      "loss": 0.0,
      "step": 8860
    },
    {
      "epoch": 10.745454545454546,
      "grad_norm": 0.16185809671878815,
      "learning_rate": 2.190656565656566e-06,
      "loss": 0.0,
      "step": 8865
    },
    {
      "epoch": 10.751515151515152,
      "grad_norm": 0.047322653234004974,
      "learning_rate": 2.1801346801346805e-06,
      "loss": 0.0001,
      "step": 8870
    },
    {
      "epoch": 10.757575757575758,
      "grad_norm": 0.01579263247549534,
      "learning_rate": 2.169612794612795e-06,
      "loss": 0.0,
      "step": 8875
    },
    {
      "epoch": 10.763636363636364,
      "grad_norm": 0.05958355590701103,
      "learning_rate": 2.1590909090909092e-06,
      "loss": 0.0,
      "step": 8880
    },
    {
      "epoch": 10.76969696969697,
      "grad_norm": 0.007108738645911217,
      "learning_rate": 2.1485690235690236e-06,
      "loss": 0.0001,
      "step": 8885
    },
    {
      "epoch": 10.775757575757575,
      "grad_norm": 0.03420194238424301,
      "learning_rate": 2.138047138047138e-06,
      "loss": 0.0001,
      "step": 8890
    },
    {
      "epoch": 10.781818181818181,
      "grad_norm": 0.11896472424268723,
      "learning_rate": 2.127525252525253e-06,
      "loss": 0.0,
      "step": 8895
    },
    {
      "epoch": 10.787878787878787,
      "grad_norm": 0.0034004354383796453,
      "learning_rate": 2.1170033670033672e-06,
      "loss": 0.0,
      "step": 8900
    },
    {
      "epoch": 10.793939393939393,
      "grad_norm": 0.017515653744339943,
      "learning_rate": 2.1064814814814816e-06,
      "loss": 0.0001,
      "step": 8905
    },
    {
      "epoch": 10.8,
      "grad_norm": 0.014937116764485836,
      "learning_rate": 2.095959595959596e-06,
      "loss": 0.0,
      "step": 8910
    },
    {
      "epoch": 10.806060606060607,
      "grad_norm": 0.12921853363513947,
      "learning_rate": 2.0854377104377104e-06,
      "loss": 0.0,
      "step": 8915
    },
    {
      "epoch": 10.812121212121212,
      "grad_norm": 0.010908307507634163,
      "learning_rate": 2.074915824915825e-06,
      "loss": 0.0,
      "step": 8920
    },
    {
      "epoch": 10.818181818181818,
      "grad_norm": 0.0016217258526012301,
      "learning_rate": 2.0643939393939396e-06,
      "loss": 0.0,
      "step": 8925
    },
    {
      "epoch": 10.824242424242424,
      "grad_norm": 0.0039551653899252415,
      "learning_rate": 2.053872053872054e-06,
      "loss": 0.0,
      "step": 8930
    },
    {
      "epoch": 10.83030303030303,
      "grad_norm": 0.009852398186922073,
      "learning_rate": 2.0433501683501688e-06,
      "loss": 0.0001,
      "step": 8935
    },
    {
      "epoch": 10.836363636363636,
      "grad_norm": 0.033810265362262726,
      "learning_rate": 2.032828282828283e-06,
      "loss": 0.0,
      "step": 8940
    },
    {
      "epoch": 10.842424242424242,
      "grad_norm": 0.0839526504278183,
      "learning_rate": 2.0223063973063976e-06,
      "loss": 0.0001,
      "step": 8945
    },
    {
      "epoch": 10.848484848484848,
      "grad_norm": 0.0283985398709774,
      "learning_rate": 2.011784511784512e-06,
      "loss": 0.0001,
      "step": 8950
    },
    {
      "epoch": 10.854545454545455,
      "grad_norm": 0.10165482759475708,
      "learning_rate": 2.0012626262626263e-06,
      "loss": 0.0001,
      "step": 8955
    },
    {
      "epoch": 10.860606060606061,
      "grad_norm": 0.006017087493091822,
      "learning_rate": 1.9907407407407407e-06,
      "loss": 0.0,
      "step": 8960
    },
    {
      "epoch": 10.866666666666667,
      "grad_norm": 0.00504458649083972,
      "learning_rate": 1.980218855218855e-06,
      "loss": 0.0,
      "step": 8965
    },
    {
      "epoch": 10.872727272727273,
      "grad_norm": 0.123459592461586,
      "learning_rate": 1.96969696969697e-06,
      "loss": 0.0001,
      "step": 8970
    },
    {
      "epoch": 10.878787878787879,
      "grad_norm": 0.12366603314876556,
      "learning_rate": 1.9591750841750843e-06,
      "loss": 0.0,
      "step": 8975
    },
    {
      "epoch": 10.884848484848485,
      "grad_norm": 0.005027249921113253,
      "learning_rate": 1.9486531986531987e-06,
      "loss": 0.0,
      "step": 8980
    },
    {
      "epoch": 10.89090909090909,
      "grad_norm": 0.014874056912958622,
      "learning_rate": 1.9381313131313135e-06,
      "loss": 0.0001,
      "step": 8985
    },
    {
      "epoch": 10.896969696969697,
      "grad_norm": 0.019134633243083954,
      "learning_rate": 1.927609427609428e-06,
      "loss": 0.0,
      "step": 8990
    },
    {
      "epoch": 10.903030303030302,
      "grad_norm": 0.12938064336776733,
      "learning_rate": 1.9170875420875423e-06,
      "loss": 0.0,
      "step": 8995
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 0.002928828587755561,
      "learning_rate": 1.9065656565656569e-06,
      "loss": 0.0,
      "step": 9000
    },
    {
      "epoch": 10.915151515151516,
      "grad_norm": 0.011603759601712227,
      "learning_rate": 1.8960437710437713e-06,
      "loss": 0.0001,
      "step": 9005
    },
    {
      "epoch": 10.921212121212122,
      "grad_norm": 0.18532447516918182,
      "learning_rate": 1.8855218855218857e-06,
      "loss": 0.0001,
      "step": 9010
    },
    {
      "epoch": 10.927272727272728,
      "grad_norm": 0.07743364572525024,
      "learning_rate": 1.8750000000000003e-06,
      "loss": 0.0001,
      "step": 9015
    },
    {
      "epoch": 10.933333333333334,
      "grad_norm": 0.06107976660132408,
      "learning_rate": 1.8644781144781146e-06,
      "loss": 0.0001,
      "step": 9020
    },
    {
      "epoch": 10.93939393939394,
      "grad_norm": 0.07085248082876205,
      "learning_rate": 1.853956228956229e-06,
      "loss": 0.0001,
      "step": 9025
    },
    {
      "epoch": 10.945454545454545,
      "grad_norm": 0.1544158160686493,
      "learning_rate": 1.8434343434343434e-06,
      "loss": 0.0001,
      "step": 9030
    },
    {
      "epoch": 10.951515151515151,
      "grad_norm": 0.09109000116586685,
      "learning_rate": 1.832912457912458e-06,
      "loss": 0.0,
      "step": 9035
    },
    {
      "epoch": 10.957575757575757,
      "grad_norm": 0.006618274375796318,
      "learning_rate": 1.8223905723905724e-06,
      "loss": 0.0001,
      "step": 9040
    },
    {
      "epoch": 10.963636363636363,
      "grad_norm": 0.006487734615802765,
      "learning_rate": 1.8118686868686868e-06,
      "loss": 0.0,
      "step": 9045
    },
    {
      "epoch": 10.969696969696969,
      "grad_norm": 0.03550316020846367,
      "learning_rate": 1.8013468013468016e-06,
      "loss": 0.0001,
      "step": 9050
    },
    {
      "epoch": 10.975757575757576,
      "grad_norm": 0.004485168494284153,
      "learning_rate": 1.790824915824916e-06,
      "loss": 0.0,
      "step": 9055
    },
    {
      "epoch": 10.981818181818182,
      "grad_norm": 0.021590782329440117,
      "learning_rate": 1.7803030303030306e-06,
      "loss": 0.0,
      "step": 9060
    },
    {
      "epoch": 10.987878787878788,
      "grad_norm": 0.0032876869663596153,
      "learning_rate": 1.769781144781145e-06,
      "loss": 0.0001,
      "step": 9065
    },
    {
      "epoch": 10.993939393939394,
      "grad_norm": 0.04084188863635063,
      "learning_rate": 1.7592592592592594e-06,
      "loss": 0.0,
      "step": 9070
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.003675292246043682,
      "learning_rate": 1.748737373737374e-06,
      "loss": 0.0,
      "step": 9075
    },
    {
      "epoch": 11.0,
      "eval_average": 0.5453074241580937,
      "eval_crossner_ai": 0.5104166666165337,
      "eval_crossner_literature": 0.5378151260004244,
      "eval_crossner_music": 0.7016011644332356,
      "eval_crossner_politics": 0.528333333283254,
      "eval_crossner_science": 0.5849268840893587,
      "eval_mit-movie": 0.6149584487039694,
      "eval_mit-restaurant": 0.3391003459798805,
      "eval_runtime": 20.6377,
      "eval_samples_per_second": 33.919,
      "eval_steps_per_second": 0.339,
      "step": 9075
    },
    {
      "epoch": 11.006060606060606,
      "grad_norm": 0.16845467686653137,
      "learning_rate": 1.7382154882154884e-06,
      "loss": 0.0001,
      "step": 9080
    },
    {
      "epoch": 11.012121212121212,
      "grad_norm": 0.05716492608189583,
      "learning_rate": 1.7276936026936027e-06,
      "loss": 0.0,
      "step": 9085
    },
    {
      "epoch": 11.018181818181818,
      "grad_norm": 0.0030528646893799305,
      "learning_rate": 1.7192760942760945e-06,
      "loss": 0.0,
      "step": 9090
    },
    {
      "epoch": 11.024242424242424,
      "grad_norm": 0.01922210305929184,
      "learning_rate": 1.708754208754209e-06,
      "loss": 0.0,
      "step": 9095
    },
    {
      "epoch": 11.030303030303031,
      "grad_norm": 0.04352935776114464,
      "learning_rate": 1.6982323232323233e-06,
      "loss": 0.0,
      "step": 9100
    },
    {
      "epoch": 11.036363636363637,
      "grad_norm": 0.001474632415920496,
      "learning_rate": 1.6877104377104379e-06,
      "loss": 0.0,
      "step": 9105
    },
    {
      "epoch": 11.042424242424243,
      "grad_norm": 0.0015105650527402759,
      "learning_rate": 1.6771885521885523e-06,
      "loss": 0.0,
      "step": 9110
    },
    {
      "epoch": 11.048484848484849,
      "grad_norm": 0.0122802359983325,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0,
      "step": 9115
    },
    {
      "epoch": 11.054545454545455,
      "grad_norm": 0.0635756105184555,
      "learning_rate": 1.6561447811447813e-06,
      "loss": 0.0002,
      "step": 9120
    },
    {
      "epoch": 11.06060606060606,
      "grad_norm": 0.00423146178945899,
      "learning_rate": 1.6456228956228956e-06,
      "loss": 0.0,
      "step": 9125
    },
    {
      "epoch": 11.066666666666666,
      "grad_norm": 0.002150400774553418,
      "learning_rate": 1.6351010101010105e-06,
      "loss": 0.0,
      "step": 9130
    },
    {
      "epoch": 11.072727272727272,
      "grad_norm": 0.0024485422763973475,
      "learning_rate": 1.6245791245791248e-06,
      "loss": 0.0,
      "step": 9135
    },
    {
      "epoch": 11.078787878787878,
      "grad_norm": 0.01651616021990776,
      "learning_rate": 1.6140572390572392e-06,
      "loss": 0.0,
      "step": 9140
    },
    {
      "epoch": 11.084848484848484,
      "grad_norm": 0.012262237258255482,
      "learning_rate": 1.6035353535353536e-06,
      "loss": 0.0,
      "step": 9145
    },
    {
      "epoch": 11.090909090909092,
      "grad_norm": 0.0011319812620058656,
      "learning_rate": 1.5930134680134682e-06,
      "loss": 0.0,
      "step": 9150
    },
    {
      "epoch": 11.096969696969698,
      "grad_norm": 0.003507893066853285,
      "learning_rate": 1.5824915824915826e-06,
      "loss": 0.0001,
      "step": 9155
    },
    {
      "epoch": 11.103030303030303,
      "grad_norm": 0.0025804657489061356,
      "learning_rate": 1.571969696969697e-06,
      "loss": 0.0,
      "step": 9160
    },
    {
      "epoch": 11.10909090909091,
      "grad_norm": 0.0030231219716370106,
      "learning_rate": 1.5614478114478116e-06,
      "loss": 0.0,
      "step": 9165
    },
    {
      "epoch": 11.115151515151515,
      "grad_norm": 0.0007838916499167681,
      "learning_rate": 1.550925925925926e-06,
      "loss": 0.0,
      "step": 9170
    },
    {
      "epoch": 11.121212121212121,
      "grad_norm": 0.003825344145298004,
      "learning_rate": 1.5404040404040404e-06,
      "loss": 0.0,
      "step": 9175
    },
    {
      "epoch": 11.127272727272727,
      "grad_norm": 0.005037765484303236,
      "learning_rate": 1.529882154882155e-06,
      "loss": 0.0,
      "step": 9180
    },
    {
      "epoch": 11.133333333333333,
      "grad_norm": 0.010366478003561497,
      "learning_rate": 1.5193602693602694e-06,
      "loss": 0.0,
      "step": 9185
    },
    {
      "epoch": 11.139393939393939,
      "grad_norm": 0.009148011915385723,
      "learning_rate": 1.5088383838383837e-06,
      "loss": 0.0,
      "step": 9190
    },
    {
      "epoch": 11.145454545454545,
      "grad_norm": 0.0022489989642053843,
      "learning_rate": 1.4983164983164986e-06,
      "loss": 0.0001,
      "step": 9195
    },
    {
      "epoch": 11.151515151515152,
      "grad_norm": 0.002115215640515089,
      "learning_rate": 1.487794612794613e-06,
      "loss": 0.0001,
      "step": 9200
    },
    {
      "epoch": 11.157575757575758,
      "grad_norm": 0.08366898447275162,
      "learning_rate": 1.4772727272727275e-06,
      "loss": 0.0,
      "step": 9205
    },
    {
      "epoch": 11.163636363636364,
      "grad_norm": 0.0037997213657945395,
      "learning_rate": 1.466750841750842e-06,
      "loss": 0.0,
      "step": 9210
    },
    {
      "epoch": 11.16969696969697,
      "grad_norm": 0.052224207669496536,
      "learning_rate": 1.4562289562289563e-06,
      "loss": 0.0,
      "step": 9215
    },
    {
      "epoch": 11.175757575757576,
      "grad_norm": 0.001339313224889338,
      "learning_rate": 1.445707070707071e-06,
      "loss": 0.0,
      "step": 9220
    },
    {
      "epoch": 11.181818181818182,
      "grad_norm": 0.0026570777408778667,
      "learning_rate": 1.4351851851851853e-06,
      "loss": 0.0,
      "step": 9225
    },
    {
      "epoch": 11.187878787878788,
      "grad_norm": 0.0014340115012601018,
      "learning_rate": 1.4246632996632997e-06,
      "loss": 0.0,
      "step": 9230
    },
    {
      "epoch": 11.193939393939393,
      "grad_norm": 0.0026357383467257023,
      "learning_rate": 1.4141414141414143e-06,
      "loss": 0.0,
      "step": 9235
    },
    {
      "epoch": 11.2,
      "grad_norm": 0.0022961643990129232,
      "learning_rate": 1.4036195286195287e-06,
      "loss": 0.0,
      "step": 9240
    },
    {
      "epoch": 11.206060606060607,
      "grad_norm": 0.003748140763491392,
      "learning_rate": 1.393097643097643e-06,
      "loss": 0.0,
      "step": 9245
    },
    {
      "epoch": 11.212121212121213,
      "grad_norm": 0.0020879560615867376,
      "learning_rate": 1.3825757575757577e-06,
      "loss": 0.0,
      "step": 9250
    },
    {
      "epoch": 11.218181818181819,
      "grad_norm": 0.0032838115002959967,
      "learning_rate": 1.3720538720538723e-06,
      "loss": 0.0,
      "step": 9255
    },
    {
      "epoch": 11.224242424242425,
      "grad_norm": 0.00387154845520854,
      "learning_rate": 1.3615319865319867e-06,
      "loss": 0.0,
      "step": 9260
    },
    {
      "epoch": 11.23030303030303,
      "grad_norm": 0.03741283714771271,
      "learning_rate": 1.3510101010101013e-06,
      "loss": 0.0,
      "step": 9265
    },
    {
      "epoch": 11.236363636363636,
      "grad_norm": 0.0010849549435079098,
      "learning_rate": 1.3404882154882156e-06,
      "loss": 0.0,
      "step": 9270
    },
    {
      "epoch": 11.242424242424242,
      "grad_norm": 0.01012762077152729,
      "learning_rate": 1.32996632996633e-06,
      "loss": 0.0,
      "step": 9275
    },
    {
      "epoch": 11.248484848484848,
      "grad_norm": 0.0010414696298539639,
      "learning_rate": 1.3194444444444446e-06,
      "loss": 0.0001,
      "step": 9280
    },
    {
      "epoch": 11.254545454545454,
      "grad_norm": 0.0036197404842823744,
      "learning_rate": 1.308922558922559e-06,
      "loss": 0.0,
      "step": 9285
    },
    {
      "epoch": 11.26060606060606,
      "grad_norm": 0.0009296511416323483,
      "learning_rate": 1.2984006734006734e-06,
      "loss": 0.0,
      "step": 9290
    },
    {
      "epoch": 11.266666666666667,
      "grad_norm": 0.005136664491146803,
      "learning_rate": 1.287878787878788e-06,
      "loss": 0.0002,
      "step": 9295
    },
    {
      "epoch": 11.272727272727273,
      "grad_norm": 0.01499556191265583,
      "learning_rate": 1.2773569023569024e-06,
      "loss": 0.0,
      "step": 9300
    },
    {
      "epoch": 11.27878787878788,
      "grad_norm": 0.0008252310217358172,
      "learning_rate": 1.2668350168350168e-06,
      "loss": 0.0,
      "step": 9305
    },
    {
      "epoch": 11.284848484848485,
      "grad_norm": 0.005773385055363178,
      "learning_rate": 1.2563131313131314e-06,
      "loss": 0.0,
      "step": 9310
    },
    {
      "epoch": 11.290909090909091,
      "grad_norm": 0.003128601936623454,
      "learning_rate": 1.245791245791246e-06,
      "loss": 0.0,
      "step": 9315
    },
    {
      "epoch": 11.296969696969697,
      "grad_norm": 0.0035407980903983116,
      "learning_rate": 1.2352693602693604e-06,
      "loss": 0.0,
      "step": 9320
    },
    {
      "epoch": 11.303030303030303,
      "grad_norm": 0.0011443112744018435,
      "learning_rate": 1.2247474747474748e-06,
      "loss": 0.0001,
      "step": 9325
    },
    {
      "epoch": 11.309090909090909,
      "grad_norm": 0.007658285554498434,
      "learning_rate": 1.2142255892255894e-06,
      "loss": 0.0,
      "step": 9330
    },
    {
      "epoch": 11.315151515151515,
      "grad_norm": 0.002984365215525031,
      "learning_rate": 1.2037037037037037e-06,
      "loss": 0.0,
      "step": 9335
    },
    {
      "epoch": 11.32121212121212,
      "grad_norm": 0.0017207639757543802,
      "learning_rate": 1.1931818181818183e-06,
      "loss": 0.0,
      "step": 9340
    },
    {
      "epoch": 11.327272727272728,
      "grad_norm": 0.002010618569329381,
      "learning_rate": 1.1826599326599327e-06,
      "loss": 0.0001,
      "step": 9345
    },
    {
      "epoch": 11.333333333333334,
      "grad_norm": 0.0027941984590142965,
      "learning_rate": 1.1721380471380473e-06,
      "loss": 0.0001,
      "step": 9350
    },
    {
      "epoch": 11.33939393939394,
      "grad_norm": 0.022078268229961395,
      "learning_rate": 1.1616161616161617e-06,
      "loss": 0.0,
      "step": 9355
    },
    {
      "epoch": 11.345454545454546,
      "grad_norm": 0.0009701899252831936,
      "learning_rate": 1.1510942760942761e-06,
      "loss": 0.0,
      "step": 9360
    },
    {
      "epoch": 11.351515151515152,
      "grad_norm": 0.0018880999414250255,
      "learning_rate": 1.1405723905723907e-06,
      "loss": 0.0,
      "step": 9365
    },
    {
      "epoch": 11.357575757575757,
      "grad_norm": 0.005369284190237522,
      "learning_rate": 1.1300505050505053e-06,
      "loss": 0.0,
      "step": 9370
    },
    {
      "epoch": 11.363636363636363,
      "grad_norm": 0.0027221667114645243,
      "learning_rate": 1.1195286195286197e-06,
      "loss": 0.0,
      "step": 9375
    },
    {
      "epoch": 11.36969696969697,
      "grad_norm": 0.0036763285752385855,
      "learning_rate": 1.109006734006734e-06,
      "loss": 0.0,
      "step": 9380
    },
    {
      "epoch": 11.375757575757575,
      "grad_norm": 0.0018373738275840878,
      "learning_rate": 1.0984848484848485e-06,
      "loss": 0.0,
      "step": 9385
    },
    {
      "epoch": 11.381818181818181,
      "grad_norm": 0.0017849159194156528,
      "learning_rate": 1.087962962962963e-06,
      "loss": 0.0,
      "step": 9390
    },
    {
      "epoch": 11.387878787878789,
      "grad_norm": 0.005599256604909897,
      "learning_rate": 1.0774410774410775e-06,
      "loss": 0.0,
      "step": 9395
    },
    {
      "epoch": 11.393939393939394,
      "grad_norm": 0.0018631734419614077,
      "learning_rate": 1.0669191919191918e-06,
      "loss": 0.0,
      "step": 9400
    },
    {
      "epoch": 11.4,
      "grad_norm": 0.01840081624686718,
      "learning_rate": 1.0563973063973064e-06,
      "loss": 0.0,
      "step": 9405
    },
    {
      "epoch": 11.406060606060606,
      "grad_norm": 0.007461434695869684,
      "learning_rate": 1.045875420875421e-06,
      "loss": 0.0,
      "step": 9410
    },
    {
      "epoch": 11.412121212121212,
      "grad_norm": 0.03892696276307106,
      "learning_rate": 1.0353535353535354e-06,
      "loss": 0.0,
      "step": 9415
    },
    {
      "epoch": 11.418181818181818,
      "grad_norm": 0.0009907878702506423,
      "learning_rate": 1.0248316498316498e-06,
      "loss": 0.0,
      "step": 9420
    },
    {
      "epoch": 11.424242424242424,
      "grad_norm": 0.0016219576355069876,
      "learning_rate": 1.0143097643097644e-06,
      "loss": 0.0,
      "step": 9425
    },
    {
      "epoch": 11.43030303030303,
      "grad_norm": 0.03163674846291542,
      "learning_rate": 1.0037878787878788e-06,
      "loss": 0.0,
      "step": 9430
    },
    {
      "epoch": 11.436363636363636,
      "grad_norm": 0.000679289223626256,
      "learning_rate": 9.932659932659934e-07,
      "loss": 0.0001,
      "step": 9435
    },
    {
      "epoch": 11.442424242424243,
      "grad_norm": 0.017730271443724632,
      "learning_rate": 9.827441077441078e-07,
      "loss": 0.0001,
      "step": 9440
    },
    {
      "epoch": 11.44848484848485,
      "grad_norm": 0.0020495031494647264,
      "learning_rate": 9.722222222222224e-07,
      "loss": 0.0,
      "step": 9445
    },
    {
      "epoch": 11.454545454545455,
      "grad_norm": 0.0008897976367734373,
      "learning_rate": 9.617003367003368e-07,
      "loss": 0.0,
      "step": 9450
    },
    {
      "epoch": 11.460606060606061,
      "grad_norm": 0.0013479067711159587,
      "learning_rate": 9.511784511784513e-07,
      "loss": 0.0,
      "step": 9455
    },
    {
      "epoch": 11.466666666666667,
      "grad_norm": 0.004131050314754248,
      "learning_rate": 9.406565656565657e-07,
      "loss": 0.0,
      "step": 9460
    },
    {
      "epoch": 11.472727272727273,
      "grad_norm": 0.004083081614226103,
      "learning_rate": 9.301346801346803e-07,
      "loss": 0.0,
      "step": 9465
    },
    {
      "epoch": 11.478787878787879,
      "grad_norm": 0.01115088164806366,
      "learning_rate": 9.196127946127948e-07,
      "loss": 0.0,
      "step": 9470
    },
    {
      "epoch": 11.484848484848484,
      "grad_norm": 0.004177519585937262,
      "learning_rate": 9.090909090909091e-07,
      "loss": 0.0,
      "step": 9475
    },
    {
      "epoch": 11.49090909090909,
      "grad_norm": 0.16345950961112976,
      "learning_rate": 8.985690235690236e-07,
      "loss": 0.0001,
      "step": 9480
    },
    {
      "epoch": 11.496969696969696,
      "grad_norm": 0.001520997961051762,
      "learning_rate": 8.880471380471381e-07,
      "loss": 0.0,
      "step": 9485
    },
    {
      "epoch": 11.50060606060606,
      "eval_average": 0.5378206770054915,
      "eval_crossner_ai": 0.5170603674039356,
      "eval_crossner_literature": 0.5318892899620523,
      "eval_crossner_music": 0.6753812635665282,
      "eval_crossner_politics": 0.5242070116360672,
      "eval_crossner_science": 0.5701357465562222,
      "eval_mit-movie": 0.616246498550115,
      "eval_mit-restaurant": 0.32982456136352106,
      "eval_runtime": 20.6357,
      "eval_samples_per_second": 33.922,
      "eval_steps_per_second": 0.339,
      "step": 9488
    },
    {
      "epoch": 11.503030303030304,
      "grad_norm": 0.02249342016875744,
      "learning_rate": 8.775252525252525e-07,
      "loss": 0.0001,
      "step": 9490
    },
    {
      "epoch": 11.50909090909091,
      "grad_norm": 0.0018518888391554356,
      "learning_rate": 8.67003367003367e-07,
      "loss": 0.0,
      "step": 9495
    },
    {
      "epoch": 11.515151515151516,
      "grad_norm": 0.0031397822313010693,
      "learning_rate": 8.564814814814816e-07,
      "loss": 0.0,
      "step": 9500
    },
    {
      "epoch": 11.521212121212121,
      "grad_norm": 0.014715353958308697,
      "learning_rate": 8.459595959595961e-07,
      "loss": 0.0,
      "step": 9505
    },
    {
      "epoch": 11.527272727272727,
      "grad_norm": 0.0008788119303062558,
      "learning_rate": 8.354377104377105e-07,
      "loss": 0.0,
      "step": 9510
    },
    {
      "epoch": 11.533333333333333,
      "grad_norm": 0.00398845924064517,
      "learning_rate": 8.24915824915825e-07,
      "loss": 0.0,
      "step": 9515
    },
    {
      "epoch": 11.539393939393939,
      "grad_norm": 0.0023801811039447784,
      "learning_rate": 8.143939393939395e-07,
      "loss": 0.0,
      "step": 9520
    },
    {
      "epoch": 11.545454545454545,
      "grad_norm": 0.0062752701342105865,
      "learning_rate": 8.038720538720539e-07,
      "loss": 0.0,
      "step": 9525
    },
    {
      "epoch": 11.55151515151515,
      "grad_norm": 0.0012923750327900052,
      "learning_rate": 7.933501683501685e-07,
      "loss": 0.0,
      "step": 9530
    },
    {
      "epoch": 11.557575757575757,
      "grad_norm": 0.001677931402809918,
      "learning_rate": 7.82828282828283e-07,
      "loss": 0.0,
      "step": 9535
    },
    {
      "epoch": 11.563636363636364,
      "grad_norm": 0.006091644987463951,
      "learning_rate": 7.723063973063974e-07,
      "loss": 0.0,
      "step": 9540
    },
    {
      "epoch": 11.56969696969697,
      "grad_norm": 0.001254797331057489,
      "learning_rate": 7.617845117845118e-07,
      "loss": 0.0,
      "step": 9545
    },
    {
      "epoch": 11.575757575757576,
      "grad_norm": 0.0022030817344784737,
      "learning_rate": 7.512626262626263e-07,
      "loss": 0.0,
      "step": 9550
    },
    {
      "epoch": 11.581818181818182,
      "grad_norm": 0.0005287985550239682,
      "learning_rate": 7.407407407407407e-07,
      "loss": 0.0,
      "step": 9555
    },
    {
      "epoch": 11.587878787878788,
      "grad_norm": 0.00128025165759027,
      "learning_rate": 7.302188552188552e-07,
      "loss": 0.0,
      "step": 9560
    },
    {
      "epoch": 11.593939393939394,
      "grad_norm": 0.0026588598266243935,
      "learning_rate": 7.196969696969698e-07,
      "loss": 0.0,
      "step": 9565
    },
    {
      "epoch": 11.6,
      "grad_norm": 0.004383790772408247,
      "learning_rate": 7.091750841750843e-07,
      "loss": 0.0,
      "step": 9570
    },
    {
      "epoch": 11.606060606060606,
      "grad_norm": 0.0041380152106285095,
      "learning_rate": 6.986531986531987e-07,
      "loss": 0.0001,
      "step": 9575
    },
    {
      "epoch": 11.612121212121211,
      "grad_norm": 0.007943314500153065,
      "learning_rate": 6.881313131313132e-07,
      "loss": 0.0,
      "step": 9580
    },
    {
      "epoch": 11.618181818181819,
      "grad_norm": 0.0013007192173972726,
      "learning_rate": 6.776094276094276e-07,
      "loss": 0.0,
      "step": 9585
    },
    {
      "epoch": 11.624242424242425,
      "grad_norm": 0.014175191521644592,
      "learning_rate": 6.670875420875421e-07,
      "loss": 0.0,
      "step": 9590
    },
    {
      "epoch": 11.63030303030303,
      "grad_norm": 0.0009122425690293312,
      "learning_rate": 6.565656565656567e-07,
      "loss": 0.0,
      "step": 9595
    },
    {
      "epoch": 11.636363636363637,
      "grad_norm": 0.0010433458955958486,
      "learning_rate": 6.460437710437712e-07,
      "loss": 0.0,
      "step": 9600
    },
    {
      "epoch": 11.642424242424243,
      "grad_norm": 0.002029564930126071,
      "learning_rate": 6.355218855218856e-07,
      "loss": 0.0,
      "step": 9605
    },
    {
      "epoch": 11.648484848484848,
      "grad_norm": 0.0037193368189036846,
      "learning_rate": 6.25e-07,
      "loss": 0.0001,
      "step": 9610
    },
    {
      "epoch": 11.654545454545454,
      "grad_norm": 0.001744540175423026,
      "learning_rate": 6.144781144781145e-07,
      "loss": 0.0,
      "step": 9615
    },
    {
      "epoch": 11.66060606060606,
      "grad_norm": 0.004530968610197306,
      "learning_rate": 6.03956228956229e-07,
      "loss": 0.0,
      "step": 9620
    },
    {
      "epoch": 11.666666666666666,
      "grad_norm": 0.0005950663471594453,
      "learning_rate": 5.934343434343435e-07,
      "loss": 0.0,
      "step": 9625
    },
    {
      "epoch": 11.672727272727272,
      "grad_norm": 0.001248208456672728,
      "learning_rate": 5.829124579124579e-07,
      "loss": 0.0,
      "step": 9630
    },
    {
      "epoch": 11.67878787878788,
      "grad_norm": 0.0006138100288808346,
      "learning_rate": 5.723905723905725e-07,
      "loss": 0.0,
      "step": 9635
    },
    {
      "epoch": 11.684848484848485,
      "grad_norm": 0.008509596809744835,
      "learning_rate": 5.618686868686869e-07,
      "loss": 0.0,
      "step": 9640
    },
    {
      "epoch": 11.690909090909091,
      "grad_norm": 0.005831688642501831,
      "learning_rate": 5.513468013468014e-07,
      "loss": 0.0,
      "step": 9645
    },
    {
      "epoch": 11.696969696969697,
      "grad_norm": 0.0015582517953589559,
      "learning_rate": 5.408249158249158e-07,
      "loss": 0.0,
      "step": 9650
    },
    {
      "epoch": 11.703030303030303,
      "grad_norm": 0.0034546134993433952,
      "learning_rate": 5.303030303030304e-07,
      "loss": 0.0,
      "step": 9655
    },
    {
      "epoch": 11.709090909090909,
      "grad_norm": 0.006403970532119274,
      "learning_rate": 5.197811447811448e-07,
      "loss": 0.0,
      "step": 9660
    },
    {
      "epoch": 11.715151515151515,
      "grad_norm": 0.003093315754085779,
      "learning_rate": 5.092592592592593e-07,
      "loss": 0.0,
      "step": 9665
    },
    {
      "epoch": 11.72121212121212,
      "grad_norm": 0.0019548817072063684,
      "learning_rate": 4.987373737373738e-07,
      "loss": 0.0001,
      "step": 9670
    },
    {
      "epoch": 11.727272727272727,
      "grad_norm": 0.0011409991420805454,
      "learning_rate": 4.882154882154883e-07,
      "loss": 0.0,
      "step": 9675
    },
    {
      "epoch": 11.733333333333333,
      "grad_norm": 0.0025656973011791706,
      "learning_rate": 4.776936026936028e-07,
      "loss": 0.0,
      "step": 9680
    },
    {
      "epoch": 11.73939393939394,
      "grad_norm": 0.0011300902115181088,
      "learning_rate": 4.6717171717171724e-07,
      "loss": 0.0,
      "step": 9685
    },
    {
      "epoch": 11.745454545454546,
      "grad_norm": 0.0032481777016073465,
      "learning_rate": 4.566498316498317e-07,
      "loss": 0.0,
      "step": 9690
    },
    {
      "epoch": 11.751515151515152,
      "grad_norm": 0.0021770033054053783,
      "learning_rate": 4.461279461279461e-07,
      "loss": 0.0,
      "step": 9695
    },
    {
      "epoch": 11.757575757575758,
      "grad_norm": 0.004153273534029722,
      "learning_rate": 4.3560606060606067e-07,
      "loss": 0.0,
      "step": 9700
    },
    {
      "epoch": 11.763636363636364,
      "grad_norm": 0.007932364009320736,
      "learning_rate": 4.250841750841751e-07,
      "loss": 0.0001,
      "step": 9705
    },
    {
      "epoch": 11.76969696969697,
      "grad_norm": 0.0036484943702816963,
      "learning_rate": 4.145622895622896e-07,
      "loss": 0.0,
      "step": 9710
    },
    {
      "epoch": 11.775757575757575,
      "grad_norm": 0.004547714721411467,
      "learning_rate": 4.040404040404041e-07,
      "loss": 0.0,
      "step": 9715
    },
    {
      "epoch": 11.781818181818181,
      "grad_norm": 0.0015076638665050268,
      "learning_rate": 3.9351851851851854e-07,
      "loss": 0.0,
      "step": 9720
    },
    {
      "epoch": 11.787878787878787,
      "grad_norm": 0.0009051641100086272,
      "learning_rate": 3.8299663299663303e-07,
      "loss": 0.0,
      "step": 9725
    },
    {
      "epoch": 11.793939393939393,
      "grad_norm": 0.0008616105769760907,
      "learning_rate": 3.724747474747475e-07,
      "loss": 0.0,
      "step": 9730
    },
    {
      "epoch": 11.8,
      "grad_norm": 0.0043497937731444836,
      "learning_rate": 3.61952861952862e-07,
      "loss": 0.0,
      "step": 9735
    },
    {
      "epoch": 11.806060606060607,
      "grad_norm": 0.0008595267427153885,
      "learning_rate": 3.5143097643097646e-07,
      "loss": 0.0,
      "step": 9740
    },
    {
      "epoch": 11.812121212121212,
      "grad_norm": 0.008405955508351326,
      "learning_rate": 3.409090909090909e-07,
      "loss": 0.0,
      "step": 9745
    },
    {
      "epoch": 11.818181818181818,
      "grad_norm": 0.007315483409911394,
      "learning_rate": 3.3038720538720545e-07,
      "loss": 0.0,
      "step": 9750
    },
    {
      "epoch": 11.824242424242424,
      "grad_norm": 0.003827132051810622,
      "learning_rate": 3.198653198653199e-07,
      "loss": 0.0,
      "step": 9755
    },
    {
      "epoch": 11.83030303030303,
      "grad_norm": 0.004643816966563463,
      "learning_rate": 3.093434343434344e-07,
      "loss": 0.0,
      "step": 9760
    },
    {
      "epoch": 11.836363636363636,
      "grad_norm": 0.0019023629138246179,
      "learning_rate": 2.988215488215488e-07,
      "loss": 0.0,
      "step": 9765
    },
    {
      "epoch": 11.842424242424242,
      "grad_norm": 0.0007905619568191469,
      "learning_rate": 2.882996632996633e-07,
      "loss": 0.0,
      "step": 9770
    },
    {
      "epoch": 11.848484848484848,
      "grad_norm": 0.0006288245203904808,
      "learning_rate": 2.7777777777777776e-07,
      "loss": 0.0,
      "step": 9775
    },
    {
      "epoch": 11.854545454545455,
      "grad_norm": 0.004908225033432245,
      "learning_rate": 2.6725589225589225e-07,
      "loss": 0.0,
      "step": 9780
    },
    {
      "epoch": 11.860606060606061,
      "grad_norm": 0.004310529679059982,
      "learning_rate": 2.5673400673400675e-07,
      "loss": 0.0,
      "step": 9785
    },
    {
      "epoch": 11.866666666666667,
      "grad_norm": 0.0024199390318244696,
      "learning_rate": 2.4621212121212124e-07,
      "loss": 0.0,
      "step": 9790
    },
    {
      "epoch": 11.872727272727273,
      "grad_norm": 0.0008011975442059338,
      "learning_rate": 2.356902356902357e-07,
      "loss": 0.0,
      "step": 9795
    },
    {
      "epoch": 11.878787878787879,
      "grad_norm": 0.0011747300159186125,
      "learning_rate": 2.251683501683502e-07,
      "loss": 0.0,
      "step": 9800
    },
    {
      "epoch": 11.884848484848485,
      "grad_norm": 0.0009300086530856788,
      "learning_rate": 2.1464646464646467e-07,
      "loss": 0.0,
      "step": 9805
    },
    {
      "epoch": 11.89090909090909,
      "grad_norm": 0.010176927782595158,
      "learning_rate": 2.0412457912457916e-07,
      "loss": 0.0,
      "step": 9810
    },
    {
      "epoch": 11.896969696969697,
      "grad_norm": 0.009259950369596481,
      "learning_rate": 1.936026936026936e-07,
      "loss": 0.0,
      "step": 9815
    },
    {
      "epoch": 11.903030303030302,
      "grad_norm": 0.0018570158863440156,
      "learning_rate": 1.830808080808081e-07,
      "loss": 0.0,
      "step": 9820
    },
    {
      "epoch": 11.909090909090908,
      "grad_norm": 0.000805194431450218,
      "learning_rate": 1.725589225589226e-07,
      "loss": 0.0,
      "step": 9825
    },
    {
      "epoch": 11.915151515151516,
      "grad_norm": 0.0016153595643118024,
      "learning_rate": 1.6203703703703703e-07,
      "loss": 0.0,
      "step": 9830
    },
    {
      "epoch": 11.921212121212122,
      "grad_norm": 0.006457231473177671,
      "learning_rate": 1.5151515151515152e-07,
      "loss": 0.0,
      "step": 9835
    },
    {
      "epoch": 11.927272727272728,
      "grad_norm": 0.0018766693538054824,
      "learning_rate": 1.4099326599326602e-07,
      "loss": 0.0001,
      "step": 9840
    },
    {
      "epoch": 11.933333333333334,
      "grad_norm": 0.00134363048709929,
      "learning_rate": 1.3047138047138049e-07,
      "loss": 0.0,
      "step": 9845
    },
    {
      "epoch": 11.93939393939394,
      "grad_norm": 0.003743812907487154,
      "learning_rate": 1.1994949494949495e-07,
      "loss": 0.0,
      "step": 9850
    },
    {
      "epoch": 11.945454545454545,
      "grad_norm": 0.0012104813940823078,
      "learning_rate": 1.0942760942760945e-07,
      "loss": 0.0,
      "step": 9855
    },
    {
      "epoch": 11.951515151515151,
      "grad_norm": 0.0011102823773398995,
      "learning_rate": 9.890572390572391e-08,
      "loss": 0.0,
      "step": 9860
    },
    {
      "epoch": 11.957575757575757,
      "grad_norm": 0.0008887446601875126,
      "learning_rate": 8.83838383838384e-08,
      "loss": 0.0,
      "step": 9865
    },
    {
      "epoch": 11.963636363636363,
      "grad_norm": 0.0019264072179794312,
      "learning_rate": 7.786195286195287e-08,
      "loss": 0.0,
      "step": 9870
    },
    {
      "epoch": 11.969696969696969,
      "grad_norm": 0.0032966509461402893,
      "learning_rate": 6.734006734006734e-08,
      "loss": 0.0,
      "step": 9875
    },
    {
      "epoch": 11.975757575757576,
      "grad_norm": 0.0030627173837274313,
      "learning_rate": 5.681818181818182e-08,
      "loss": 0.0,
      "step": 9880
    },
    {
      "epoch": 11.981818181818182,
      "grad_norm": 0.0006500394083559513,
      "learning_rate": 4.6296296296296295e-08,
      "loss": 0.0,
      "step": 9885
    },
    {
      "epoch": 11.987878787878788,
      "grad_norm": 0.01358767505735159,
      "learning_rate": 3.577441077441078e-08,
      "loss": 0.0,
      "step": 9890
    },
    {
      "epoch": 11.993939393939394,
      "grad_norm": 0.007107733748853207,
      "learning_rate": 2.5252525252525256e-08,
      "loss": 0.0,
      "step": 9895
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.0007035115268081427,
      "learning_rate": 1.4730639730639732e-08,
      "loss": 0.0,
      "step": 9900
    },
    {
      "epoch": 12.0,
      "eval_average": 0.5413806964256739,
      "eval_crossner_ai": 0.5138339920447332,
      "eval_crossner_literature": 0.5210084033113527,
      "eval_crossner_music": 0.6990572878397369,
      "eval_crossner_politics": 0.5257048092368151,
      "eval_crossner_science": 0.5771812080035644,
      "eval_mit-movie": 0.6183844010647932,
      "eval_mit-restaurant": 0.33449477347872136,
      "eval_runtime": 20.6639,
      "eval_samples_per_second": 33.876,
      "eval_steps_per_second": 0.339,
      "step": 9900
    }
  ],
  "logging_steps": 5,
  "max_steps": 9900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 12,
  "save_steps": 9223372036854775807,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.841633932256215e+18,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
