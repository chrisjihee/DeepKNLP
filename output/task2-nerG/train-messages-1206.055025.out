[12.06 05:50:51] ┇ INFO     ┇                                   chrisbase.data ┇ =========================================================================================================================================
[12.06 05:50:51] ┇ INFO     ┇                                   chrisbase.data ┇ [INIT] python /raid/chrisjihee/proj/DeepKNLP/DeepKNLP/arguments.py 
[12.06 05:50:51] ┇ INFO     ┇                                   chrisbase.data ┇ =========================================================================================================================================
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇ -----+---------------------------+----------------------------------------------------------------
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇    # | NewTrainerArguments       | value
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇ -----+---------------------------+----------------------------------------------------------------
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇    1 | env.hostname              | dgx-a100
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇    2 | env.hostaddr              | 129.254.23.12
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇    3 | env.global_rank           | 0
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇    4 | env.local_rank            | 0
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇    5 | env.node_rank             | 0
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇    6 | env.world_size            | 2
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇    7 | env.time_stamp            | 1206.055025
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇    8 | env.python_path           | /raid/chrisjihee/miniforge3/envs/DeepKNLP/bin/python
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇    9 | env.current_dir           | /raid/chrisjihee/proj/DeepKNLP
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   10 | env.current_file          | /raid/chrisjihee/proj/DeepKNLP/DeepKNLP/arguments.py
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   11 | env.command_args          | []
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   12 | env.output_home           | /raid/chrisjihee/proj/DeepKNLP/output
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   13 | env.logging_home          | /raid/chrisjihee/proj/DeepKNLP/output/task2-nerG
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   14 | env.logging_file          | train-messages.out
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   15 | env.argument_file         | train-arguments.json
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   16 | env.max_workers           | 4
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   17 | env.debugging             | False
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   18 | env.date_format           | [%m.%d %H:%M:%S]
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   19 | env.message_level         | 20
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   20 | env.message_format        | %(asctime)s ┇ %(levelname)-8s ┇ %(name)48s ┇ %(message)s
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   21 | time.t1                   | 2024-12-06 05:50:51.993391
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   22 | time.t2                   | 2024-12-06 05:50:25.773789
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   23 | time.started              | [12.06 05:50:51]
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   24 | time.settled              |
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   25 | time.elapsed              |
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   26 | data.train_path           | /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-train.jsonl
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   27 | data.eval_path            |
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   28 | data.test_path            |
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   29 | data.max_train_samples    | 256
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   30 | data.max_eval_samples     | -1
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   31 | data.max_test_samples     | -1
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   32 | data.num_prog_samples     | 5000
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   33 | data.max_source_length    | 512
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   34 | data.max_target_length    | 512
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   35 | data.use_cache_data       | False
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   36 | model.pretrained          | meta-llama/Llama-3.2-1B
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   37 | hardware.gpu_index        | 4
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   38 | hardware.num_device       | 2
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   39 | hardware.grad_steps       | 8
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   40 | hardware.train_batch      | 4
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   41 | hardware.infer_batch      | 32
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   42 | hardware.accelerator      | gpu
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   43 | hardware.precision        | bf16-mixed
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   44 | hardware.strategy         | deepspeed
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   45 | hardware.ds_stage         | 2
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   46 | hardware.devices          | 2
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   47 | learning.random_seed      | 7.0
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   48 | learning.weight_decay     | 0.0
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   49 | learning.learning_rate    | 2e-05
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇   50 | learning.num_train_epochs | 1.0
[12.06 05:50:52] ┇ INFO     ┇                               DeepKNLP.arguments ┇ -----+---------------------------+----------------------------------------------------------------
[12.06 05:50:52] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 0] Seed set to 7
[12.06 05:50:52] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 1] Seed set to 7
[12.06 05:50:52] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 05:50:52] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 05:50:52] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 05:50:52] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file chat_template.jinja from cache at None
[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file chat_template.jinja from cache at None
[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 05:50:52] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 05:50:53] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 05:50:53] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 05:50:53] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 05:50:53] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.0",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 05:50:53] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 05:50:53] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 05:50:53] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 05:50:53] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 05:50:53] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 05:50:53] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 05:50:53] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 05:50:53] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 05:50:53] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 05:50:53] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 05:50:53] ┇ INFO     ┇                                         __main__ ┇ type(model)=<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> - True
[12.06 05:50:53] ┇ INFO     ┇                                         __main__ ┇ type(config)=<class 'transformers.models.llama.configuration_llama.LlamaConfig'> - True
[12.06 05:50:53] ┇ INFO     ┇                                         __main__ ┇ type(tokenizer)=<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'> - True
[12.06 05:50:53] ┇ INFO     ┇                                         __main__ ┇ len(tokenizer)=128256, embedding_size=128256
[12.06 05:50:53] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 05:50:53] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 05:50:53] ┇ INFO     ┇                                         __main__ ┇ ----------------------------------------------------------------------------------------------------
[12.06 05:50:54] ┇ INFO     ┇                                         __main__ ┇ Use /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-train.jsonl as train dataset: 256 samples
[12.06 05:50:55] ┇ INFO     ┇                                         __main__ ┇ ----------------------------------------------------------------------------------------------------
[12.06 05:50:56] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=168.60 Hz, eta=0:00:00, total=0:00:01
[12.06 05:50:58] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=176.75 Hz, eta=0:00:00, total=0:00:01
[12.06 05:50:59] ┇ INFO     ┇                                         __main__ ┇ ----------------------------------------------------------------------------------------------------
[12.06 05:50:59] ┇ INFO     ┇                                         __main__ ┇ type(optimizer)=<class 'torch.optim.adamw.AdamW'> - True
[12.06 05:51:16] ┇ INFO     ┇                                         __main__ ┇ type(model)=<class 'lightning.fabric.wrappers._FabricModule'> - True
[12.06 05:51:16] ┇ INFO     ┇                                         __main__ ┇ type(optimizer)=<class 'lightning.fabric.wrappers.FabricDeepSpeedZeroOptimizer'> - True
[12.06 05:51:16] ┇ INFO     ┇                                         __main__ ┇ ****************************************************************************************************
[12.06 05:51:16] ┇ INFO     ┇                                         __main__ ┇ ----------------------------------------------------------------------------------------------------
[12.06 05:51:16] ┇ INFO     ┇                                         __main__ ┇ Epoch: 0
[12.06 05:51:16] ┇ INFO     ┇                                         __main__ ┇ i=1
[12.06 05:51:16] ┇ INFO     ┇                                         __main__ ┇ loss=0.844304084777832
[12.06 05:51:16] ┇ INFO     ┇                                         __main__ ┇ i=2
[12.06 05:51:17] ┇ INFO     ┇                                         __main__ ┇ loss=0.9579153656959534
[12.06 05:51:17] ┇ INFO     ┇                                         __main__ ┇ i=3
[12.06 05:51:19] ┇ INFO     ┇                                         __main__ ┇ loss=1.1316871643066406
[12.06 05:51:19] ┇ INFO     ┇                                         __main__ ┇ i=4
[12.06 05:51:19] ┇ INFO     ┇                                         __main__ ┇ loss=1.0699800252914429
[12.06 05:51:19] ┇ INFO     ┇                                         __main__ ┇ i=5
[12.06 05:51:19] ┇ INFO     ┇                                         __main__ ┇ loss=0.8766821026802063
[12.06 05:51:19] ┇ INFO     ┇                                         __main__ ┇ i=6
[12.06 05:51:19] ┇ INFO     ┇                                         __main__ ┇ loss=1.4173146486282349
[12.06 05:51:19] ┇ INFO     ┇                                         __main__ ┇ i=7
[12.06 05:51:19] ┇ INFO     ┇                                         __main__ ┇ loss=0.7909431457519531
[12.06 05:51:19] ┇ INFO     ┇                                         __main__ ┇ i=8
[12.06 05:51:20] ┇ INFO     ┇                                         __main__ ┇ loss=1.030330777168274
[12.06 05:51:22] ┇ INFO     ┇                                         __main__ ┇ global_step=1
[12.06 05:51:22] ┇ INFO     ┇                                         __main__ ┇ i=9
[12.06 05:51:22] ┇ INFO     ┇                                         __main__ ┇ loss=0.8407523036003113
[12.06 05:51:22] ┇ INFO     ┇                                         __main__ ┇ i=10
[12.06 05:51:22] ┇ INFO     ┇                                         __main__ ┇ loss=0.7666146159172058
[12.06 05:51:22] ┇ INFO     ┇                                         __main__ ┇ i=11
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ loss=0.6094443798065186
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ i=12
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ loss=0.8033022284507751
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ i=13
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ loss=0.7917348146438599
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ i=14
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ loss=0.803385853767395
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ i=15
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ loss=0.6598423719406128
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ i=16
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ loss=0.8118489384651184
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ global_step=2
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ i=17
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ loss=0.9421792030334473
[12.06 05:51:23] ┇ INFO     ┇                                         __main__ ┇ i=18
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ loss=0.9059137105941772
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ i=19
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ loss=1.068523645401001
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ i=20
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ loss=0.8171312808990479
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ i=21
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ loss=0.9384055137634277
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ i=22
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ loss=0.8478488326072693
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ i=23
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ loss=0.7294140458106995
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ i=24
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ loss=1.0428766012191772
[12.06 05:51:24] ┇ INFO     ┇                                         __main__ ┇ global_step=3
[12.06 05:51:25] ┇ INFO     ┇                                         __main__ ┇ i=25
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ loss=0.25927141308784485
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ i=26
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ loss=0.26664501428604126
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ i=27
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ loss=0.32736772298812866
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ i=28
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ loss=0.4479699730873108
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ i=29
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ loss=0.3509403467178345
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ i=30
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ loss=0.3075837790966034
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ i=31
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ loss=0.3730337917804718
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ i=32
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ loss=0.33188310265541077
[12.06 05:51:26] ┇ INFO     ┇                                         __main__ ┇ global_step=4
[12.06 05:51:27] ┇ INFO     ┇                                   chrisbase.data ┇ =========================================================================================================================================
[12.06 05:51:27] ┇ INFO     ┇                                   chrisbase.data ┇ [EXIT] python /raid/chrisjihee/proj/DeepKNLP/DeepKNLP/arguments.py  ($=00:00:34.990)
[12.06 05:51:27] ┇ INFO     ┇                                   chrisbase.data ┇ =========================================================================================================================================
