[12.06 03:04:21] ┇ INFO     ┇                                   chrisbase.data ┇ =========================================================================================================================================
[12.06 03:04:21] ┇ INFO     ┇                                   chrisbase.data ┇ [INIT] python /raid/chrisjihee/proj/DeepKNLP/DeepKNLP/arguments.py 
[12.06 03:04:21] ┇ INFO     ┇                                   chrisbase.data ┇ =========================================================================================================================================
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇ -----+---------------------------+----------------------------------------------------------------
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇    # | NewTrainerArguments       | value
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇ -----+---------------------------+----------------------------------------------------------------
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇    1 | env.hostname              | dgx-a100
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇    2 | env.hostaddr              | 129.254.23.12
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇    3 | env.global_rank           | 0
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇    4 | env.local_rank            | 0
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇    5 | env.node_rank             | 0
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇    6 | env.world_size            | 8
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇    7 | env.time_stamp            | 1206.030403
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇    8 | env.python_path           | /raid/chrisjihee/miniforge3/envs/DeepKNLP/bin/python
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇    9 | env.current_dir           | /raid/chrisjihee/proj/DeepKNLP
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   10 | env.current_file          | /raid/chrisjihee/proj/DeepKNLP/DeepKNLP/arguments.py
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   11 | env.command_args          | []
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   12 | env.output_home           | /raid/chrisjihee/proj/DeepKNLP/output
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   13 | env.logging_home          | /raid/chrisjihee/proj/DeepKNLP/output/task2-nerG
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   14 | env.logging_file          | train-messages.out
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   15 | env.argument_file         | train-arguments.json
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   16 | env.max_workers           | 4
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   17 | env.debugging             | False
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   18 | env.date_format           | [%m.%d %H:%M:%S]
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   19 | env.message_level         | 20
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   20 | env.message_format        | %(asctime)s ┇ %(levelname)-8s ┇ %(name)48s ┇ %(message)s
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   21 | time.t1                   | 2024-12-06 03:04:21.501549
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   22 | time.t2                   | 2024-12-06 03:04:03.156684
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   23 | time.started              | [12.06 03:04:21]
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   24 | time.settled              |
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   25 | time.elapsed              |
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   26 | data.train_path           | /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-train.jsonl
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   27 | data.eval_path            | /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-dev.jsonl
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   28 | data.test_path            | /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-test.jsonl
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   29 | data.max_train_samples    | 256
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   30 | data.max_eval_samples     | -1
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   31 | data.max_test_samples     | -1
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   32 | data.num_prog_samples     | 5000
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   33 | data.max_source_length    | 512
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   34 | data.max_target_length    | 512
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   35 | data.use_cache_data       | False
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   36 | model.pretrained          | meta-llama/Llama-3.2-1B
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   37 | hardware.gpu_index        | [0, 1, 2, 3, 4, 5, 6, 7]
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   38 | hardware.grad_steps       | 8
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   39 | hardware.train_batch      | 4
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   40 | hardware.infer_batch      | 32
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   41 | hardware.accelerator      | gpu
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   42 | hardware.precision        | bf16-mixed
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   43 | hardware.strategy         | ddp
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   44 | learning.random_seed      | 7.0
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   45 | learning.weight_decay     | 0.0
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   46 | learning.learning_rate    | 2e-05
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇   47 | learning.num_train_epochs | 1.0
[12.06 03:04:21] ┇ INFO     ┇                               DeepKNLP.arguments ┇ -----+---------------------------+----------------------------------------------------------------
[12.06 03:04:21] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 0] Seed set to 7
[12.06 03:04:21] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 4] Seed set to 7
[12.06 03:04:21] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 3] Seed set to 7
[12.06 03:04:21] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 2] Seed set to 7
[12.06 03:04:21] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 7] Seed set to 7
[12.06 03:04:21] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 1] Seed set to 7
[12.06 03:04:21] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 5] Seed set to 7
[12.06 03:04:21] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 6] Seed set to 7
[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:21] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 03:04:21] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:22] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 03:04:22] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:22] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 03:04:22] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 03:04:22] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 03:04:22] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 03:04:22] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 03:04:22] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:22] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:22] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 03:04:22] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 03:04:22] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 03:04:22] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 03:04:22] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 03:04:23] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:23] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 03:04:23] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:04:23] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 03:04:23] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 03:04:23] ┇ INFO     ┇                                         __main__ ┇ type(model)=<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> - True
[12.06 03:04:23] ┇ INFO     ┇                                         __main__ ┇ type(config)=<class 'transformers.models.llama.configuration_llama.LlamaConfig'> - True
[12.06 03:04:23] ┇ INFO     ┇                                         __main__ ┇ type(tokenizer)=<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'> - True
[12.06 03:04:23] ┇ INFO     ┇                                         __main__ ┇ len(tokenizer)=128256, embedding_size=128256
[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 03:04:23] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 03:04:24] ┇ INFO     ┇                                         __main__ ┇ ----------------------------------------------------------------------------------------------------
[12.06 03:04:24] ┇ INFO     ┇                                         __main__ ┇ Use /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-train.jsonl as train dataset: 256 samples
[12.06 03:04:25] ┇ INFO     ┇                                         __main__ ┇ Use /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-dev.jsonl as eval dataset: 6,085 samples
[12.06 03:04:26] ┇ INFO     ┇                                         __main__ ┇ Use /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-dev.jsonl as test dataset: 6,470 samples
[12.06 03:04:26] ┇ INFO     ┇                                         __main__ ┇ ----------------------------------------------------------------------------------------------------
[12.06 03:04:27] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=192.67 Hz, eta=0:00:00, total=0:00:01
[12.06 03:04:30] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=193.65 Hz, eta=0:00:00, total=0:00:01
[12.06 03:04:30] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=188.40 Hz, eta=0:00:00, total=0:00:01
[12.06 03:04:30] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=181.66 Hz, eta=0:00:00, total=0:00:01
[12.06 03:04:30] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=169.25 Hz, eta=0:00:00, total=0:00:01
[12.06 03:04:30] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=169.02 Hz, eta=0:00:00, total=0:00:01
[12.06 03:04:30] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=161.65 Hz, eta=0:00:00, total=0:00:01
[12.06 03:04:30] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=133.00 Hz, eta=0:00:00, total=0:00:01
[12.06 03:04:33] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 82.17% 5000/6085... rate=2324.91 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:33] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 100.00% 6085/6085... rate=2612.64 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:36] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 82.17% 5000/6085... rate=2264.39 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:36] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 100.00% 6085/6085... rate=2538.40 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:36] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 82.17% 5000/6085... rate=2017.63 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:37] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 82.17% 5000/6085... rate=1994.28 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:37] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 82.17% 5000/6085... rate=1940.45 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:37] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 82.17% 5000/6085... rate=1886.03 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:37] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 100.00% 6085/6085... rate=2286.53 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:37] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 100.00% 6085/6085... rate=2256.53 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:37] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 100.00% 6085/6085... rate=2197.11 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:37] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 82.17% 5000/6085... rate=1799.92 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:37] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 100.00% 6085/6085... rate=2143.93 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:37] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 82.17% 5000/6085... rate=1713.82 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:37] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 100.00% 6085/6085... rate=2056.00 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:37] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 100.00% 6085/6085... rate=1966.01 Hz, eta=0:00:00, total=0:00:03
[12.06 03:04:40] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 77.28% 5000/6470... rate=2314.73 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:40] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 100.00% 6470/6470... rate=2686.99 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:43] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 77.28% 5000/6470... rate=2433.87 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:43] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 77.28% 5000/6470... rate=2379.36 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:43] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 77.28% 5000/6470... rate=2221.55 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:43] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 77.28% 5000/6470... rate=2174.39 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:43] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 100.00% 6470/6470... rate=2813.14 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:43] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 100.00% 6470/6470... rate=2756.25 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:43] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 77.28% 5000/6470... rate=2038.75 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:43] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 77.28% 5000/6470... rate=2030.78 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:43] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 100.00% 6470/6470... rate=2589.87 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:43] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 100.00% 6470/6470... rate=2546.77 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:43] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 100.00% 6470/6470... rate=2402.95 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:43] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 100.00% 6470/6470... rate=2383.51 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:44] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 77.28% 5000/6470... rate=1788.56 Hz, eta=0:00:00, total=0:00:02
[12.06 03:04:44] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 100.00% 6470/6470... rate=2123.52 Hz, eta=0:00:00, total=0:00:03
[12.06 03:04:45] ┇ INFO     ┇                                         __main__ ┇ ----------------------------------------------------------------------------------------------------
[12.06 03:04:45] ┇ INFO     ┇                                         __main__ ┇ type(optimizer)=<class 'torch.optim.adamw.AdamW'> - True
[12.06 03:04:48] ┇ INFO     ┇                                         __main__ ┇ type(model)=<class 'lightning.fabric.wrappers._FabricModule'> - True
[12.06 03:04:48] ┇ INFO     ┇                                         __main__ ┇ type(optimizer)=<class 'lightning.fabric.wrappers.FabricAdamW'> - True
[12.06 03:04:48] ┇ INFO     ┇                                         __main__ ┇ ****************************************************************************************************
[12.06 03:04:48] ┇ INFO     ┇                                         __main__ ┇ ----------------------------------------------------------------------------------------------------
[12.06 03:04:48] ┇ INFO     ┇                                         __main__ ┇ Epoch: 0
[12.06 03:04:48] ┇ INFO     ┇                                         __main__ ┇ i=1
[12.06 03:04:48] ┇ INFO     ┇                                         __main__ ┇ loss=1.1757843494415283
[12.06 03:04:48] ┇ INFO     ┇                                         __main__ ┇ i=2
[12.06 03:04:48] ┇ INFO     ┇                                         __main__ ┇ loss=0.9884088635444641
[12.06 03:04:48] ┇ INFO     ┇                                         __main__ ┇ i=3
[12.06 03:04:49] ┇ INFO     ┇                                         __main__ ┇ loss=0.930074155330658
[12.06 03:04:49] ┇ INFO     ┇                                         __main__ ┇ i=4
[12.06 03:04:49] ┇ INFO     ┇                                         __main__ ┇ loss=1.0997982025146484
[12.06 03:04:49] ┇ INFO     ┇                                         __main__ ┇ i=5
[12.06 03:04:49] ┇ INFO     ┇                                         __main__ ┇ loss=0.9041901230812073
[12.06 03:04:49] ┇ INFO     ┇                                         __main__ ┇ i=6
[12.06 03:04:49] ┇ INFO     ┇                                         __main__ ┇ loss=0.9155694842338562
[12.06 03:04:49] ┇ INFO     ┇                                         __main__ ┇ i=7
[12.06 03:04:49] ┇ INFO     ┇                                         __main__ ┇ loss=0.8673604726791382
[12.06 03:04:49] ┇ INFO     ┇                                         __main__ ┇ i=8
[12.06 03:04:50] ┇ INFO     ┇                                         __main__ ┇ loss=0.8781905770301819
[12.06 03:04:50] ┇ INFO     ┇                                         __main__ ┇ global_step=1
[12.06 03:04:50] ┇ INFO     ┇                                   chrisbase.data ┇ =========================================================================================================================================
[12.06 03:04:50] ┇ INFO     ┇                                   chrisbase.data ┇ [EXIT] python /raid/chrisjihee/proj/DeepKNLP/DeepKNLP/arguments.py  ($=00:00:28.701)
[12.06 03:04:50] ┇ INFO     ┇                                   chrisbase.data ┇ =========================================================================================================================================
