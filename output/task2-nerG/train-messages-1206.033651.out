[12.06 03:37:13] ┇ INFO     ┇                                   chrisbase.data ┇ =========================================================================================================================================
[12.06 03:37:13] ┇ INFO     ┇                                   chrisbase.data ┇ [INIT] python /raid/chrisjihee/proj/DeepKNLP/DeepKNLP/arguments.py 
[12.06 03:37:13] ┇ INFO     ┇                                   chrisbase.data ┇ =========================================================================================================================================
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇ -----+---------------------------+----------------------------------------------------------------
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇    # | NewTrainerArguments       | value
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇ -----+---------------------------+----------------------------------------------------------------
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇    1 | env.hostname              | dgx-a100
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇    2 | env.hostaddr              | 129.254.23.12
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇    3 | env.global_rank           | 0
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇    4 | env.local_rank            | 0
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇    5 | env.node_rank             | 0
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇    6 | env.world_size            | 4
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇    7 | env.time_stamp            | 1206.033651
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇    8 | env.python_path           | /raid/chrisjihee/miniforge3/envs/DeepKNLP/bin/python
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇    9 | env.current_dir           | /raid/chrisjihee/proj/DeepKNLP
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   10 | env.current_file          | /raid/chrisjihee/proj/DeepKNLP/DeepKNLP/arguments.py
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   11 | env.command_args          | []
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   12 | env.output_home           | /raid/chrisjihee/proj/DeepKNLP/output
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   13 | env.logging_home          | /raid/chrisjihee/proj/DeepKNLP/output/task2-nerG
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   14 | env.logging_file          | train-messages.out
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   15 | env.argument_file         | train-arguments.json
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   16 | env.max_workers           | 4
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   17 | env.debugging             | False
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   18 | env.date_format           | [%m.%d %H:%M:%S]
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   19 | env.message_level         | 20
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   20 | env.message_format        | %(asctime)s ┇ %(levelname)-8s ┇ %(name)48s ┇ %(message)s
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   21 | time.t1                   | 2024-12-06 03:37:13.456482
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   22 | time.t2                   | 2024-12-06 03:36:51.265467
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   23 | time.started              | [12.06 03:37:13]
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   24 | time.settled              |
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   25 | time.elapsed              |
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   26 | data.train_path           | /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-train.jsonl
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   27 | data.eval_path            | /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-dev.jsonl
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   28 | data.test_path            | /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-test.jsonl
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   29 | data.max_train_samples    | 256
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   30 | data.max_eval_samples     | -1
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   31 | data.max_test_samples     | -1
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   32 | data.num_prog_samples     | 5000
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   33 | data.max_source_length    | 512
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   34 | data.max_target_length    | 512
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   35 | data.use_cache_data       | False
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   36 | model.pretrained          | meta-llama/Llama-3.2-1B
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   37 | hardware.gpu_index        | 4
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   38 | hardware.num_device       | 4
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   39 | hardware.grad_steps       | 8
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   40 | hardware.train_batch      | 4
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   41 | hardware.infer_batch      | 32
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   42 | hardware.accelerator      | gpu
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   43 | hardware.precision        | bf16-mixed
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   44 | hardware.strategy         | ddp
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   45 | hardware.devices          | [4, 5, 6, 7]
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   46 | learning.random_seed      | 7.0
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   47 | learning.weight_decay     | 0.0
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   48 | learning.learning_rate    | 2e-05
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇   49 | learning.num_train_epochs | 1.0
[12.06 03:37:13] ┇ INFO     ┇                               DeepKNLP.arguments ┇ -----+---------------------------+----------------------------------------------------------------
[12.06 03:37:13] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 0] Seed set to 7
[12.06 03:37:13] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 1] Seed set to 7
[12.06 03:37:13] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 3] Seed set to 7
[12.06 03:37:13] ┇ INFO     ┇                  lightning.fabric.utilities.seed ┇ [rank: 2] Seed set to 7
[12.06 03:37:13] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:37:13] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:37:13] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:37:13] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:37:13] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:37:13] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:37:13] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:37:13] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 03:37:13] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 03:37:14] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer.json
[12.06 03:37:14] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer.model from cache at None
[12.06 03:37:14] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file added_tokens.json from cache at None
[12.06 03:37:14] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file special_tokens_map.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/special_tokens_map.json
[12.06 03:37:14] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ loading file tokenizer_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/tokenizer_config.json
[12.06 03:37:14] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 03:37:14] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 03:37:14] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 03:37:14] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:37:14] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:37:14] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:37:14] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:37:14] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:37:14] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:37:14] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 03:37:14] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 03:37:14] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 03:37:14] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 03:37:14] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 03:37:14] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 03:37:14] ┇ INFO     ┇             transformers.tokenization_utils_base ┇ Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[12.06 03:37:14] ┇ INFO     ┇                 transformers.configuration_utils ┇ loading configuration file config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/config.json
[12.06 03:37:14] ┇ INFO     ┇                 transformers.configuration_utils ┇ Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-3.2-1B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 128256
}

[12.06 03:37:14] ┇ INFO     ┇                      transformers.modeling_utils ┇ loading weights file model.safetensors from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/model.safetensors
[12.06 03:37:14] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[12.06 03:37:15] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 03:37:15] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 03:37:15] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 03:37:15] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 03:37:15] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 03:37:15] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 03:37:15] ┇ INFO     ┇                      transformers.modeling_utils ┇ All model checkpoint weights were used when initializing LlamaForCausalLM.

[12.06 03:37:15] ┇ INFO     ┇                      transformers.modeling_utils ┇ All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[12.06 03:37:15] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 03:37:15] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 03:37:15] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 03:37:15] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 03:37:15] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 03:37:15] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 03:37:15] ┇ INFO     ┇                                         __main__ ┇ type(model)=<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> - True
[12.06 03:37:15] ┇ INFO     ┇                                         __main__ ┇ type(config)=<class 'transformers.models.llama.configuration_llama.LlamaConfig'> - True
[12.06 03:37:15] ┇ INFO     ┇                                         __main__ ┇ type(tokenizer)=<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'> - True
[12.06 03:37:15] ┇ INFO     ┇                                         __main__ ┇ len(tokenizer)=128256, embedding_size=128256
[12.06 03:37:16] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ loading configuration file generation_config.json from cache at /raid/chrisjihee/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08/generation_config.json
[12.06 03:37:16] ┇ INFO     ┇      transformers.generation.configuration_utils ┇ Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[12.06 03:37:16] ┇ INFO     ┇                                         __main__ ┇ ----------------------------------------------------------------------------------------------------
[12.06 03:37:16] ┇ INFO     ┇                                         __main__ ┇ Use /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-train.jsonl as train dataset: 256 samples
[12.06 03:37:17] ┇ INFO     ┇                                         __main__ ┇ Use /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-dev.jsonl as eval dataset: 6,085 samples
[12.06 03:37:18] ┇ INFO     ┇                                         __main__ ┇ Use /raid/chrisjihee/proj/DeepKNLP/data/gner/zero-shot-dev.jsonl as test dataset: 6,470 samples
[12.06 03:37:18] ┇ INFO     ┇                                         __main__ ┇ ----------------------------------------------------------------------------------------------------
[12.06 03:37:20] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=157.64 Hz, eta=0:00:00, total=0:00:01
[12.06 03:37:22] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=175.35 Hz, eta=0:00:00, total=0:00:01
[12.06 03:37:22] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=175.18 Hz, eta=0:00:00, total=0:00:01
[12.06 03:37:22] ┇ INFO     ┇                                         __main__ ┇ Preprocess train samples 100.00% 256/256... rate=175.09 Hz, eta=0:00:00, total=0:00:01
[12.06 03:37:25] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 82.17% 5000/6085... rate=2053.95 Hz, eta=0:00:00, total=0:00:02
[12.06 03:37:25] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 100.00% 6085/6085... rate=2332.81 Hz, eta=0:00:00, total=0:00:02
[12.06 03:37:29] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 82.17% 5000/6085... rate=1725.24 Hz, eta=0:00:00, total=0:00:02
[12.06 03:37:29] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 82.17% 5000/6085... rate=1718.92 Hz, eta=0:00:00, total=0:00:02
[12.06 03:37:29] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 82.17% 5000/6085... rate=1721.90 Hz, eta=0:00:00, total=0:00:02
[12.06 03:37:29] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 100.00% 6085/6085... rate=1968.25 Hz, eta=0:00:00, total=0:00:03
[12.06 03:37:29] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 100.00% 6085/6085... rate=1965.63 Hz, eta=0:00:00, total=0:00:03
[12.06 03:37:29] ┇ INFO     ┇                                         __main__ ┇ Preprocess eval samples 100.00% 6085/6085... rate=1963.92 Hz, eta=0:00:00, total=0:00:03
[12.06 03:37:32] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 77.28% 5000/6470... rate=2049.90 Hz, eta=0:00:00, total=0:00:02
[12.06 03:37:32] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 100.00% 6470/6470... rate=2426.62 Hz, eta=0:00:00, total=0:00:02
[12.06 03:37:35] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 77.28% 5000/6470... rate=2362.91 Hz, eta=0:00:00, total=0:00:02
[12.06 03:37:35] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 77.28% 5000/6470... rate=2188.75 Hz, eta=0:00:00, total=0:00:02
[12.06 03:37:35] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 77.28% 5000/6470... rate=2160.61 Hz, eta=0:00:00, total=0:00:02
[12.06 03:37:35] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 100.00% 6470/6470... rate=2754.64 Hz, eta=0:00:00, total=0:00:02
[12.06 03:37:35] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 100.00% 6470/6470... rate=2555.06 Hz, eta=0:00:00, total=0:00:02
[12.06 03:37:36] ┇ INFO     ┇                                         __main__ ┇ Preprocess test samples 100.00% 6470/6470... rate=2505.15 Hz, eta=0:00:00, total=0:00:02
[12.06 03:37:36] ┇ INFO     ┇                                         __main__ ┇ ----------------------------------------------------------------------------------------------------
[12.06 03:37:36] ┇ INFO     ┇                                         __main__ ┇ type(optimizer)=<class 'torch.optim.adamw.AdamW'> - True
[12.06 03:37:47] ┇ INFO     ┇                                         __main__ ┇ type(model)=<class 'lightning.fabric.wrappers._FabricModule'> - True
[12.06 03:37:47] ┇ INFO     ┇                                         __main__ ┇ type(optimizer)=<class 'lightning.fabric.wrappers.FabricAdamW'> - True
[12.06 03:37:47] ┇ INFO     ┇                                         __main__ ┇ ****************************************************************************************************
[12.06 03:37:47] ┇ INFO     ┇                                         __main__ ┇ ----------------------------------------------------------------------------------------------------
[12.06 03:37:47] ┇ INFO     ┇                                         __main__ ┇ Epoch: 0
[12.06 03:37:47] ┇ INFO     ┇                                         __main__ ┇ i=1
[12.06 03:37:50] ┇ INFO     ┇                                         __main__ ┇ loss=0.9298353791236877
[12.06 03:37:50] ┇ INFO     ┇                                         __main__ ┇ i=2
[12.06 03:37:50] ┇ INFO     ┇                                         __main__ ┇ loss=1.2626253366470337
[12.06 03:37:50] ┇ INFO     ┇                                         __main__ ┇ i=3
[12.06 03:37:53] ┇ INFO     ┇                                         __main__ ┇ loss=1.186231255531311
[12.06 03:37:53] ┇ INFO     ┇                                         __main__ ┇ i=4
[12.06 03:37:53] ┇ INFO     ┇                                         __main__ ┇ loss=0.8426823616027832
[12.06 03:37:53] ┇ INFO     ┇                                         __main__ ┇ i=5
[12.06 03:37:53] ┇ INFO     ┇                                         __main__ ┇ loss=0.9166041016578674
[12.06 03:37:53] ┇ INFO     ┇                                         __main__ ┇ i=6
[12.06 03:37:53] ┇ INFO     ┇                                         __main__ ┇ loss=0.8525665402412415
[12.06 03:37:53] ┇ INFO     ┇                                         __main__ ┇ i=7
[12.06 03:37:53] ┇ INFO     ┇                                         __main__ ┇ loss=1.149337649345398
[12.06 03:37:53] ┇ INFO     ┇                                         __main__ ┇ i=8
[12.06 03:37:54] ┇ INFO     ┇                                         __main__ ┇ loss=1.0218591690063477
[12.06 03:37:56] ┇ INFO     ┇                                         __main__ ┇ global_step=1
[12.06 03:37:56] ┇ INFO     ┇                                         __main__ ┇ i=9
[12.06 03:37:56] ┇ INFO     ┇                                         __main__ ┇ loss=0.7963378429412842
[12.06 03:37:56] ┇ INFO     ┇                                         __main__ ┇ i=10
[12.06 03:37:56] ┇ INFO     ┇                                         __main__ ┇ loss=0.7113561034202576
[12.06 03:37:56] ┇ INFO     ┇                                         __main__ ┇ i=11
[12.06 03:37:57] ┇ INFO     ┇                                         __main__ ┇ loss=0.878730833530426
[12.06 03:37:57] ┇ INFO     ┇                                         __main__ ┇ i=12
[12.06 03:37:57] ┇ INFO     ┇                                         __main__ ┇ loss=0.7311134338378906
[12.06 03:37:57] ┇ INFO     ┇                                         __main__ ┇ i=13
[12.06 03:37:59] ┇ INFO     ┇                                         __main__ ┇ loss=0.8547625541687012
[12.06 03:37:59] ┇ INFO     ┇                                         __main__ ┇ i=14
[12.06 03:38:00] ┇ INFO     ┇                                         __main__ ┇ loss=0.732170581817627
[12.06 03:38:00] ┇ INFO     ┇                                         __main__ ┇ i=15
[12.06 03:38:00] ┇ INFO     ┇                                         __main__ ┇ loss=0.709916353225708
[12.06 03:38:00] ┇ INFO     ┇                                         __main__ ┇ i=16
[12.06 03:38:04] ┇ INFO     ┇                                         __main__ ┇ loss=0.783307671546936
[12.06 03:38:04] ┇ INFO     ┇                                         __main__ ┇ global_step=2
[12.06 03:38:05] ┇ INFO     ┇                                   chrisbase.data ┇ =========================================================================================================================================
[12.06 03:38:05] ┇ INFO     ┇                                   chrisbase.data ┇ [EXIT] python /raid/chrisjihee/proj/DeepKNLP/DeepKNLP/arguments.py  ($=00:00:51.433)
[12.06 03:38:05] ┇ INFO     ┇                                   chrisbase.data ┇ =========================================================================================================================================
